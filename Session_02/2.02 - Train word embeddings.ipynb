{"cells":[{"cell_type":"markdown","metadata":{"id":"C4drq3zd0Yl6"},"source":["# <font color=\"#003660\">Applied Machine Learning for Text Analysis (M.184.5331)</font>\n","  "]},{"cell_type":"markdown","metadata":{"id":"fkB8pITZ0dYo"},"source":["# <font color=\"#003660\">Session 2: Unsupervised NLP</font>"]},{"cell_type":"markdown","metadata":{"id":"MbNXbRF50d1S"},"source":["# <font color=\"#003660\">Notebook 2: Train Your Own Word Embeddings</font>\n","\n","<center><br><img width=256 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/dag.png\"/><br></center>\n","\n","<p>\n","<center>\n","<div>\n","    <font color=\"#085986\"><b>By the end of this lesson, you ...</b><br><br>\n","        ... are able to train your own word embeddings from data.\n","    </font>\n","</div>\n","</center>\n","</p>"]},{"cell_type":"markdown","metadata":{"id":"tpXD2GY90UNN"},"source":["# Import packages\n","\n","As always, we first need to load a number of required Python packages:\n","- `pandas` provides high-performance, easy-to-use data structures and data analysis tools.\n","- `SQLAlchemy`, together with `pymysql`, allows to communicate with SQL databases.\n","- `getpass` provides function to safely enter passwords.\n","- `spacy` offers industrial-strength natural language processing.\n","- `gensim` is a fast library for training of vector embeddings and topic models.\n","- `sklearn` is the de-facto standard machine learning package in Python.\n","- `plotly` is a library for creating interactive plots."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4494,"status":"ok","timestamp":1667484929767,"user":{"displayName":"Oliver Mueller","userId":"12717968064814035358"},"user_tz":-60},"id":"Q_ajKrg-FU9T","outputId":"ca15eb28-1f53-4f22-994e-5bbb7bcd4268"},"outputs":[],"source":["# Install packages\n","!pip install pymysql"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QNgYvVea0UNN","scrolled":true},"outputs":[],"source":["import pandas as pd\n","import pickle\n","from sqlalchemy import create_engine, text\n","import getpass\n","import spacy\n","from gensim.models import word2vec\n","from gensim.models import KeyedVectors\n","import gensim.downloader as api\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE\n","import plotly.express as px\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","metadata":{"id":"TpnK3Eb00UNX"},"source":["# How are word embeddings learned?"]},{"cell_type":"markdown","metadata":{"id":"UiPasIBY0UNX"},"source":["Word embeddings can be learned from a given corpus by training a shallow neural network. The training objective of the network is either to predict a target word from its context words in a sentence (CBOW) or, vice versa, to predict the context words of a target word in a sentence (Skip-gram). After training, the weights matrix W represents the actual embedding vectors. (Mikolov et al., 2013)\n","\n","<br>\n","\n","<center><img width=512 src=\"https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/week_3/images/cbow_skipgram.jpg\"/>Source: Kimothi et al. (2020)</center>"]},{"cell_type":"markdown","metadata":{"id":"NGxkqnFe0UNX"},"source":["# Load documents"]},{"cell_type":"markdown","metadata":{"id":"cmaWF3ER0UNX"},"source":["We will work with a dataset consisitng of approx. 130.000 wine reviews written by professional sommeliers. Each review has review text and rating and additional meta data about the wine, such as, variety, location, winery, or price. You can find the original dataset here: https://www.kaggle.com/zynicide/wine-reviews"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":34211,"status":"ok","timestamp":1667485314941,"user":{"displayName":"Oliver Mueller","userId":"12717968064814035358"},"user_tz":-60},"id":"rqk69P-S0UNX","outputId":"d7237bc8-6893-4f5c-adf8-5cfd80ea7212"},"outputs":[],"source":["# Get credentials\n","user = input(\"Username: \")\n","passwd = getpass.getpass(\"Password: \")\n","server = input(\"Server: \")\n","db = input(\"Database: \")\n","\n","# Create an engine instance (SQLAlchemy)\n","engine = create_engine(\"mysql+pymysql://{}:{}@{}/{}\".format(user, passwd ,server, db))\n","\n","# Define SQL query\n","sql_query = \"SELECT * FROM WineDataset\"\n","\n","# Query dataset (pandas)\n","corpus = pd.DataFrame(engine.connect().execute(text(sql_query)))"]},{"cell_type":"markdown","metadata":{"id":"AFD6jgiF0UNY"},"source":["# Preprocess documents"]},{"cell_type":"markdown","metadata":{"id":"e4nBClQR0UNY"},"source":["Perform some standard natural language preprocessing steps with spaCy. As word embeddings are best trained on sentences, not documents, we first cut the reviews into sentences and then preprocess them sentence by sentence.\n","\n","Warning: This takes 10+ minutes!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w-TLOTX50UNY","scrolled":true},"outputs":[],"source":["nlp = spacy.load('en_core_web_sm', disable=['ner', 'parser', 'tagger'])\n","nlp.add_pipe('sentencizer')\n","\n","sentences = []\n","for i, entry in corpus.iterrows():\n","    tokens = nlp(entry['description'])\n","    for sentence in tokens.sents:\n","        tokens_to_keep = []\n","        for t in sentence:\n","            if t.is_alpha: # only consider alphanumerical tokens\n","                tokens_to_keep.append(t.text.lower()) # append lower-cased word\n","        sentences.append(tokens_to_keep)"]},{"cell_type":"markdown","metadata":{"id":"pqcc-uex0UNZ"},"source":["Save sentences to disk."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18189,"status":"ok","timestamp":1667485415130,"user":{"displayName":"Oliver Mueller","userId":"12717968064814035358"},"user_tz":-60},"id":"TagnCxW00UNH","outputId":"47c98e78-25ae-4e3b-83d3-910ada6fb7ed"},"outputs":[],"source":["# Set up Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"utcB3K1G0UNZ","scrolled":true},"outputs":[],"source":["with open(\"/content/drive/MyDrive/amlta/sentences.pkl\", \"wb\") as output:\n","    pickle.dump(sentences, output, pickle.HIGHEST_PROTOCOL)"]},{"cell_type":"markdown","metadata":{"id":"SEIjL6fC0UNZ"},"source":["Load sentences from disk."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lhtt9nxa0UNZ","scrolled":true},"outputs":[],"source":["with open(\"/content/drive/MyDrive/amlta/sentences.pkl\", \"rb\") as input:\n","    sentences = pickle.load(input)"]},{"cell_type":"markdown","metadata":{"id":"bwMrrusB0UNZ"},"source":["How many sentences do we have?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":207,"status":"ok","timestamp":1667486816128,"user":{"displayName":"Oliver Mueller","userId":"12717968064814035358"},"user_tz":-60},"id":"ZMk9OqVP0UNZ","outputId":"389ad762-df2b-4be8-bb22-5714bc16a14f"},"outputs":[],"source":["len(sentences)"]},{"cell_type":"markdown","metadata":{"id":"wTzW5f3_0UNa"},"source":["Look at the first one."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":472,"status":"ok","timestamp":1667486827557,"user":{"displayName":"Oliver Mueller","userId":"12717968064814035358"},"user_tz":-60},"id":"q_QskZol0UNa","outputId":"01aacc20-2bc7-457e-c586-b741da11dd59","scrolled":true},"outputs":[],"source":["sentences[0]"]},{"cell_type":"markdown","metadata":{"id":"rRjy3qYn0UNa"},"source":["# Learn word embeddings from data"]},{"cell_type":"markdown","metadata":{"id":"A1Nz2tVV0UNa"},"source":["We use Gensim's implementation of word2vec to create word embeddings. See https://radimrehurek.com/gensim/models/keyedvectors.html#module-gensim.models.keyedvectors for documentation."]},{"cell_type":"markdown","metadata":{"id":"IcQEgmXF0UNa"},"source":["Create a model with 300 dimensions and a context window of 6 words. Only consider words that appear at least in 2 documents. Use 6 CPU cores for estimating the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QgwWepqb0UNa","scrolled":true},"outputs":[],"source":["model = word2vec.Word2Vec(sentences, vector_size=300, window = 6, min_count = 2, workers=6)"]},{"cell_type":"markdown","metadata":{"id":"FKUV1Kd00UNa"},"source":["Get word vectors from model and store as file for later reuse."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IgG_6jaZ0UNa","scrolled":true},"outputs":[],"source":["word_vectors = model.wv\n","word_vectors.save_word2vec_format(\"/content/drive/MyDrive/amlta/wine_300dim_2minwords_6context\")"]},{"cell_type":"markdown","metadata":{"id":"5g1G58UA0UNb"},"source":["Load word vectors from file."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KsSUXtqk0UNb","scrolled":true},"outputs":[],"source":["word_vectors = KeyedVectors.load_word2vec_format(\"/content/drive/MyDrive/amlta/wine_300dim_2minwords_6context\")"]},{"cell_type":"markdown","metadata":{"id":"opj1RKV_0UNb"},"source":["# Explore word embeddings"]},{"cell_type":"markdown","metadata":{"id":"bTqNH4C-0UNb"},"source":["Retrieve most similar words to a given word."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":190,"status":"ok","timestamp":1667486940458,"user":{"displayName":"Oliver Mueller","userId":"12717968064814035358"},"user_tz":-60},"id":"6C17oBpp0UNb","outputId":"fa8cd4a1-b0dd-40f6-deca-af4c18698c4f","scrolled":true},"outputs":[],"source":["word_vectors.most_similar(\"red\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":459,"status":"ok","timestamp":1667486968897,"user":{"displayName":"Oliver Mueller","userId":"12717968064814035358"},"user_tz":-60},"id":"yCYSauoD0UNb","outputId":"82f0a3aa-00e0-4443-ca30-9194adec36a3","scrolled":true},"outputs":[],"source":["word_vectors.most_similar(\"white\")"]},{"cell_type":"markdown","metadata":{"id":"wuP3ubCn0UNc"},"source":["Which word doesn't belong to the set?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":631,"status":"ok","timestamp":1667486975463,"user":{"displayName":"Oliver Mueller","userId":"12717968064814035358"},"user_tz":-60},"id":"fuaPa_520UNc","outputId":"0986d58b-a523-4c41-aa75-7c1611749fe9","scrolled":true},"outputs":[],"source":["word_vectors.doesnt_match([\"red\", \"raspberry\", \"cranberry\", \"peach\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":229,"status":"ok","timestamp":1667486995836,"user":{"displayName":"Oliver Mueller","userId":"12717968064814035358"},"user_tz":-60},"id":"EYRxgd5P0UNc","outputId":"2607df94-e7bc-4009-afde-4033ff92aaa0","scrolled":true},"outputs":[],"source":["word_vectors.doesnt_match([\"white\", \"cherry\", \"cantaloupe\", \"citrus\"])"]},{"cell_type":"markdown","metadata":{"id":"-osmaKEv0UNc"},"source":["Let's look at some analogies using \"King – Man + Woman = Queen\"-style vector arithmetic"]},{"cell_type":"markdown","metadata":{"id":"PdWXHwgx0UNc"},"source":["Fig - Red + White = ?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":881,"status":"ok","timestamp":1667487005777,"user":{"displayName":"Oliver Mueller","userId":"12717968064814035358"},"user_tz":-60},"id":"KkhETmQy0UNc","outputId":"3689acf8-7418-4cab-cc73-d6a9896f0ecc","scrolled":true},"outputs":[],"source":["word_vectors.most_similar(positive=['fig', 'white'], negative=['red'])"]},{"cell_type":"markdown","metadata":{"id":"jv_8FBe30UNd"},"source":["Honey - White + Red = ?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":274,"status":"ok","timestamp":1667487054908,"user":{"displayName":"Oliver Mueller","userId":"12717968064814035358"},"user_tz":-60},"id":"s1GD8puB0UNd","outputId":"19bae8e6-f090-4314-ecc9-32fa94528f96","scrolled":true},"outputs":[],"source":["word_vectors.most_similar(positive=['honey', 'red'], negative=['white'])"]},{"cell_type":"markdown","metadata":{"id":"RktVMiFc0UNd"},"source":["Riesling - White + Red = ?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":190,"status":"ok","timestamp":1667487099584,"user":{"displayName":"Oliver Mueller","userId":"12717968064814035358"},"user_tz":-60},"id":"I60IAqsX0UNd","outputId":"685a754f-58ca-42e0-a1b1-2da503bfa4f2","scrolled":true},"outputs":[],"source":["word_vectors.most_similar(positive=['riesling', 'red'], negative=['white'])"]},{"cell_type":"markdown","metadata":{"id":"_79S_bDC0UNd"},"source":["# Visualize embeddings"]},{"cell_type":"markdown","metadata":{"id":"Dx8CfMfI0UNd"},"source":["Get a list of all the words in the vocabulary."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gx6ndOIA0UNd","scrolled":true},"outputs":[],"source":["vocab = list(word_vectors.key_to_index)"]},{"cell_type":"markdown","metadata":{"id":"ezf-0WXm0UNe"},"source":["Retrieve the associated word embedding vectors from the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OrfDB_s40UNe","scrolled":true},"outputs":[],"source":["X = word_vectors[vocab]"]},{"cell_type":"markdown","metadata":{"id":"gNLw0kPB0UNe"},"source":["Reduce the dimensionality of the data with PCA."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Am2TOjDu0UNe","scrolled":true},"outputs":[],"source":["X_pca = PCA(n_components=2).fit_transform(X)"]},{"cell_type":"markdown","metadata":{"id":"m_yIxxpd0UNf"},"source":["Reformat data, add similarity to a \"seed\" word, (filter to most similar words), and create an interactive scatterplot."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":542},"executionInfo":{"elapsed":1834,"status":"ok","timestamp":1667487136433,"user":{"displayName":"Oliver Mueller","userId":"12717968064814035358"},"user_tz":-60},"id":"vyuCr_qh0UNf","outputId":"4cb84d05-df69-470a-cf6c-d55dae113ded"},"outputs":[],"source":["pca_df = pd.DataFrame(X_pca, index=vocab, columns=['x', 'y'])\n","pca_df[\"word\"] = vocab\n","\n","seed = \"citrus\"\n","pca_df[\"sim\"] = 0\n","\n","for word, sim in word_vectors.most_similar(seed, topn=100):\n","    pca_df.loc[word, 'sim'] = sim\n","\n","# filter to most similar words?\n","pca_df = pca_df[pca_df[\"sim\"]>0]\n","\n","fig = px.scatter(pca_df, x=\"x\", y=\"y\", color=\"sim\",\n","                 hover_data=[\"word\"],\n","                 range_x = [-11, 11], range_y = [-11, 11],\n","                 opacity = 0.2, color_continuous_scale='agsunset_r')\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"CxuYC6ZoRSEM"},"outputs":[],"source":[]}],"metadata":{"colab":{"machine_shape":"hm","provenance":[{"file_id":"1HhlHsqxx4kYPaCdu6NmguaXjX9-GoKJh","timestamp":1636383280879}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.13"}},"nbformat":4,"nbformat_minor":0}
