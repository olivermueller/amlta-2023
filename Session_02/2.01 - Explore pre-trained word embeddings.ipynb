{"cells":[{"cell_type":"markdown","metadata":{"id":"C4drq3zd0Yl6"},"source":["# <font color=\"#003660\">Applied Machine Learning for Text Analysis (M.184.5331)</font>\n","  "]},{"cell_type":"markdown","metadata":{"id":"fkB8pITZ0dYo"},"source":["# <font color=\"#003660\">Week 2: Unsupervised NLP</font>"]},{"cell_type":"markdown","metadata":{"id":"MbNXbRF50d1S"},"source":["# <font color=\"#003660\">Notebook 1: Explore Pre-trained Word Embeddings</font>\n","\n","<center><br><img width=256 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/dag.png\"/><br></center>\n","\n","<p>\n","<center>\n","<div>\n","    <font color=\"#085986\"><b>By the end of this lesson, you ...</b><br><br>\n","        ... understand what word embeddings are, <br>\n","        ... are able to to load and use pre-trained word embeddings for determining semantic similarity between words, <br>\n","        ... are able to visualize word embeddings, and <br>\n","        ... can answer word analogy queries with word embeddings.\n","    </font>\n","</div>\n","</center>\n","</p>"]},{"cell_type":"markdown","metadata":{"id":"tpXD2GY90UNN"},"source":["# Import packages\n","\n","As always, we first need to load a number of required Python packages:\n","- `pandas` provides high-performance, easy-to-use data structures and data analysis tools.\n","- `numpy` is a library adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\n","- `spacy` offers industrial-strength natural language processing.\n","- `gensim` is a fast library for training of vector embeddings and topic models.\n","- `sklearn` is the de-facto standard machine learning package in Python.\n","- `plotly` is a library for creating interactive plots."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QNgYvVea0UNN","scrolled":true},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import pickle\n","import spacy\n","from gensim.models import word2vec\n","from gensim.models import KeyedVectors\n","import gensim.downloader as api\n","from sklearn.decomposition import PCA\n","from sklearn.manifold import TSNE\n","import plotly.express as px\n","import warnings\n","warnings.filterwarnings(\"ignore\")"]},{"cell_type":"markdown","metadata":{"id":"F8Y2Sfc_0UNO"},"source":["# What are word embeddings?"]},{"cell_type":"markdown","metadata":{"id":"N1vPJxUd0UNO"},"source":["Word embeddings (e.g., word2vec, Glove) are an alternative to representing words through one-hot encoding. In contrast to one-hot encoding, which are hard-coded high-dimensional and sparse representations, word embeddings are lower-dimensional dense representations that are learned from the data. Word embeddings represent each word in a dictionary by a real-valued numeric vector. (Chollet, 2018)"]},{"cell_type":"markdown","metadata":{"id":"HZZpFFIb0UNP"},"source":["<center><img width=512 src=\"https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/week_3/images/one-hot_vs_word-embeddings.png\"/></center>"]},{"cell_type":"markdown","metadata":{"id":"sOWdCXxp0UNQ"},"source":["In addition, word embeddings are able to capture the semantic meaning of words and map it into geometric space. This is achieved by assigning a numeric vector to each word in the vocabulary, such that the distance (e.g., cosine distance) between any two word vectors would capture part of the semantic relationship between the two associated words. For example, \"apple\" and \"dog\" are words that are semantically quite different, so a reasonable embedding space would represent them as vectors that would be very far apart. But \"kitchen\" and \"fridge\" are related words, so their vectors hould be close to each other. (Chollet, 2018)\n","\n","Ideally, in a good embeddings space, the path (which is a vector itself) to go from \"kitchen\" to \"fridge\" would capture precisely the semantic relationship between these two concepts. This idea is illustrated in the following figure (adapted from Chollet, 2018)."]},{"cell_type":"markdown","metadata":{"id":"npw9kC1E0UNR"},"source":["<center><img width=768 src=\"https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/week_3/images/wolf_dog_tiger_cat.png\"/></center>"]},{"cell_type":"markdown","metadata":{"id":"pij6tX4p0UNS"},"source":["# Download pre-trained word embeddings"]},{"cell_type":"markdown","metadata":{"id":"Dqn3rgwK0UNS"},"source":["Download pre-trained word vectors from Gensim-data. The word vectors have 300 dimensions and were learned from 6 billion tokens from Wikipedia (2014) and Gigaword 5 (https://catalog.ldc.upenn.edu/LDC2011T07). See https://radimrehurek.com/gensim/models/keyedvectors.html#module-gensim.models.keyedvectors for documentation."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":149842,"status":"ok","timestamp":1668628176348,"user":{"displayName":"Oliver Mueller","userId":"12717968064814035358"},"user_tz":-60},"id":"FZ9Pz1EO0UNT","outputId":"db10c497-c786-475f-9914-40f9c838c6ec","scrolled":true},"outputs":[],"source":["word_vectors = api.load(\"glove-wiki-gigaword-300\")"]},{"cell_type":"markdown","metadata":{"id":"SnjhXgRG0UNU"},"source":["# Explore embeddings"]},{"cell_type":"markdown","metadata":{"id":"U9JnskJ60UNU"},"source":["Look at a single vector."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":306,"status":"ok","timestamp":1667483716560,"user":{"displayName":"Oliver Mueller","userId":"12717968064814035358"},"user_tz":-60},"id":"GecJp8Cf2ouN","outputId":"295d2c2b-ffc2-45ca-ba3d-580ae728062d"},"outputs":[],"source":["word_vectors[\"man\"]"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":408,"status":"ok","timestamp":1667483726800,"user":{"displayName":"Oliver Mueller","userId":"12717968064814035358"},"user_tz":-60},"id":"aPvVbjK_0UNU","outputId":"deb2ecd5-441d-4979-9c1a-673c2cc97981"},"outputs":[],"source":["len(word_vectors[\"man\"])"]},{"cell_type":"markdown","metadata":{"id":"Q1MZU8Xy0UNV"},"source":["Use Gensim's built-in function most_similar() to retrieve most similar words to a given word."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1571,"status":"ok","timestamp":1667483750694,"user":{"displayName":"Oliver Mueller","userId":"12717968064814035358"},"user_tz":-60},"id":"BUIE4x7N0UNV","outputId":"6935ee23-2ff7-434b-d9da-9c00bd30bbfb","scrolled":true},"outputs":[],"source":["word_vectors.most_similar(\"man\")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1667483787529,"user":{"displayName":"Oliver Mueller","userId":"12717968064814035358"},"user_tz":-60},"id":"we5PBPhc0UNV","outputId":"0006764a-786a-467d-9416-6264ec072427","scrolled":true},"outputs":[],"source":["word_vectors.most_similar(\"woman\")"]},{"cell_type":"markdown","metadata":{"id":"bULoKGGl0UNW"},"source":["Which word doesn't belong to the set?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"executionInfo":{"elapsed":923,"status":"ok","timestamp":1667483834224,"user":{"displayName":"Oliver Mueller","userId":"12717968064814035358"},"user_tz":-60},"id":"U5UNvxNf0UNW","outputId":"b5c2a013-b20d-40db-e1ec-9196751b1b0d","scrolled":true},"outputs":[],"source":["word_vectors.doesnt_match([\"red\", \"green\", \"blue\", \"sky\"])"]},{"cell_type":"markdown","metadata":{"id":"7m1BYijF0UNW"},"source":["Let's look at some analogies using vector arithmetic: King – Man + Woman = ?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":309,"status":"ok","timestamp":1667483882350,"user":{"displayName":"Oliver Mueller","userId":"12717968064814035358"},"user_tz":-60},"id":"1zrasewX0UNW","outputId":"2a6d819a-cc41-4ae5-87f0-031831be68ed","scrolled":true},"outputs":[],"source":["word_vectors.most_similar(positive=['king', 'woman'], negative=['man'])"]},{"cell_type":"markdown","metadata":{"id":"eYS6JaA10UNW"},"source":["Berlin – Germany + France = ?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":371,"status":"ok","timestamp":1667484519889,"user":{"displayName":"Oliver Mueller","userId":"12717968064814035358"},"user_tz":-60},"id":"NLbDYH-n0UNX","outputId":"ecd7b205-e003-494d-d6f6-80e46e816c1f","scrolled":true},"outputs":[],"source":["word_vectors.most_similar(positive=['berlin', 'france'], negative=['germany'])"]},{"cell_type":"markdown","metadata":{"id":"ezmWLFueytpN"},"source":["Wolf - Dog + Cat = ?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":655,"status":"ok","timestamp":1667484026157,"user":{"displayName":"Oliver Mueller","userId":"12717968064814035358"},"user_tz":-60},"id":"lcAFO8s9DppR","outputId":"7ead1796-3f68-4c94-acdd-2c374fc93da7"},"outputs":[],"source":["word_vectors.most_similar(positive=['wolf', 'cat'], negative=['dog'])"]},{"cell_type":"markdown","metadata":{"id":"g-4r4EnO3b5M"},"source":["Now it's your turn. Explore the similarities and analogies between word embeddings..."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GHUDPGRb3cR8"},"outputs":[],"source":["# YOUR CODE GOES HERE"]},{"cell_type":"markdown","metadata":{"id":"_79S_bDC0UNd"},"source":["\n","# Visualize embeddings"]},{"cell_type":"markdown","metadata":{"id":"Dx8CfMfI0UNd"},"source":["Get a list of all the words in the vocabulary."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Gx6ndOIA0UNd","scrolled":true},"outputs":[],"source":["vocab = list(word_vectors.key_to_index)"]},{"cell_type":"markdown","metadata":{"id":"ezf-0WXm0UNe"},"source":["Retrieve the associated word embedding vectors from the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OrfDB_s40UNe","scrolled":true},"outputs":[],"source":["X = word_vectors[vocab]"]},{"cell_type":"markdown","metadata":{"id":"BOiyMRDy0UNe"},"source":["Reduce the dimensionality of the data with PCA."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Am2TOjDu0UNe","scrolled":true},"outputs":[],"source":["X_pca = PCA(n_components=2).fit_transform(X)"]},{"cell_type":"markdown","metadata":{"id":"m_yIxxpd0UNf"},"source":["Reformat data, add similarity to a \"seed\" word, and create an interactive scatterplot with plotly."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":542,"output_embedded_package_id":"158rM1fekP5SflfQme8UfiXa_yJ5ke5m8"},"executionInfo":{"elapsed":40218,"status":"ok","timestamp":1667484746312,"user":{"displayName":"Oliver Mueller","userId":"12717968064814035358"},"user_tz":-60},"id":"vyuCr_qh0UNf","outputId":"6af78581-874d-4e5e-9133-2c60f67afc35"},"outputs":[],"source":["pca_df = pd.DataFrame(X_pca, index=vocab, columns=['x', 'y'])\n","pca_df[\"word\"] = vocab\n","\n","seed = \"berlin\"\n","pca_df[\"sim\"] = 0\n","\n","for word, sim in word_vectors.most_similar(seed, topn=100):\n","    pca_df.loc[word, 'sim'] = sim\n","\n","# filter to 100 most similar words?\n","# pca_df = pca_df[pca_df[\"sim\"]>0]\n","\n","fig = px.scatter(pca_df, x=\"x\", y=\"y\", color=\"sim\",\n","                 hover_data=[\"word\"],\n","                 range_x = [-6, 6], range_y = [-4, 4],\n","                 opacity = 0.2, color_continuous_scale='agsunset_r')\n","fig.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rlqpw0Yt1WFl"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.5"}},"nbformat":4,"nbformat_minor":0}
