{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF USING GOOGLE COLABORATORY -> RUN FIRST!!!\n",
    "# OTHERWISE -> IGNORE ;-)\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Packages\n",
    "\n",
    "!pip install -q -U bitsandbytes\n",
    "!pip install -q -U git+https://github.com/huggingface/transformers.git\n",
    "!pip install -q -U git+https://github.com/huggingface/peft.git\n",
    "!pip install -q -U git+https://github.com/huggingface/accelerate.git\n",
    "!pip install -q -U git+https://github.com/adapter-hub/adapters.git\n",
    "!pip install -q -U git+https://github.com/huggingface/datasets.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#003660\">Applied Machine Learning for Text Analysis (M.184.5331)</font>\n",
    "# <font color=\"#003660\">Lesson 10: Hands-On Training: Fine-Tuning LLMs using Quantization and Low-Rank Adaptation</font>\n",
    "\n",
    "<center><br><img width=256 src=\"https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/resources/dag.png\"/><br></center>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<div>\n",
    "    <font color=\"#085986\"><b>By the end of this lesson, you will be able to...</b><br><br>\n",
    "        comprehend and apply quantization methods for model optimization;<br>\n",
    "        understand and implement LoRA methods for model fine-tuning; and<br>\n",
    "        integrate quantization and LoRA techniques to fine-tune models using QLoRA.<br>\n",
    "    </font>\n",
    "</div>\n",
    "</center>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Task</h2>\n",
    "\n",
    "<p>In this week's lecture, we will explore the fine-tuning of Large Language Models (LLMs) for text generation, focusing on the application of 4-bit quantization and low-rank adaptation (LoRA). This session marks a shift from our previous focus on mixed-precision training, introducing advanced techniques to optimize LLMs within the constraints of limited hardware resources.</p>\n",
    "\n",
    "<p>4-bit quantization will be utilised to effectively reduce the memory and computational requirements of LLMs. This technique is particularly relevant for training and running these models in environments with restricted hardware, allowing for more efficient use of resources.</p>\n",
    "\n",
    "<p>We will also delve into low-rank adaptation (LoRA), a method for modifying pre-trained models that enables efficient fine-tuning without the need to retrain the entire model. This approach is beneficial for adapting LLMs to specific tasks, providing a means to enhance model performance while considering computational efficiency.</p>\n",
    "\n",
    "<p>Yet, it is essential to apply the concepts learned in the previous session, especially in terms of managing document lengths and optimizing batch sizes. These practices remain vital in the efficient processing of data and in maintaining the smooth operation of the model, particularly in the context of the larger scale of LLMs.</p>\n",
    "\n",
    "<h2>Useful Links</h2>\n",
    "\n",
    "<ul>\n",
    "  <li><a href=\"https://huggingface.co/blog/hf-bitsandbytes-integration\">A Gentle Introduction to 8-bit Matrix Multiplication for transformers at scale using Hugging Face Transformers, Accelerate and bitsandbytes</a></li>\n",
    "  <li><a href=\"https://huggingface.co/blog/4bit-transformers-bitsandbytes\">Making LLMs even more accessible with bitsandbytes, 4-bit quantization and QLoRA</a></li>\n",
    "  <li><a href=\"https://huggingface.co/docs/transformers/main/en/perf_train_gpu_one\">Methods and tools for efficient training on a single GPU</a></li>\n",
    "  <li><a href=\"https://huggingface.co/docs/transformers/main/en/quantization?bnb=8-bit#bitsandbytes\">Quantization |Â BitsAndBytes</a></li>\n",
    "  <li><a href=\"https://huggingface.co/blog/lora\">Using LoRA for Efficient Stable Diffusion Fine-Tuning</a></li>\n",
    "  <li><a href=\"https://arxiv.org/pdf/2106.09685.pdf\">LoRA: Low-Rank Adaptation of Large Language Models</a></li>\n",
    "  <li><a href=\"https://arxiv.org/pdf/2305.14314.pdf\">QLORA: Efficient Finetuning of Quantized LLMs</a></li>\n",
    "</ul> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>With the continuous growth in the size of language models, exemplified by PaLM's 540 billion parameters and BLOOM's 176 billion, running them on standard devices presents increasing challenges. For instance, BLOOM-176B requires 8x 80GB A100 GPUs for inference and 72 GPUs for fine-tuning. To meet these challenges, solutions such as quantization are emerging. For instance, the implementation of 8-bit inference, recently incorporated into Hugging Face transformers, effectively reduces the memory requirements of these large models by half while maintaining their predictive accuracy, greatly facilitating their operation on a reduced number of GPUs.</p>\n",
    "\n",
    "<p>As discussed in a previous session, quantization is the process that focuses on reducing the precision of the weights in a neural network, typically from floating point to lower bit-width integers, such as 8-bit or 4-bit. This reduction in precision leads to smaller model sizes and faster computation, making it especially useful for deploying models on hardware with limited resources. Using tools like bitsandbytes, models can be effectively quantized to 8-bit or 4-bit. Let's take a look!</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "def bytes_to_mb(bytes):\n",
    "    return bytes / (1024 ** 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2 (Vanilla)\n",
    "# https://huggingface.co/gpt2-large\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2-large', device_map='auto')\n",
    "print(f'Footprint: {bytes_to_mb(model.get_memory_footprint())} MB')\n",
    "\n",
    "# Clear memory\n",
    "del model; torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2 (8-bit)\n",
    "# https://huggingface.co/gpt2-large\n",
    "\n",
    "config = BitsAndBytesConfig(\n",
    "    load_in_8bit=True, # 8-bit\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Speedup computation -> Used for its balance between maintaining a wide numeric range and reducing memory usage.\n",
    "    device_map='auto' # Device\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2-large', quantization_config=config)\n",
    "print(f'Footprint: {bytes_to_mb(model.get_memory_footprint())} MB')\n",
    "\n",
    "# Clear memory\n",
    "del config, model; torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GPT-2 (4-bit)\n",
    "# https://huggingface.co/gpt2-large\n",
    "\n",
    "config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True, # 4-bit\n",
    "    bnb_4bit_quant_type='nf4', # 4-bit data type from the QLoRA paper\n",
    "    bnb_4bit_use_double_quant=True, # Double quantization\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, # Speedup computation -> Used for its balance between maintaining a wide numeric range and reducing memory usage.\n",
    "    device_map='auto' # Device\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2-large', quantization_config=config)\n",
    "print(f'Footprint: {bytes_to_mb(model.get_memory_footprint())} MB')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>It's important to note that training with 8-bit and 4-bit weights is only supported for training extra parameters, not the entire model. This limitation arises because the reduced precision may not adequately capture the complexity of the entire model. Therefore, we focus on low-rank adaptation, which allows for the efficient fine-tuning of the model by adjusting only a small subset of its parameters. This approach maintains model performance while benefiting from the efficiencies of quantization.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Low-Rank Adaptation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p><center><i>\"For a large Transformer trained with Adam, we reduce that VRAM usage by up to 2/3 [...] as we do not need to store the optimizer states for the frozen parameters.\" (Hu et al., 2021, p.5)</i></center></p>\n",
    "<p><center><i>\"On GPT-3 175B, we reduce the VRAM consumption during training from 1.2TB to 350GB.\" (Hu et al., 2021, p.5)</i></center></p>\n",
    "\n",
    "<p>Low-Rank Adaptation (LoRA) is a technique designed to efficiently fine-tune large pre-trained models, such as those in natural language processing. The core idea behind LoRA is to introduce trainable low-rank matrices into the architecture of the pre-trained model. These matrices are much smaller in size compared to the original model's weights and thus require significantly less computational resources to update.</p>\n",
    "\n",
    "<p>The LoRA approach involves modifying specific layers of a pre-trained model, such as the attention and feedforward layers in a transformer model. Instead of training all the parameters of these layers, LoRA introduces two low-rank matrices for each layer. During fine-tuning, only these low-rank matrices are updated, while the original weights of the model remain frozen. This allows for efficient adaptation of the model to new tasks or datasets.</p>\n",
    "\n",
    "<p>The key advantage of LoRA is that it enables the fine-tuning of massive models with a relatively small increase in the number of trainable parameters. This approach significantly reduces the computational resources required for training, making it feasible to adapt large models on hardware with limited capabilities.</p>\n",
    "\n",
    "<p>HuggingFace and BigScience's research on LoRA further demonstrates its effectiveness in reducing the computational burden of fine-tuning large language models. By incorporating LoRA into their transformers library, they have enabled the wider machine learning community to access and fine-tune state-of-the-art models more efficiently.</p>\n",
    "\n",
    "<ul>\n",
    "    <li><a href=\"https://docs.adapterhub.ml\">AdapterHub</a></li>\n",
    "    <li><a href=\"https://huggingface.co/docs/hub/adapter-transformers\">Using Adapter Transformers at Hugging Face</a></li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "from peft import LoraConfig\n",
    "\n",
    "# Modules\n",
    "# https://github.com/huggingface/peft/blob/39ef2546d5d9b8f5f8a7016ec10657887a867041/src/peft/utils/other.py\n",
    "TARGET_MODULES = {\n",
    "    \"t5\": [\"q\", \"v\"],\n",
    "    \"mt5\": [\"q\", \"v\"],\n",
    "    \"bart\": [\"q_proj\", \"v_proj\"],\n",
    "    \"gpt2\": [\"c_attn\"],\n",
    "    \"bloom\": [\"query_key_value\"],\n",
    "    \"blip-2\": [\"q\", \"v\", \"q_proj\", \"v_proj\"],\n",
    "    \"opt\": [\"q_proj\", \"v_proj\"],\n",
    "    \"gptj\": [\"q_proj\", \"v_proj\"],\n",
    "    \"gpt_neox\": [\"query_key_value\"],\n",
    "    \"gpt_neo\": [\"q_proj\", \"v_proj\"],\n",
    "    \"bert\": [\"query\", \"value\"],\n",
    "    \"roberta\": [\"query\", \"value\"],\n",
    "    \"xlm-roberta\": [\"query\", \"value\"],\n",
    "    \"electra\": [\"query\", \"value\"],\n",
    "    \"deberta-v2\": [\"query_proj\", \"value_proj\"],\n",
    "    \"deberta\": [\"in_proj\"],\n",
    "    \"layoutlm\": [\"query\", \"value\"],\n",
    "    \"llama\": [\"q_proj\", \"v_proj\"],\n",
    "    \"chatglm\": [\"query_key_value\"],\n",
    "    \"gpt_bigcode\": [\"c_attn\"],\n",
    "    \"mpt\": [\"Wqkv\"],\n",
    "}\n",
    "\n",
    "# Configuration\n",
    "config = LoraConfig(\n",
    "    r=8, # This parameter determines the reparameterization scale. Smaller values train fewer parameters, reducing computational cost but possibly limiting learning efficacy. \n",
    "    lora_alpha=16, # Alters the size of the weight matrix, thereby increasing the importance of the fine-tuning relative to the existing, unchanged weights.\n",
    "    lora_dropout=0.01, # Dropout ;-)\n",
    "    target_modules=TARGET_MODULES['gpt2'], # This specifies the modules targeted for training.\n",
    "    bias='none', # Options between 'none', 'lora_only', 'all'\n",
    ")\n",
    "\n",
    "# ...\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-trained adapters\n",
    "\n",
    "from adapters import list_adapters\n",
    "\n",
    "# source can be \"ah\" (AdapterHub), \"hf\" (huggingface.co) or None (for both, default)\n",
    "adapter_infos = list_adapters(source='ah', model_name='gpt2')\n",
    "adapter_infos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Quantization + Low-Rank Adaptation = QLoRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>QLoRA, the amalgamation of Quantization and Low-Rank Adaptation (LoRA), is an efficient method employed in this course for fine-tuning Large Language Models (LLMs) using the Parameter Efficient Fine-Tuning Methods (PEFT) library. By leveraging the strengths of both quantization and LoRA, QLoRA significantly reduces the computational overhead and memory requirements typically associated with fine-tuning large models. Quantization minimizes the model's memory footprint, while LoRA allows for targeted, efficient adaptation of the model's weights. This combination results in a potent and resource-efficient approach, making the fine-tuning of LLMs more accessible and practical, even on hardware with limited computational capabilities.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "import peft\n",
    "import torch\n",
    "from transformers import BitsAndBytesConfig\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "MODEL_PATH = 'my_favourite_LLM'\n",
    "\n",
    "# BitsAndBytes\n",
    "# Configuration\n",
    "config = BitsAndBytesConfig(\n",
    "    # Some parameters...\n",
    ")\n",
    "\n",
    "# Tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_PATH)\n",
    "\n",
    "# Model\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_PATH, quantization_config=config, use_cache=False)\n",
    "\n",
    "# Enable gradient checkpointing\n",
    "# Reduces GPU memory consumption by storing only a subset of all activations (i.e., the rest are computed on-the-fly during backpropagation)\n",
    "model.gradient_checkpointing_enable()\n",
    "\n",
    "# Prepare model for k-bit training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "# LoRA\n",
    "# Configuration\n",
    "lora_config = LoraConfig(\n",
    "    # Some parameters...\n",
    ")\n",
    "\n",
    "# Wrap model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "# ...\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Foundation Models\n",
    "\n",
    "# https://huggingface.co/gpt2\n",
    "# https://huggingface.co/gpt2-medium\n",
    "# https://huggingface.co/gpt2-large\n",
    "# https://huggingface.co/gpt2-xl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tutorial: Causal Language Modeling\n",
    "# + what you have learned last week!\n",
    "# P.S. The provided code will not work as is, you will have to adapt it to your needs ;-)\n",
    "\n",
    "# https://huggingface.co/docs/transformers/tasks/language_modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import\n",
    "\n",
    "import peft\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from peft import prepare_model_for_kbit_training\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from transformers import BitsAndBytesConfig, Trainer, TrainingArguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have fun ;-)\n",
    "# ..."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpt-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
