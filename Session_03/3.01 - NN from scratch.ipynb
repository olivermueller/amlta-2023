{"cells":[{"cell_type":"markdown","metadata":{"id":"Rg6G-ZhCtn6W"},"source":["# <font color=\"#003660\">Applied Machine Learning for Text Analysis (M.184.5331)</font>\n","# <font color=\"#003660\">Week 3: NN from Scratch</font>\n","# <font color=\"#003660\">Notebook 1: NN with Numpy</font>\n","\n","\n","<center><br><img width=256 src=\"https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/resources/dag.png\"/><br></center>\n","\n","<p>\n","<center>\n","<div>\n","    <font color=\"#085986\"><b>By the end of this lesson, you will be able to...</b><br><br>\n","        ... describe the reasoning behind single-layer perceptrons;<br>\n","        ... interpret the functionality of artificial neural networks;<br>\n","        ... build your first artificial neural network.\n","    </font>\n","</div>\n","</center>\n","</p>"]},{"cell_type":"markdown","metadata":{"id":"Ts8ZpxVFtn6Z"},"source":["# 1. What are Artificial Neural Networks?\n","\n","<p>In this lesson, we will start experimenting with so-called artificial neural networks (ANNs). Neural networks are a class of machine learning algorithms that can provide you with extremely powerful models and are, as a result, a great addition to your machine learning toolbox. ANNs are known as universal function approximators &mdash; i.e., they are capable of representing and approximating almost any arbitrary function, and that, regardless of the complexity. Self-driving cars, voice assistants, and modern medical imagery systems are all modern examples of what artifical neural networks can do. Even though artifical neural networks can be extremely powerful, it is important to keep in mind that they do not provide a one-size-fits-all solution out of the box.</p>\n","\n","<table class=\"image\">\n","<center>\n","<caption align=\"bottom\">(Patterson &amp; Gibson, 2017, p.55)</caption>\n","<tr><td><img width=540 src='https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/week_2/images/nn_topology.png'></td></tr>\n","</center>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"8laK-gnrtn6a"},"source":["# 2. Perceptron: The Building Block of Neural Networks\n","\n","<p><center><font color=\"#085986\"><strong><i>The following section is based on the book \"Python Machine Learning\" by Sebastian Raschka and Vahid Mirjalili (2017).</i></strong></font></center></p>\n","\n","<p>In order to get a better understanding of artifical neural networks, one must first understand its underlying building block &mdash; i.e., the perceptron. In its primitive form, the artificial perceptron &mdash; i.e., the Rosenblatt perceptron (Rosenblatt, 1958) &mdash; is a mathematical function that mimics how neurons work in the human brain by enabling the binary classification of linearly seperable observations. As shown below, a single-layer perceptron consists of a set of input features ($x_{n}$) and their corresponding weights ($w_{n}$) as well as a threshold function (a.k.a. step function). Ultimately, the goal is to minimise the error by iteratively optimising the weight coefficients.</p>\n","\n","<table class=\"image\">\n","<center>\n","<caption align=\"bottom\">(Raschka &amp; Mirjalili, 2017, p.24)</caption>\n","<tr><td><img width=540 src='https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/week_2/images/perceptron.png'></td></tr>\n","</center>\n","</table>\n","\n","<p>In a more algorithmic way, the model illustrated above can be represented as follows:</p>\n","\n","\\begin{equation}\n","\\LARGE\n","   \\phi(z)=%\n","   \\begin{cases}\n","     \\;\\;\\;\\;1 \\;\\;\\;\\; \\text{if $z$} \\geq \\theta\\\\\n","     -1 \\;\\;\\;\\; \\text{otherwise}\n","   \\end{cases}\n","\\end{equation}\n","\n","\\begin{align}\n","\\LARGE\n","z = w_0x_0 + w_1x_1 + \\ldots + w_mx_m = \\sum_{j}^{}w_{j}x_{j}\n","\\end{align}\n","\n","<p>where $z$ represents the weighted sum or dot product of all inputs and weights and $\\theta$ the pre-defined threshold value for the threshold function. The model above would then output $1$ if $z$ $\\geq$ $\\theta$ &mdash; i.e., positive class &mdash; or $-1$ if $z$ $<$ $\\theta$ &mdash; i.e., negative class.</p>\n","\n","<table class=\"image\">\n","<center>\n","<caption align=\"bottom\">(Raschka &amp; Mirjalili, 2017, p.21)</caption>\n","<tr><td><img width=540 src='https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/week_2/images/threshold.png'></td></tr>\n","</center>\n","</table>\n","\n","<p>Since our goal is to minimise the error, we can, in an iterative manner, simultaneously update the weight coefficients of our model by following Rosenblatt's perceptron learning rule:</p>\n","\n","<ol>\n","    <li>Initialise all weight coefficients &mdash; e.g., with zeros or random values.</li>\n","    <li>For each training sample $x^i$:\n","        <ol>\n","            <li>Compute the output value $\\hat{y}$ &mdash; i.e., the class label.</li>\n","            <li>Update the weights accordingly (as shown below).</li>\n","        </ol>\n","</ol>\n","\n","\\begin{align}\n","\\LARGE\n","w_j := w_j + \\Delta w_j\n","\\end{align}\n","\n","\\begin{align}\n","\\LARGE\n","\\Delta w_j = \\eta \\; (y^{(i)} - \\hat{y}^{(i)}) \\; {x_{j}}^{(i)}\n","\\end{align}\n","\n","<p>where $\\Delta w_j$ represents the update value for each weight $w_j$ in the weight vector, $\\eta$ the so-called learning rate &mdash; i.e., a constant between $0.0$ and $1.0$ &mdash;, $y^i$ the true class label of a given training sample, and $\\hat{y}^i$ the predicted class label of a given training sample.</p>\n","\n","<p><center><i>(Raschka &amp; Mirjalili, 2017, pp.18-24)</i></center></p>"]},{"cell_type":"markdown","metadata":{"id":"8_tsUDFutn6a"},"source":["## 2.1 Single-Layer Perceptron: An Example\n","\n","<p><center><font color=\"#085986\"><strong><i>The following section is based on the book \"Python Machine Learning\" by Sebastian Raschka and Vahid Mirjalili (2017).</i></strong></font></center></p>\n","\n","<table class=\"image\">\n","<center>\n","<caption align=\"bottom\"><a href=\"https://archive.ics.uci.edu/ml/datasets/iris\">https://archive.ics.uci.edu/ml/datasets/iris</a></caption>\n","<tr><td><img width=540 src='https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/week_2/images/iris.png'></td></tr>\n","</center>\n","</table>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w9sZhD_Utn6b"},"outputs":[],"source":["################################################\n","# Load Iris dataset                            #\n","# https://archive.ics.uci.edu/ml/datasets/iris #\n","################################################\n","\n","# Import\n","import numpy as np\n","import pandas as pd\n","\n","# Load dataset (UCI)\n","df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n","df = df.sample(n=50, random_state=42).reset_index(drop=True)\n","\n","# Define features (X)\n","X = df.iloc[:, [0, 2]].values\n","\n","# Define targets (y)\n","y = df.iloc[:, 4].values\n","y = np.where(y == 'Iris-setosa', -1, 1)\n","\n","print('Training set:')\n","print('X: {}'.format(X.shape))\n","print('y: {}'.format(y.shape))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GtPkdCngtn6c"},"outputs":[],"source":["#########################################\n","# Define perceptron class               #\n","# (Raschka & Mirjalili, 2017, pp.24-33) #\n","#########################################\n","\n","class Perceptron(object):\n","\n","    def __init__(self, eta=0.1, n_iter=3, random_state=42):\n","\n","        # Define learning rate\n","        self.eta = eta\n","\n","        # Define number of epochs\n","        self.n_iter = n_iter\n","\n","        # Define random state\n","        self.random_state = random_state\n","\n","    def fit(self, X, y):\n","\n","        # Initialise weights (random)\n","        rgen = np.random.RandomState(self.random_state)\n","        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=1 + X.shape[1])\n","\n","        self.errors_ = []\n","\n","        for _ in range(self.n_iter):\n","\n","            # Print\n","            print('=========================================================')\n","            print(f'\\t\\t\\tEpoch {_+1}')\n","            print('=========================================================')\n","\n","            errors = 0\n","\n","            for index, (xi, target) in enumerate(zip(X, y)):\n","\n","                # Make prediction\n","                prediction = self.predict(xi)\n","\n","                # Prediction != Target\n","                if prediction != target:\n","\n","                    # Calculate update\n","                    update = # TODO\n","\n","                    # Update weight coefficients\n","                    self.w_[1:] += # TODO\n","\n","                    # Update bias coefficient\n","                    self.w_[0] += # TODO\n","\n","                    # Print\n","                    print(f'Index: {index:2} | Pred.: {self.predict(xi):2} | Target: {target:2} | Update: {update:2}')\n","\n","                    # Count errors\n","                    errors += 1\n","\n","            self.errors_.append(errors); print(); print()\n","\n","        return self\n","\n","    def net_input(self, X):\n","        return # TODO\n","\n","    def predict(self, X):\n","        return np.where(self.net_input(X) >= 0.0, 1, -1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_TWa9Ozxtn6d"},"outputs":[],"source":["######################\n","# Train model        #\n","# Epochs: 5          #\n","# Learning rate: 0.1 #\n","######################\n","\n","ppn = Perceptron(n_iter=3, eta=0.1)\n","ppn.fit(X, y)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GaNcxzaftn6d"},"outputs":[],"source":["#########################\n","# Plot training history #\n","#########################\n","\n","%matplotlib inline\n","\n","import matplotlib.pyplot as plt\n","from mlxtend.plotting import plot_decision_regions\n","\n","plot_decision_regions(X, y, clf=ppn)\n","\n","plt.title('Perceptron')\n","plt.xlabel('sepal length [cm]')\n","plt.ylabel('petal length [cm]')\n","plt.show()\n","\n","plt.plot(range(1, len(ppn.errors_)+1), ppn.errors_, marker='o')\n","plt.xlabel('Iterations')\n","plt.ylabel('Misclassifications')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"OtNIN4jktn6e"},"source":["<p><center><i>(Raschka &amp; Mirjalili, 2017, pp.24-33)</i></center></p>"]},{"cell_type":"markdown","metadata":{"id":"bPtp7Tj8tn6e"},"source":["# 3. Building our First Artifical Neural Network\n","\n","<p><center><font color=\"#085986\"><strong><i>The following section is (partly) based on the book \"Neural Network Projects with Python\" by James Loy (2019).</i></strong></font></center></p>\n","\n","<table class=\"image\">\n","<center>\n","<caption align=\"bottom\">(Loy, 2019, p.16)</caption>\n","<tr><td><img width=420 src='https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/week_2/images/mlp.png'></td></tr>\n","</center>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"2t1kh0GAtn6f"},"source":["## 3.1 Multilayer Perceptron: A Simple Architecture\n","\n","<p>The model illustrated above is known as a multilayer perceptron (MLP) &mdash; i.e.,  a type of feedforward artificial neural networks consisiting of an <b>input layer, one or multiple hidden layers, and an output layer</b>. This simple architecture, which we will be implementing in this section, generates an output that can be expressed as follows:</p>\n","\n","\\begin{align}\n","\\LARGE\n","\\hat{y} = \\phi(W_{2} \\; \\phi(W_{1}x + b_{1})+b_{2})\n","\\end{align}\n","\n","<p>where $W_1$ and $W_2$ represent the weight vectors, $b_1$ and $b_2$ the bias units, and $\\phi$ the activation functions. Please note that MLPs, in contrast to single-layer perceptrons, make use of <b>non-linear activation functions</b>. As a result, in order to move from discrete to continuous outputs and therefore obtain non-linearity, threshold functions should be replaced with more suitable alternatives.</p>\n","\n","<table class=\"image\">\n","<center>\n","<caption align=\"bottom\"><a href=\"https://towardsdatascience.com/complete-guide-of-activation-functions-34076e95d044\">https://towardsdatascience.com/complete-guide-of-activation-functions-34076e95d044</a></caption>\n","<tr><td><img width=540 align='middle' src='https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/week_2/images/activ_functions.png'></td></tr>\n","</center>\n","</table>\n","\n","<center><img width=100 src=\"https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/resources/tip.png\"></center>\n","\n","<p>As it is often the case with technical literature, it is important to keep in mind that different notations and styles are used to explain identical concepts. Even though the algorithm exposed above may seem different to the one presented in section 2, its core idea remains the same.</p>"]},{"cell_type":"markdown","metadata":{"id":"hlNDY4XYtn6f"},"source":["## 3.2 Training a Neural Network: An Iterative Process\n","\n","<p>As exposed in the previous section, the overall quality of a model is determined by the the weights and biases obtained during the training process. The term training is used, since the idea is to fine-tune these weights and biases over several iterations. <b>Keep in mind that the ultimate goal is to minimise the cost function</b>. Closely analoguous to Rosenblatt's perceptron learning rule, each iteration of the training process consists of the following steps:</p>\n","\n","<ul style=\"list-style-type:round\">\n","    <li>feedforward &mdash; i.e., feeding the training examples from the input layer to the output layer ($\\hat{y}$);</li>\n","    <li>backpropagation &mdash; i.e., updating the weights and biases based on the obtained error.</li>\n","</ul>\n","\n","<table class=\"image\">\n","<center>\n","<caption align=\"bottom\">(Loy, 2019, p.18)</caption>\n","<tr><td><img width=800 src='https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/week_2/images/nn_sequence.png'></td></tr>\n","</center>\n","</table>"]},{"cell_type":"markdown","metadata":{"id":"7lkeivK1tn6f"},"source":["## 3.3 Implementation (PyTorch)\n","\n","<p>Feeling a bit overwhelmed? No worries! Let's start by building our first model using the <b>PyTorch</b> framework provided by <a href=\"https://ai.facebook.com\">Facebook AI</a>. Even though not necessarily the simplest alternative available, we believe that this highly popular framework provides the most suitable environment for an introductory course and will allow you to better understand the inner workings of neural networks.</p><br>\n","\n","<center><a href=\"https://pytorch.org\"><img width=384 src=\"https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/week_2/images/pytorch-logo-dark.png\"/></a></center>\n","    \n","\n","<p>Based on the illustration found at the beginning of this section, our model is pretty straightforward and contains only one hidden layer.  However, since our training data only contains two features (a.k.a. independent variables), the input layer of our model must only contain 2 neurons instead of 3. The resulting architecture can be pictured as follows:</p><br>\n","\n","<table class=\"image\">\n","<center>\n","<tr><td><img width=520 align='middle' src='https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/week_2/images/ann_example.png'></td></tr>\n","</center>\n","</table>\n","\n","<p>Using the example below, implement the architecture exposed above. As can be seen, the <code>__init__(self)</code> function is used to define the layers of the model while the <code>forward(self, x)</code> function is used to define the feedforward network. Do not forget the non-linear activation functions!!!</p>\n","\n","```python\n","class MyPyTorchModel(nn.Module):\n","    \n","    # Define layers here...\n","    def __init__(self):\n","        \n","        # Required to initialize the nn.Module\n","        super(MyPyTorchModel, self).__init__()\n","\n","        self.layer1 = nn.linear()\n","        self.layer2 = nn.linear()\n","        self.layer3 = nn.linear()\n","        self.layer4 = nn.linear()\n","        \n","    # Define feedforward here...\n","    def forward(self, x):\n","        \n","        x = self.layer1(x)\n","        x = torch.sigmoid(x)\n","        \n","        x = self.layer2(x)\n","        x = torch.sigmoid(x)\n","        \n","        x = self.layer3(x)\n","        x = torch.sigmoid(x)\n","        \n","        x = self.layer4(x)\n","        x = torch.sigmoid(x)\n","        \n","        return x\n","```\n","\n","<center><img width=100 src=\"https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/resources/tip.png\"></center>\n","\n","\n","<p>Make sure to pass <code>nn.Module</code> to the models's class. By doing so, your class inherits all required <b>PyTorch</b> methods and functions, which is essential to the implementation!</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SXsK0POTtn6f"},"outputs":[],"source":["#################################\n","# Define architecture (PyTorch) #\n","#################################\n","\n","# Import\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","\n","# Set seed\n","torch.manual_seed(42)\n","\n","# Define architecture\n","class FirstTorchModel(nn.Module):\n","\n","    def __init__(self):\n","\n","        super(FirstTorchModel, self).__init__()\n","\n","        # Define hidden layer (4 units)\n","        self.hidden_layer = # TODO\n","\n","        # Define output layer (1 unit)\n","        self.output_layer = # TODO\n","\n","    # Feedforward\n","    def forward(self, x):\n","\n","        # Hidden layer (Sigmoid activation)\n","        x = # TODO\n","        x = # TODO\n","\n","        # Output layer\n","        x = # TODO\n","        x = # TODO\n","\n","        return x"]},{"cell_type":"markdown","metadata":{"id":"D6UCiHDRtn6g"},"source":["<p>Before we can go any further, we must first initialise our model and define a few more key ingredients, namely:</p>\n","\n","<ul style=\"list-style-type:round\">\n","    <li>a loss function; and</li>\n","    <li>an optimizer &mdash; i.e., the method used to update our weights and biases based on the backpropagated error.</li>\n","</ul>\n","\n","<p>For the sake of this tutorial, we will stick to a common binary approach by using the <code><a href=\"https://pytorch.org/docs/stable/generated/torch.nn.BCELoss.html\">torch.nn.BCELoss (Binary Cross Entropy)</a></code> loss function in combination with the <code><a href=\"https://pytorch.org/docs/stable/optim.html\">torch.optim.SGD (Stochastic Gradient Descent)</a></code> optimizer. Run the code below in order to initialise the model and all required components!</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dbtl1yzNtn6g"},"outputs":[],"source":["#################\n","# Compile model #\n","#################\n","\n","# Initialise model\n","model = FirstTorchModel()\n","\n","# Define loss function (a.k.a. criterion)\n","criterion = # TODO\n","\n","# Define optimizer\n","optimizer = # TODO\n","\n","# Print overview\n","print(model)"]},{"cell_type":"markdown","metadata":{"id":"KOvpB25btn6g"},"source":["<center><img width=100 src=\"https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/resources/question.png\"></center>\n","\n","<p><center><b>Based on the architecture above, how many parameters (a.k.a. weights) will we be training?</b></center></p>\n","\n","\\begin{align}\n","params_{layer} = inputs \\times outputs + biases\n","\\end{align}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y8KkjPcBtn6g"},"outputs":[],"source":["########################\n","# How many parameters? #\n","########################\n","\n","# Hidden layer\n","params_hidden = # TODO\n","\n","# Output layer\n","params_output = # TODO\n","\n","# Total parameters\n","total_params = params_hidden + params_output\n","\n","print(f'>>> The model contains {total_params} parameters in total!')"]},{"cell_type":"markdown","metadata":{"id":"jNlC0u1wtn6g"},"source":["<p>As can be seen, our model contains a total of 17 trainable parameters (a.k.a. weights). Here is the calculation:</p>\n","\n","\\begin{align}\n","params_{hidden} = 2 \\times 4 + 4 \\;\\;\\;\\;\\;\\;\\;\\;\\;\\; params_{output} = 4 \\times 1 + 1\n","\\end{align}\n","\n","\\begin{align}\n","params_{total} = 12 + 5 = 17\n","\\end{align}\n","\n","<p>Akin to our <code>Perceptron</code> implementation, loading the model's class initialises the model's weights and bias units with random values. Let's take a look at these values!</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Rnqa9slQtn6g"},"outputs":[],"source":["############################\n","# Initial weights & biases #\n","############################\n","\n","# Hidden layer\n","print('>>> Hidden Layer:')\n","print(model.hidden_layer.weight)\n","print(model.hidden_layer.bias)\n","\n","# Output layer\n","print('\\n>>> Output Layer:')\n","print(model.output_layer.weight)\n","print(model.output_layer.bias)\n","\n","# Save initial weights and biases (not relevant!)\n","# For tutorial only!\n","w1_initial = torch.transpose(model.hidden_layer.weight.detach().clone(), 0, 1)\n","b1_initial = model.hidden_layer.bias.detach().clone()\n","w2_initial = torch.transpose(model.output_layer.weight.detach().clone(), 0, 1)\n","b2_initial = model.output_layer.bias.detach().clone()"]},{"cell_type":"markdown","metadata":{"id":"j4KWpMKatn6g"},"source":["<p>As we can see, the 17 parameters contained within our model are initialised and ready to go! As a result, we are now ready to load and prepare our dataset before training our model.</p><br>\n","\n","<center><img width=25% src=\"https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/week_2/images/ready.jpg\"/></center>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sIENz0Agtn6g"},"outputs":[],"source":["################################################\n","# Reload Iris dataset                          #\n","# https://archive.ics.uci.edu/ml/datasets/iris #\n","################################################\n","\n","# Import\n","import numpy as np\n","import pandas as pd\n","\n","# Load dataset (UCI)\n","df = pd.read_csv('https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data', header=None)\n","\n","# Shuffle\n","df = df.sample(frac=1., random_state=42).reset_index(drop=True)\n","\n","# Define features (X)\n","X = df.iloc[:, [0, 2]].values\n","\n","# Define targets (y)\n","y = df.iloc[:, 4].values\n","y = np.where(y == 'Iris-setosa', 0, 1).reshape((y.shape[0], 1))\n","\n","print('Shapes:')\n","print('X: {}'.format(X.shape))\n","print('y: {}'.format(y.shape))"]},{"cell_type":"markdown","metadata":{"id":"E5a7K3hptn6h"},"source":["<p>Lastly, before we can fit (or train) our model, we must define the following hyperparameters:</p>\n","\n","<ul style=\"list-style-type:round\">\n","    <li><code>epochs</code> &mdash; i.e., the number of iterations;</li>\n","    <li><code>batch_size</code> &mdash; the number of samples per gradient update.</li>\n","</ul>\n","\n","<p>For the sake of simplicity, we will train our model for 100 epochs and split our data into 6 batches &mdash; i.e., 25 samples per batch. By doing so, we can compute the loss and update the weights after every 25 samples, and therefore making the training process more efficient.</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yHyv2S8ztn6h"},"outputs":[],"source":["##########################\n","# Define hyperparameters #\n","##########################\n","\n","NUM_EPOCHS = 100\n","BATCH_SIZE = 25"]},{"cell_type":"markdown","metadata":{"id":"uyYmG4GBtn6h"},"source":["<center><img width=100 src=\"https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/resources/tip.png\"></center>\n","\n","<p>The <b>PyTorch</b> framework can provide you with useful utilities, such as <a href=\"https://pytorch.org/docs/stable/data.html\">torch.utils.data.TensorDataset</a> and <a href=\"https://pytorch.org/docs/stable/data.html#torch.utils.data.DataLoader\">torch.utils.data.DataLoader</a>, that will help you prepare and split your dataset into trainable batches!</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cMF9J4OGtn6h"},"outputs":[],"source":["###################\n","# Prepare dataset #\n","###################\n","\n","dataset = torch.utils.data.TensorDataset(torch.from_numpy(X).float(), torch.from_numpy(y).float())\n","train_loader = torch.utils.data.DataLoader(dataset, shuffle=False, batch_size=BATCH_SIZE)"]},{"cell_type":"markdown","metadata":{"id":"GNqniCKJtn6h"},"source":["<p>Finally, we can go ahead and train our model using the following steps:></p>\n","\n","<ol>\n","    <li>Sets the gradients to zero;</li>\n","    <li>Feed the data through the network;</li>\n","    <li>Compute the loss using the outputs from the model;</li>\n","    <li>Backpropagate the loss and compute the gradients; and</li>\n","    <li>Update the weights using the optimizer.</li>\n","</ol>"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"wUnGUzFItn6h"},"outputs":[],"source":["###############\n","# Train model #\n","###############\n","\n","torch_training_history = []\n","\n","# Training loop\n","for epoch in range(NUM_EPOCHS):\n","\n","    running_loss = torch.zeros(int(X.shape[0]/BATCH_SIZE))\n","\n","    for batch_id, batch in enumerate(train_loader):\n","\n","        data, target = batch\n","\n","        # Clear gradients\n","        # TODO\n","\n","        # Feedforward\n","        # TODO\n","\n","        # Compute loss\n","        loss = # TODO\n","\n","        # Backpropagate errors\n","        # TODO\n","\n","        # Update weights\n","        # TODO\n","\n","        # Append loss (epoch)\n","        running_loss[batch_id] = loss.item()\n","\n","    print('==========================================')\n","    print(f'>>> Epoch {epoch+1}')\n","    print('==========================================')\n","    print(f'>>> Batch 1: {running_loss[0]:.4}')\n","    print(f'>>> Batch 2: {running_loss[1]:.4}')\n","    print(f'>>> Batch 3: {running_loss[2]:.4}')\n","    print(f'>>> Batch 4: {running_loss[3]:.4}')\n","    print(f'>>> Batch 5: {running_loss[4]:.4}')\n","    print(f'>>> Batch 6: {running_loss[5]:.4}')\n","    print(f'\\n>>> Epoch:   {torch.mean(running_loss):.4}\\n')\n","\n","    # Append loss (training)\n","    torch_training_history.append(running_loss)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GMbweiTdtn6h"},"outputs":[],"source":["##################################\n","# Weights & biases post training #\n","##################################\n","\n","# Hidden layer\n","print('>>> Hidden Layer:')\n","print(model.hidden_layer.weight)\n","print(model.hidden_layer.bias)\n","\n","# Output layer\n","print('\\n>>> Output Layer:')\n","print(model.output_layer.weight)\n","print(model.output_layer.bias)"]},{"cell_type":"markdown","metadata":{"id":"0lSLNop-tn6h"},"source":["## 3.4 Implementation (Own)\n","\n","<p>For learning purposes, let's try to reproduce the results generated above without using all the tools provided by the <code>nn.Module</code>. High-level APIs, such as <b>Keras</b> or even <b>PyTorch</b>, often come at a cost &mdash; i.e., not knowing what's really happening behind the scenes! Below is a step-by-step guide on how to compute the feedforward pass based on the architecture presented in the previous section. <b>Keep in mind that this procedure must be repeated for each batch!</b></p>\n","\n","### 1. Compute weighted sums for the hidden layer\n","\n","\\begin{align}\n","\\LARGE\n","z_1 = W_1x + b_1\n","\\end{align}\n","\n","### 2. Activate hidden layer with sigmoid function\n","\n","<p>With the weighted sums $z_1$ at hand, we still need to activate the hidden neurons with a non-linear activation function. As mentioned earlier, we will be using the sigmoid function ($\\sigma$) throughout this example.</p>\n","\n","\\begin{align}\n","\\LARGE\n","\\sigma = \\frac{1}{1+e^{-x}}\n","\\end{align}\n","\n","<p>The output of our layer is then called $a_1$ and can be computed as follows:</p>\n","\n","\\begin{align}\n","\\LARGE\n","a_1 = \\sigma(z_1) = \\sigma(W_1x + b_1)\n","\\end{align}\n","\n","### 3. Compute weighted sum for the output layer\n","\n","\\begin{align}\n","\\LARGE\n","z_2 = W_2a_1 + b_2\n","\\end{align}\n","\n","### 4. Activate output layer with sigmoid function and generate predictions\n","\n","\\begin{align}\n","\\LARGE\n","a_2 = \\sigma(z_2) = \\sigma(W_2z_1 + b_2)\n","\\end{align}"]},{"cell_type":"markdown","metadata":{"id":"TJabXAjJtn6i"},"source":["<center><img width=100 src=\"https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/resources/tip.png\"></center>\n","\n","<p>As can be seen above, only two functions are required to compute the aforementioned forward pass. <b>Keep in mind that it is essential to use the activated outputs from the hidden layer &mdash; i.e., $a_1$ &mdash; as inputs for the output layer</b>.</p>\n","\n","<p>Let's define our <code>MyNeuralNetwork</code> class and compute the feedforward pass for our first batch using the step-by-step guide exposed above. To do so, we will start by splitting our training set into two batches containing 2 samples each.</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pur6JZ32tn6i"},"outputs":[],"source":["###############\n","# Get batches #\n","###############\n","\n","# DataLoader\n","loader_iter = iter(train_loader)\n","\n","# Batches\n","X_batch1, y_batch1 = next(loader_iter)\n","X_batch2, y_batch2 = next(loader_iter)\n","X_batch3, y_batch3 = next(loader_iter)\n","X_batch4, y_batch4 = next(loader_iter)\n","X_batch5, y_batch5 = next(loader_iter)\n","X_batch6, y_batch6 = next(loader_iter)\n","\n","# Shapes\n","print(X_batch1.shape, y_batch1.shape)\n","print(X_batch2.shape, y_batch2.shape)\n","print(X_batch3.shape, y_batch3.shape)\n","print(X_batch4.shape, y_batch4.shape)\n","print(X_batch5.shape, y_batch5.shape)\n","print(X_batch6.shape, y_batch6.shape)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uBMEQiyttn6i"},"outputs":[],"source":["##########################\n","# Define MyNeuralNetwork #\n","##########################\n","\n","# MyNeuralNetwork\n","class MyNeuralNetwork:\n","\n","    def __init__(self, weights1, bias1, weights2, bias2):\n","\n","        self.weights1 = weights1\n","        self.bias1 = bias1\n","        self.weights2 = weights2\n","        self.bias2 = bias2\n","\n","    def _sigmoid_activation(self, x):\n","        return # TODO\n","\n","    def _feedforward(self, x):\n","\n","        # Step 1\n","        z_1 = # TODO\n","\n","        # Step 2\n","        a_1 = # TODO\n","\n","        # Step 3\n","        z_2 = # TODO\n","\n","        # Step 4\n","        a_2 = # TODO\n","\n","        return z_1, a_1, z_2, a_2\n","\n","    def fit_one_batch(self, x):\n","\n","        # Feedforward\n","        self.z_1, self.a_1, self.z_2, self.a_2 = self._feedforward(x)\n","\n","# Initialise model\n","my_model = MyNeuralNetwork(\n","    weights1 = w1_initial,\n","    bias1 = b1_initial,\n","    weights2 = w2_initial,\n","    bias2 = b2_initial\n",")\n","\n","# Fit model\n","my_model.fit_one_batch(X_batch1)\n","\n","# Display predictions from output layer\n","print(f'>>> Predictions:\\n{my_model.a_2}')"]},{"cell_type":"markdown","metadata":{"id":"UCD-FSSmtn6i"},"source":["<p>Without any surprise, our model outputs five predictions &mdash; i.e., one for every sample in our batch. By comparing these outputs with the true labels &mdash; i.e., $y$ &mdash; we can compute the binary cross-entropy loss, or logarithmic loss, for (every batch) by using the following formula:</p>\n","\n","\\begin{align}\n","\\LARGE\n","H(p, y) = -(y \\; \\log(p) + (1-y) \\; \\log(1-p))\n","\\end{align}\n","\n","<p>where $p$ reprensents the model's prediction and $y$ the true label.</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fDAKJyDJtn6i"},"outputs":[],"source":["########################################\n","# Compare predictions with true labels #\n","########################################\n","\n","for p, y in zip(my_model.a_2, y_batch1):\n","    print(f'Pred.: {p[0]:.4} -> Target: {y[0]}')"]},{"cell_type":"markdown","metadata":{"id":"0T5RsQ5utn6i"},"source":["<p>Since our predictions aren't perfect, let's add the log loss function to our <code>MyNeuralNetwork</code> class and compute the error after every feedforward pass.</p>\n","\n","```python\n","    def _loss_computation(self, p, y):\n","            return torch.mean(-(y * torch.log(p) + (1-y) * torch.log(1-p)))\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"dhIFsUk4tn6i"},"outputs":[],"source":["##########################\n","# Define MyNeuralNetwork #\n","##########################\n","\n","# MyNeuralNetwork\n","class MyNeuralNetwork:\n","\n","    def __init__(self, weights1, bias1, weights2, bias2):\n","        self.weights1 = weights1\n","        self.bias1 = bias1\n","        self.weights2 = weights2\n","        self.bias2 = bias2\n","\n","    def _sigmoid_activation(self, x):\n","        return # TODO\n","\n","    def _loss_computation(self, p, y):\n","        return # TODO\n","\n","    def _feedforward(self, x):\n","\n","        # Step 1\n","        z_1 = # TODO\n","\n","        # Step 2\n","        a_1 = # TODO\n","\n","        # Step 3\n","        z_2 = # TODO\n","\n","        # Step 4\n","        a_2 = # TODO\n","\n","        return z_1, a_1, z_2, a_2\n","\n","    def fit_one_batch(self, x, y):\n","\n","        # Feedforward\n","        self.z_1, self.a_1, self.z_2, self.a_2 = self._feedforward(x)\n","\n","         # Loss\n","        self.loss = # TODO\n","\n","# Initialise model\n","my_model = MyNeuralNetwork(\n","    weights1 = w1_initial,\n","    bias1 = b1_initial,\n","    weights2 = w2_initial,\n","    bias2 = b2_initial\n",")\n","\n","# Fit model\n","my_model.fit_one_batch(X_batch1, y_batch1)\n","\n","# Loss\n","print(f'>>> Batch 1: {my_model.loss:.4}')"]},{"cell_type":"markdown","metadata":{"id":"_1xRNTcPtn6i"},"source":["<p>Because the loss is calculated at the end of every batch, our computation returns one error value for every sample. The overall error for a given batch is simply the mean of all loss values. Before moving on to backpropagation, let's validate the results of our manual computations by comparing them with the ones obtained using the <b>PyTorch</b> framework.</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"amWiPxTAtn6i"},"outputs":[],"source":["###################\n","# Own vs PyTorch  #\n","# 1st batch only! #\n","###################\n","\n","print(f'Our implementation:     {my_model.loss:.4}')\n","print(f'PyTorch implementation: {torch_training_history[0][0]:.4}')"]},{"cell_type":"markdown","metadata":{"id":"UZaVCyCWtn6n"},"source":["<center><img width=40% src=\"https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/week_2/images/worked.jpg\"/></center>"]},{"cell_type":"markdown","metadata":{"id":"eztCtITGtn6n"},"source":["## 3.4 Backpropagation &amp; Gradient Descent\n","\n","<p>Now that we are done training our first batch &mdash; i.e., we generated predictions and computed the model's error &mdash; we can now proceed with backpropagation &mdash; i.e., <b>a method used to efficiently compute the gradient of the error function with respect to the weights in artificial neural networks</b>. Gradient descent, on the other hand,  is <b>\"an optimization algorithm used to minimise some function by iteratively moving in the direction of steepest descent as defined by the negative of the gradient\"</b> (Machine Learning Glossary, 2017). Since the concepts of backpropagation and gradient descent can be considered rather complex, we will, in this tutorial, set our focus on the most essential ideas.</p>\n","\n","<table class=\"image\">\n","<center>\n","<caption align=\"bottom\">(Machine Learning Glossary, 2017)</caption>\n","<tr><td><img width=550 src='https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/week_2/images/gradient_descent.png'></td></tr>\n","</center>\n","</table>\n","\n","<p>From a visual standpoint, it is evident, when we look back at our model's architecture, that the output of a given layer is determined by the weights preceeding this layer. Hence, the resultant error must also be dependent from these weights. This is one of the main assumptions that needs to be made when dealing with gradient descent and the reason why backpropagation is used to optimise neural networks. By backpropagating the error through a network &mdash; i.e., from output to input &mdash; we can fine-tune the model's weights and biases based on their effect on the cost function.</p>\n","\n","## 3.5 Backpropagation (Step-by-Step)\n","\n","<p><center><font color=\"red\"><strong><i>Since the concepts of backpropagation and gradient descent are rather complex,<br>they will not be covered in this introductory lesson on artificial neural networks.</i></strong></font></center></p>\n","\n","### 1. Compute the error term $\\delta$ for the output layer\n","\n","\\begin{align}\n","\\LARGE\n","\\delta_{output} = H^{\\prime}(p,y) \\cdot \\sigma^{\\prime}(a_2)\n","\\end{align}\n","\n","\\begin{align}\n","H^{\\prime}(p,y) = -\\left ( \\frac{y}{p} - \\frac{1-y}{1-p} \\right ) \\;\\;\\;\\;\\;\\;\\;\\;\\;\\; \\sigma^{\\prime}(a_2) = a_2 \\cdot (1-a_2)\n","\\end{align}\n","\n","where $H^{\\prime}(p,y)$ represents the partial derivative of the cross-entropy loss function and $\\sigma^{\\prime}(a_2)$ the partial derivative of the sigmoid activation function at the output layer. Keep in mind that $a_2$ also represents the predictions made by the model. However, for the sake of simplicity, the variable $p$ is used to reprensent the prediction within the loss function.\n","\n","### 2. Compute the error term $\\delta$ for the hidden layer\n","\n","\\begin{align}\n","\\LARGE\n","\\delta_{hidden} = \\delta_{output}W_2 \\cdot \\sigma^{\\prime}(a_1)\n","\\end{align}\n","\n","where $\\delta_{output}W_2$ represents the weighted $\\delta_{output}$ and $\\sigma^{\\prime}(a_1)$ the partial derivative of the sigmoid activation function at the hidden layer.\n","\n","### 3. Update weights &rarr; $W_2$ and $b_2$\n","\n","\\begin{align}\n","\\LARGE\n","W_2 := W_2 - \\eta \\; (a_1 \\cdot \\delta_{output})\n","\\end{align}\n","\n","\\begin{align}\n","\\LARGE\n","b_2 := b_2 - \\eta \\; (\\delta_{output})\n","\\end{align}\n","\n","### 4. Update weights &rarr; $W_1$ and $b_1$\n","\n","\\begin{align}\n","\\LARGE\n","W_1 := W_1 - \\eta \\; (x \\cdot \\delta_{hidden})\n","\\end{align}\n","\n","\\begin{align}\n","\\LARGE\n","b_1 := b_1 - \\eta \\; (\\delta_{hidden})\n","\\end{align}\n","\n","<p>In order to be able to perform the backpropagation step presented above, we will be adding, besides the computations, the following functions to our <code>MyNeuralNetwork</code> class:</p>\n","\n","\n","```python\n","    def _sigmoid_derivative(self, p):\n","            return p * (1.0 - p)\n","\n","    def _loss_derivative(self, p, y):\n","        return (1 / y.shape[0]) * (-torch.divide(y, p) + torch.divide((1-y), (1-p)))\n","```"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eO8GyJY0tn6n"},"outputs":[],"source":["##########################\n","# Define MyNeuralNetwork #\n","##########################\n","\n","# MyNeuralNetwork\n","class MyNeuralNetwork:\n","\n","    def __init__(self, weights1, bias1, weights2, bias2, eta):\n","\n","        self.weights1 = weights1\n","        self.bias1 = bias1\n","        self.weights2 = weights2\n","        self.bias2 = bias2\n","        self.eta = eta\n","\n","    def _sigmoid_activation(self, x):\n","        return 1 / (1 + torch.exp(-x))\n","\n","    def _loss_computation(self, p, y):\n","        return torch.mean(-(y * torch.log(p) + (1-y) * torch.log(1-p)))\n","\n","    def _sigmoid_derivative(self, p):\n","        return p * (1.0 - p)\n","\n","    def _loss_derivative(self, p, y):\n","        return (1 / y.shape[0]) * (-torch.divide(y, p) + torch.divide((1-y), (1-p)))\n","\n","    def _feedforward(self, x):\n","\n","        # Step 1\n","        z_1 = torch.matmul(x, self.weights1) + self.bias1\n","\n","        # Step 2\n","        a_1 = self._sigmoid_activation(z_1)\n","\n","        # Step 3\n","        z_2 = torch.matmul(a_1, self.weights2) + self.bias2\n","\n","        # Step 4\n","        a_2 = self._sigmoid_activation(z_2)\n","\n","        return z_1, a_1, z_2, a_2\n","\n","    def _backpropagation(self, a_2, a_1, y):\n","\n","        # Output layer\n","        output_delta = torch.mul(self._loss_derivative(a_2, y), self._sigmoid_derivative(a_2))\n","\n","        # Hidden layer\n","        hidden_delta = torch.mul(torch.mul(output_delta, self.weights2.T), self._sigmoid_derivative(a_1))\n","\n","        return output_delta, hidden_delta\n","\n","    def fit(self, n_epochs, batches):\n","\n","        self.training_history = []\n","\n","        # Training loop\n","        for epoch in range(n_epochs):\n","\n","            running_loss = torch.zeros(int(X.shape[0]/BATCH_SIZE))\n","\n","            for batch_id, batch in enumerate(batches):\n","\n","                data, target = batch\n","\n","                # Feedforward\n","                z_1, a_1, z_2, a_2 = self._feedforward(data)\n","\n","                 # Loss\n","                loss = self._loss_computation(a_2, target)\n","\n","                # Backpropagation\n","                output_delta, hidden_delta = self._backpropagation(a_2, a_1, target)\n","\n","                # Update weights (Output layer)\n","                self.weights2 -= # TODO\n","                self.bias2 -= # TODO\n","\n","                # Update weights (Hidden layer)\n","                self.weights1 -= # TODO\n","                self.bias1 -= # TODO\n","\n","                # Append loss (epoch)\n","                running_loss[batch_id] = loss\n","\n","            print('==========================================')\n","            print(f'>>> Epoch {epoch+1}')\n","            print('==========================================')\n","            print(f'>>> Batch 1: {running_loss[0]:.4}')\n","            print(f'>>> Batch 2: {running_loss[1]:.4}')\n","            print(f'>>> Batch 3: {running_loss[2]:.4}')\n","            print(f'>>> Batch 4: {running_loss[3]:.4}')\n","            print(f'>>> Batch 5: {running_loss[4]:.4}')\n","            print(f'>>> Batch 6: {running_loss[5]:.4}')\n","            print(f'\\n>>> Epoch:   {torch.mean(running_loss):.4}\\n')\n","\n","            # Append loss (training)\n","            self.training_history.append(running_loss)\n","\n","\n","# Initialise model\n","my_model = MyNeuralNetwork(\n","    weights1 = w1_initial,\n","    bias1 = b1_initial,\n","    weights2 = w2_initial,\n","    bias2 = b2_initial,\n","    eta=0.1\n",")"]},{"cell_type":"markdown","metadata":{"id":"FJ7HJj7qtn6o"},"source":["<p>Finally, we are now ready to train/fit our model.</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rvaWCMz3tn6o"},"outputs":[],"source":["##################\n","# Initial values #\n","##################\n","\n","print('>>> Hidden layer:')\n","print(my_model.weights1)\n","print(my_model.bias1)\n","\n","print('\\n>>>> Output layer:')\n","print(my_model.weights2)\n","print(my_model.bias2)"]},{"cell_type":"code","execution_count":null,"metadata":{"scrolled":true,"id":"A__1zC5xtn6o"},"outputs":[],"source":["#########\n","# Train #\n","#########\n","\n","# Dataset\n","batches = [\n","    [X_batch1, y_batch1],\n","    [X_batch2, y_batch2],\n","    [X_batch3, y_batch3],\n","    [X_batch4, y_batch4],\n","    [X_batch5, y_batch5],\n","    [X_batch6, y_batch6]\n","]\n","\n","# Fit\n","my_model.fit(n_epochs=NUM_EPOCHS, batches=batches)"]},{"cell_type":"markdown","metadata":{"id":"FqTe3S87tn6o"},"source":["<p>We are now done training our model. Once again, we can validate the results of our manual computations by comparing them with the ones obtained using the <b>PyTorch</b> framework. Please note that a small variation is expected due to disprencies between both implementations &mdash; e.g., the implementation of the loss function and the optimizer &mdash; as well as floating point precision.</p>"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5UouQgTMtn6o"},"outputs":[],"source":["##################\n","# Own vs PyTorch #\n","##################\n","\n","print(f'Our implementation:     {torch.mean(my_model.training_history[-1])}')\n","print(f'PyTorch implementation: {torch.mean(torch_training_history[-1])}')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_O0IenKJtn6o"},"outputs":[],"source":["############\n","# Training #\n","# Overview #\n","############\n","\n","import matplotlib.pyplot as plt\n","\n","plt.plot([torch.mean(epoch) for epoch in torch_training_history], label='PyTorch implementation')\n","plt.plot([torch.mean(epoch) for epoch in my_model.training_history], label='Our implementation')\n","\n","plt.title('Training Overview')\n","plt.xlabel('Epoch')\n","plt.ylabel('BCELoss')\n","\n","plt.legend()\n","plt.grid()\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"AwSxx9YUtn6o"},"source":["<center><img width=60% src=\"https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/week_2/images/done.jpg\"/></center>"]},{"cell_type":"markdown","metadata":{"id":"cYo6e0Kftn6o"},"source":["<ul style=\"list-style-type:round\">\n","<i>\n","    <li>Lane, H., Howard, C., & Hapke, H.M. (2019). Natural Language Processing in Action. Shelter Island, NY: Manning Publications Co.</li>\n","    <li>Loy, J. (2019). Neural Network Projects with Python. Birgmingham, UK: Packt Publishing Ltd.</li>\n","    <li>Machine Learning Glossary. (2017). Gradient Descent. Retrieved from https://ml-cheatsheet.readthedocs.io/en/latest/gradient_descent.html.</li>\n","    <li>Patterson, J., & Gibson, A. (2017). Deep learning. Sebastopol, CA: O’Reilly Media, Inc.</li>\n","    <li>Raschka, S., &amp; Mirjalili, V. (2017). Python Machine Learning (2nd ed.). Birgmingham: Packt Publishing Ltd.</li>\n","</i>\n","</ul>"]}],"metadata":{"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}