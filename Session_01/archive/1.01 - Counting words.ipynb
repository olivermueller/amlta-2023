{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"},"colab":{"provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"uc-DY3aSRYzs"},"source":["# <font color=\"#003660\">Applied Machine Learning for Text Analysis (M.184.5331)</font>\n"]},{"cell_type":"markdown","metadata":{"id":"HRduPPJcRbIZ"},"source":["# <font color=\"#003660\">Week 1: Basics of Natural Language Processing</font>"]},{"cell_type":"markdown","metadata":{"id":"P80mDwREn1EN"},"source":["# <font color=\"#003660\">Notebook 1: Counting Words</font>\n","\n","<center><br><img width=256 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/dag.png\"/><br></center>\n","\n","<p>\n","<center>\n","<div>\n","    <font color=\"#085986\"><b>By the end of this lesson, you will be able to...</b><br><br>\n","        ... load text data from files and databases,<br>\n","        ... conduct basic NLP preprocessing (e.g., tokenization, stopword removal, stemming, lemmatization), and<br>\n","        ... calculate corpus statistics (esp. word frequencies).</font>\n","</div>\n","</center>\n","</p>"]},{"cell_type":"markdown","metadata":{"id":"ZcE9JGeLn1EO"},"source":["# Import packages\n","\n","As always, we first need to load a number of required Python packages:\n","- `pandas` provides high-performance, easy-to-use data structures and data analysis tools.\n","- `NLTK` is a well-known package for building Python programs to work with human language data."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"gxDlLxaIn1EP"},"source":["import pandas as pd\n","import nltk\n","from nltk.stem import PorterStemmer\n","from nltk.stem import WordNetLemmatizer\n","from nltk.corpus import stopwords"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5k6bQI2lBku7"},"source":["To work with the `NLTK` package, you also need to download some additional data (e.g., stopword lists)."]},{"cell_type":"code","metadata":{"id":"FIx7k3tsn-JB"},"source":["nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","nltk.download('omw-1.4')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"kVRbkD1rn1EP"},"source":["# Load documents\n","For this first example, we create 7 documents by hand, each consisting of only one sentence. The documents are stored in a list called `corpus`."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"IhXCC-7-n1EQ"},"source":["corpus = [\n","    \"Hello World.\",\n","    \"How are you today?\",\n","    \"The world is nice.\",\n","    \"The weather is also nice.\",\n","    \"Yesterday, the weather was also nice.\",\n","    \"I own two bicycles.\",\n","    \"I love to ride my bicycle.\"\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fa2zBIOen1EQ"},"source":["corpus"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"v--VH7vCn1ER"},"source":["corpus[5]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"6b-O5zTWn1ER"},"source":["# Preprocess documents\n","As many natural language processing techniques process sequences of words, we have to tokenize the documents, that is, break them up into sequences of words. To do this, we iterate through each document in the corpus and transform them into lists of tokens. The result is a list of lists."]},{"cell_type":"code","metadata":{"id":"sM-ZIMbgn1ER"},"source":["docs_tokenized = []\n","for doc in corpus:\n","    tokens = nltk.word_tokenize(doc)\n","    docs_tokenized.append(tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_-Lhhfbhn1ES"},"source":["docs_tokenized"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ojCfBrNin1ET"},"source":["# Counting words\n","We are now ready to analyze the documents. We use NLTK's `FreqDist` method (http://www.nltk.org/api/nltk.html?highlight=freqdist#nltk.probability.FreqDist) to create a word frequency distribution from the preprocessed documents. The method expects a single list of words as the input parameter. Hence, we have to first flatten our list of lists (https://stackoverflow.com/questions/952914/making-a-flat-list-out-of-list-of-lists-in-python)."]},{"cell_type":"code","metadata":{"id":"AgtnvsZOn1ET"},"source":["flat_docs_tokenized_1 = [item for sublist in docs_tokenized for item in sublist]\n","freq_dist_1 = nltk.FreqDist(flat_docs_tokenized_1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NWI-rbNWn1ET"},"source":["Print the results."]},{"cell_type":"code","metadata":{"id":"k5wgy3s_n1ET"},"source":["freq_dist_1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jsOMoBGln1EU"},"source":["When plotting the results, we observe some unwanted results. For example, the tokens `The` and `the` are counted as separate words."]},{"cell_type":"code","metadata":{"id":"D91B4kznn1EU"},"source":["freq_dist_1.plot(30, cumulative=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HuMGsgUXn1EU"},"source":["# More preprocessing\n","To address the above problem, we go back to preprocessing and transform all tokens to lowercase."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"wC-uw7tVn1EU"},"source":["docs_tokenized_lower = []\n","for doc in docs_tokenized:\n","    tokens = []\n","    for token in doc:\n","        tokens.append(token.lower())\n","    docs_tokenized_lower.append(tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jXTk7xbZn1EV"},"source":["docs_tokenized_lower"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ppb_TIgvn1EW"},"source":["# Recounting words\n","And we create and plot the frequency distribution again."]},{"cell_type":"code","metadata":{"id":"s7_Eothvn1EW"},"source":["flat_docs_tokenized_2 = [item for sublist in docs_tokenized_lower for item in sublist]\n","freq_dist_2 = nltk.FreqDist(flat_docs_tokenized_2)\n","freq_dist_2.plot(30, cumulative=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PjstVCzCn1EW"},"source":["Now, `the` is only counted once (3 occurences). But there are still problems. For example, `bicycle` and `bicycles`."]},{"cell_type":"markdown","metadata":{"id":"NQMoSNoKn1EW"},"source":["# More preprocessing\n","We can fix this issue with\n","- stemming (https://en.wikipedia.org/wiki/Stemming) or"]},{"cell_type":"code","metadata":{"id":"36R_GwjRn1EX"},"source":["stemmer = PorterStemmer()\n","print(stemmer.stem('bicycles'))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gSIHXUhkn1EX"},"source":["- lemmatization (https://en.wikipedia.org/wiki/Lemmatisation).\n","\n","`NLTK`'s WordNetLemmatizer reduces tokens to their dictionary form. We have to tell the lemmatization method which part of speech (https://en.wikipedia.org/wiki/Part_of_speech) the word to be lemmatized is."]},{"cell_type":"code","metadata":{"id":"jzWYg-jun1EX"},"source":["lemmatizer = WordNetLemmatizer()\n","print(lemmatizer.lemmatize('bicycles','n')) # n = noun, v = verb, ..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"u278onMQn1EX"},"source":["To lemmatize all tokens in all documents, we again iterate through our list of documents and call the lemmatize method on each word. Unfortunately, the results are somewhat surprising, as we don't know which part of speech the tokens are (see, e.g., `was` -> `wa`). For now, we ignore these mistakes. We will learn how to consider part of speech later."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"yx5EYi0Wn1EX"},"source":["docs_tokenized_lower_lemmatized = []\n","for doc in docs_tokenized_lower:\n","    tokens = []\n","    for token in doc:\n","        tokens.append(str(lemmatizer.lemmatize(token, 'n')))\n","    docs_tokenized_lower_lemmatized.append(tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KigZtpMxn1EY"},"source":["docs_tokenized_lower_lemmatized"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"YHSLpYdmn1EZ"},"source":["# Recounting words, again"]},{"cell_type":"code","metadata":{"id":"M05XYuOPn1EZ"},"source":["flat_docs_tokenized_3 = [item for sublist in docs_tokenized_lower_lemmatized for item in sublist]\n","freq_dist_3 = nltk.FreqDist(flat_docs_tokenized_3)\n","freq_dist_3.plot(30, cumulative=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jPbxa3gon1EZ"},"source":["Now, `bicycle` appears only once in the results. But we see some words which are not very informative for our analysis, e.g., `the`, `is`, or `.`."]},{"cell_type":"markdown","metadata":{"id":"F9_xSM4hn1EZ"},"source":["# Again, back to preprocessing\n","One typically removes these so-called stopwords before mining the texts. `NLTK` provides a list of standard English stopwords."]},{"cell_type":"code","metadata":{"id":"ozIslTwun1EZ"},"source":["stopwords.words('english')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HtX3rTK8n1EZ"},"source":["Again, we iterate through our list of documents and only extract tokens that are not stopwords and alphanumeric (no punctuation or special characters)."]},{"cell_type":"code","metadata":{"collapsed":true,"id":"NqweAmeEn1EZ"},"source":["docs_tokenized_lower_lemmatized_reduced = []\n","for doc in docs_tokenized_lower_lemmatized:\n","    tokens = []\n","    for token in doc:\n","        if token.isalpha() and token not in stopwords.words('english'):\n","            tokens.append(token)\n","    docs_tokenized_lower_lemmatized_reduced.append(tokens)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DvQAL0m7n1Ea"},"source":["docs_tokenized_lower_lemmatized_reduced"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"GtDR_1Inn1Ea"},"source":["# Plot the final results"]},{"cell_type":"code","metadata":{"id":"Qw9_cxVtn1Ea"},"source":["flat_docs_tokenized_4 = [item for sublist in docs_tokenized_lower_lemmatized_reduced for item in sublist]\n","freq_dist_4 = nltk.FreqDist(flat_docs_tokenized_4)\n","freq_dist_4.plot(30, cumulative=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gGyCRO_PGMxH"},"source":["Now we are happy!"]},{"cell_type":"code","source":[],"metadata":{"id":"-xM2FPf8lJ7K"},"execution_count":null,"outputs":[]}]}