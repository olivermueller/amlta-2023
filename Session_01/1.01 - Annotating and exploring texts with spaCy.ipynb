{"cells":[{"cell_type":"markdown","metadata":{"id":"DqqfUmU7Rsk_"},"source":["# <font color=\"#003660\">Applied Machine Learning for Text Analysis (M.184.5331)</font>\n"]},{"cell_type":"markdown","metadata":{"id":"LTr0oOHpRuwL"},"source":["# <font color=\"#003660\">Week 1: Introduction to Natural Language Processing</font>"]},{"cell_type":"markdown","metadata":{"id":"q21pvAXnqXAF"},"source":["# <font color=\"#003660\">Notebook 1: Annotating and Exploring Texts with spaCy</font>\n","\n","<center><br><img width=256 src=\"https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/resources/dag.png\"/><br></center>\n","\n","<p>\n","<center>\n","<div>\n","    <font color=\"#085986\"><b>By the end of this lesson, you will be able to...</b><br><br>\n","        ... perform NLP preprocessing with Spacy.</font>\n","</div>\n","</center>\n","</p>"]},{"cell_type":"markdown","metadata":{"id":"xtu_FbD9qXAJ"},"source":["# Import packages\n","\n","As always, we first need to load a number of required Python packages:\n","- `pandas` provides high-performance, easy-to-use data structures and data analysis tools.\n","- `NLTK` is a leading platform for building Python programs to work with human language data.\n","- `Spacy` is a library for for industrial-strength natural language processing.\n","- `SQLAlchemy`, together with `pymysql`, allows to communicate with SQL databases.\n","- `getpass` provides function to safely enter passwords.\n","- `altair` is a visualization library based on the grammar of graphics."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9221,"status":"ok","timestamp":1696488332500,"user":{"displayName":"Oliver Mueller","userId":"12717968064814035358"},"user_tz":-120},"id":"Gtb_6VbuwUOU","outputId":"2e7a580b-aca1-474e-b7f7-4ee93b3b6d85"},"outputs":[],"source":["# Install missing packages\n","!pip install pymysql"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"BAE0ESOwqXAK"},"outputs":[],"source":["import pandas as pd\n","import nltk\n","import spacy\n","from spacy import displacy\n","from sqlalchemy import create_engine, text\n","import getpass\n","import altair as alt"]},{"cell_type":"markdown","metadata":{"id":"MUMZ6WKIV6Zc"},"source":["# Quick tour of spaCy"]},{"cell_type":"markdown","metadata":{"id":"dVRpEHMh6S1y"},"source":["spaCy is an open-source library for Natural Language Processing (NLP) in Python. It helps you build NLP applications that process and understand large volumes of unstructured text. One of the main features of spaCy are linguistic annotations that give you insights into a text’s grammatical structure (e.g., word order, types of words, parts of speech, grammatical roles and relations).\n","\n","At the center of spaCy is the processing pipeline, an object which is usually called `nlp`. The pipeline is build on top of a language-specific machine learning model and a set of handcrafted rules.\n","\n","The pipeline contains different components, each specialized for a specific NLP task.\n","\n","[More...](https://spacy.io/usage/spacy-101#whats-spacy)\n","\n","<center><br><img src=\"\n","https://d33wubrfki0l68.cloudfront.net/3ad0582d97663a1272ffc4ccf09f1c5b335b17e9/7f49c/pipeline-fde48da9b43661abcdf62ab70a546d71.svg\"/><br></center>"]},{"cell_type":"markdown","metadata":{"id":"0L8JP2C08tOO"},"source":["The following code creates a pipeline based on the `en_core_web_sm` model and assigns it to the variable `nlp`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TcBsfj0ZVl_F"},"outputs":[],"source":["nlp = spacy.load(\"en_core_web_sm\")"]},{"cell_type":"markdown","metadata":{"id":"Dq50IrBq84rg"},"source":["Let's feed the `nlp` object with a simple sentence. When you process a text with the `nlp` object, spaCy outputs a `doc` object. The `doc` lets you access information about the text in a structured way."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lb6ohjGMV-Pu"},"outputs":[],"source":["doc = nlp(u\"Yesterday, I went to five pubs in Oxford. It was fun.\")"]},{"cell_type":"markdown","metadata":{"id":"2wJ3wY5oXJdy"},"source":["The first component of every spaCy pipeline is the `tokenizer`, which segments an unstructured text into words, punctuation, and so on. These `tokens` are the main contents of the `doc` object. [More...](https://spacy.io/usage/spacy-101#annotations-token)\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1696488356141,"user":{"displayName":"Oliver Mueller","userId":"12717968064814035358"},"user_tz":-120},"id":"DhHHoEUaWPpG","outputId":"468ec2e5-74c2-4a85-b29b-283efbe2e5f1"},"outputs":[],"source":["for token in doc:\n","  print(token.text)"]},{"cell_type":"markdown","metadata":{"id":"FdP8BASnXLF1"},"source":["The tokens contain many useful attributes.\n","\n","\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1696489002360,"user":{"displayName":"Oliver Mueller","userId":"12717968064814035358"},"user_tz":-120},"id":"mJ-ahxi-VL3_","outputId":"b8728854-eb6c-4542-8610-f7301bd5eb2c"},"outputs":[],"source":["print(doc[0].text)\n","print(doc[0].i)\n","print(doc[3].idx)\n","print(doc[1].is_sent_start)\n","print(doc[9].is_sent_end)"]},{"cell_type":"markdown","metadata":{"id":"wGumQjzga3eC"},"source":["Spacy also recognized that the doc consists of two sentences. We can access these `sents` just like we can access tokens."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":245,"status":"ok","timestamp":1696488378955,"user":{"displayName":"Oliver Mueller","userId":"12717968064814035358"},"user_tz":-120},"id":"ODeLOpGubTAo","outputId":"e483c757-8124-4f62-a6c4-e8e2f564e4e9"},"outputs":[],"source":["doc.sents"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":256,"status":"ok","timestamp":1696488388437,"user":{"displayName":"Oliver Mueller","userId":"12717968064814035358"},"user_tz":-120},"id":"HPR-hBXnbFQU","outputId":"9d4772f7-665a-4979-8b0e-0786adaa156e"},"outputs":[],"source":["list(doc.sents)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":246,"status":"ok","timestamp":1696488403984,"user":{"displayName":"Oliver Mueller","userId":"12717968064814035358"},"user_tz":-120},"id":"dWgISW2Klo6C","outputId":"76b295d7-1914-4fa9-83f2-b60c72f84db5"},"outputs":[],"source":["list(doc.sents)[1]"]},{"cell_type":"markdown","metadata":{"id":"r5scljWiVMEI"},"source":["We can also iterate through all tokens of a doc and access their attributes. For example, we can access the `lemma` of each token. [More...](https://spacy.io/usage/linguistic-features#lemmatization)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1696489200527,"user":{"displayName":"Oliver Mueller","userId":"12717968064814035358"},"user_tz":-120},"id":"jk9gJUiqWTpt","outputId":"29d55286-d414-4241-8cb0-828582d3ed27"},"outputs":[],"source":["for token in doc:\n","  print(f\"{token.text} -> {token.lemma_}\")"]},{"cell_type":"markdown","metadata":{"id":"ShidHKjEXMzt"},"source":["`Part-of-speech` tagging is the process of marking up a word in a text as corresponding to a particular part of speech, based on both its definition and its context. Examples of POS include words as nouns, verbs, adjectives, adverbs, etc. [More](https://spacy.io/usage/linguistic-features#pos-tagging\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1696489282120,"user":{"displayName":"Oliver Mueller","userId":"12717968064814035358"},"user_tz":-120},"id":"0J2uHwQHXeV3","outputId":"3b1ba97e-3551-4ea6-9402-087df73f9be3"},"outputs":[],"source":["for token in doc:\n","  print(f\"{token.text} -> {token.pos_} ({spacy.explain(token.pos_)})\")"]},{"cell_type":"markdown","metadata":{"id":"cq5wwmmFXilm"},"source":["We can also recognize so-called named entities. A `named entity` is a “real-world object” that’s assigned a name – for example, a person, a country, or  a product. spaCy can recognize various types of named entities in a document. [More...](https://spacy.io/usage/linguistic-features#named-entities)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":262,"status":"ok","timestamp":1696488433807,"user":{"displayName":"Oliver Mueller","userId":"12717968064814035358"},"user_tz":-120},"id":"Jc2yurJiXlw_","outputId":"1d5c0940-b0df-4cff-95bc-7c38b7b98459"},"outputs":[],"source":["for ent in doc.ents:\n","  print(ent.text, \"->\", ent.label_, \"(\", spacy.explain(ent.label_), \")\")"]},{"cell_type":"markdown","metadata":{"id":"AIJjj_ZGbwD9"},"source":["Spacy also has some nice visualization features..."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":52},"executionInfo":{"elapsed":230,"status":"ok","timestamp":1696488438077,"user":{"displayName":"Oliver Mueller","userId":"12717968064814035358"},"user_tz":-120},"id":"A265ndTKcbFl","outputId":"240dabc0-9200-4406-cbdb-49b5bba31e36"},"outputs":[],"source":["displacy.render(doc, style='ent', jupyter=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":440},"executionInfo":{"elapsed":260,"status":"ok","timestamp":1696488445773,"user":{"displayName":"Oliver Mueller","userId":"12717968064814035358"},"user_tz":-120},"id":"3ZLwPlppb9gD","outputId":"59a3fa28-8d59-43c5-bf8a-43b8a3fdc191"},"outputs":[],"source":["displacy.render(list(doc.sents)[0], style='dep', jupyter=True)"]},{"cell_type":"markdown","metadata":{"id":"D3JZNGZtpaEf"},"source":["# Exploring song lyrics\n"]},{"cell_type":"markdown","metadata":{"id":"4PhOPcrmqXAM"},"source":["## Load documents\n","This time, we load our data from a MySQL database. For security reasons, we don't store the database credentials here; please have a look at Panda how to get them."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":389},"collapsed":true,"executionInfo":{"elapsed":33916,"status":"ok","timestamp":1665671566478,"user":{"displayName":"Oliver Mueller","userId":"12717968064814035358"},"user_tz":-120},"id":"aV5f4froqXAM","outputId":"4ff8fc99-5485-41f6-d52c-3f2dff9f5da0"},"outputs":[],"source":["# Get credentials\n","user = input(\"Username: \")\n","passwd = getpass.getpass(\"Password: \")\n","server = input(\"Server: \")\n","db = input(\"Database: \")\n","\n","# Create an engine instance (SQLAlchemy)\n","engine = create_engine(\"mysql+pymysql://{}:{}@{}/{}\".format(user, passwd ,server, db))\n","\n","# Define SQL query\n","sql_query = \"SELECT * FROM BillboardLyrics\"\n","\n","# Query dataset (pandas)\n","corpus = pd.DataFrame(engine.connect().execute(text(sql_query)))\n","\n","# Sample\n","corpus.head()"]},{"cell_type":"markdown","metadata":{"id":"EPv6koXpqXAQ"},"source":["## Preprocess documents\n","Tokenization, stopword removal and lemmatization in one go."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VqTCwDpVY22T"},"outputs":[],"source":["docs_prep = corpus.to_dict(\"records\")\n","for i, entry in enumerate(docs_prep):\n","  if entry[\"Lyrics\"]:\n","    doc = nlp(entry[\"Lyrics\"])\n","    tokens_prep = []\n","    for token in doc:\n","      if token.is_alpha and not token.is_stop:\n","        tokens_prep.append(token.lemma_)\n","    entry[\"Lyrics_prep\"] = tokens_prep\n","  else:\n","    entry[\"Lyrics_prep\"] = []"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1139,"status":"ok","timestamp":1665671859977,"user":{"displayName":"Oliver Mueller","userId":"12717968064814035358"},"user_tz":-120},"id":"l7QIP7RiZoLS","outputId":"6d95db8f-69fa-4b96-c614-c7a459721370"},"outputs":[],"source":["docs_prep[42]"]},{"cell_type":"markdown","metadata":{"id":"uXXYgWwBqXAT"},"source":["## Counting words\n","We seperately count words for each condition, that is, for each year. We have to do this \"by hand\" and iterate through all docs and tokens and increase the token count for the respective condition."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"PPKYDzHDqXAU"},"outputs":[],"source":["cfreq = nltk.ConditionalFreqDist()\n","\n","for doc in docs_prep:\n","  for token in doc[\"Lyrics_prep\"]:\n","    condition = doc[\"Year\"]\n","    cfreq[condition][token] += 1"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1665671886621,"user":{"displayName":"Oliver Mueller","userId":"12717968064814035358"},"user_tz":-120},"id":"6DB3itP_7dze","outputId":"a8ba9481-6be9-43f7-9144-cdfaf17fc181"},"outputs":[],"source":["cfreq[2010]"]},{"cell_type":"markdown","metadata":{"id":"34QLgxiTKlVN"},"source":["## Time series of word counts"]},{"cell_type":"markdown","metadata":{"id":"vi0AhH7mqXAV"},"source":["For all years between 1965 and 2015, get the frequency of the word \"money\"."]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"PKo19RFMqXAX"},"outputs":[],"source":["word = u\"money\"\n","years = range(1965,2016)\n","occurences = []\n","for year in years:\n","  occurences.append(cfreq[year][word])"]},{"cell_type":"markdown","metadata":{"id":"EZYCBOauK4oK"},"source":["Merge the years and the word occurcences in one dataframe."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uG4vql4MK3rM"},"outputs":[],"source":["timeseries = pd.DataFrame(list(zip(years, occurences)),columns=['years','count'])\n","timeseries['years'] = pd.to_datetime(timeseries['years'], format='%Y')"]},{"cell_type":"markdown","metadata":{"id":"_UGGmV3kqXAX"},"source":["Plot the time series."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":369},"executionInfo":{"elapsed":4,"status":"ok","timestamp":1665671973632,"user":{"displayName":"Oliver Mueller","userId":"12717968064814035358"},"user_tz":-120},"id":"IPP0bsYaqXAY","outputId":"3000f482-e34a-47fc-8578-3cdd7952ba8a"},"outputs":[],"source":["alt.Chart(timeseries).mark_line().encode(\n","    x='years',\n","    y='count'\n",").interactive()"]}],"metadata":{"colab":{"provenance":[{"file_id":"15KO9tURVGVmKzyjQ1nWhE6x_211LG8MC","timestamp":1634290705642}]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
