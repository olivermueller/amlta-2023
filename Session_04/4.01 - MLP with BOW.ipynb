{"cells":[{"cell_type":"markdown","metadata":{"id":"LJrKCbjZsqpo"},"source":["# <font color=\"#003660\">Applied Machine Learning for Text Analysis (M.184.5331)</font>\n"]},{"cell_type":"markdown","metadata":{"id":"V-DU0hkyVyPi"},"source":["# <font color=\"#003660\">Week 4: From MLP over RNN to LSTM</font>"]},{"cell_type":"markdown","metadata":{"id":"mhy42GjRV3ON"},"source":["# <font color=\"#003660\">Notebook 1: MLP with BOW</font>\n","\n","<center><br><img width=256 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/dag.png\"/><br></center>\n","\n","<p>\n","<center>\n","<div>\n","    <font color=\"#085986\"><b>By the end of this lesson, you ...</b><br><br>\n","        ... understand the idea behind simple neural networks, and <br>\n","        ... are able to transform text data so that it can be processed by a neural networks.\n","    </font>\n","</div>\n","</center>\n","</p>"]},{"cell_type":"markdown","metadata":{"id":"EpN_2wlmiMcB"},"source":["# What is a Neural Network?"]},{"cell_type":"markdown","metadata":{"id":"ijhPAVIhiPgK"},"source":["A neural network takes an input vector of *m* variables *X* = (X1, X2, ...,Xm) and learns a nonlinear function *f(X*) to predict the response *Y*. The figure below shows a single neuron with inputs, weights, aggregation function, activation function, and output."]},{"cell_type":"markdown","metadata":{"id":"7Yn2txmElndz"},"source":["<br><img width=512 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/perceptron.png\"/><br>\n","<center>Source: Raschka (2021)</center>"]},{"cell_type":"markdown","metadata":{"id":"gRx4t8pGlnky"},"source":["A neural network with just a single neuron is not more powerful than an ordinary linear or logistic regression. The high predictive power of modern neural networks comes from stacking multiple layers of neurons so that the outputs of one layer are the inputs of the next."]},{"cell_type":"markdown","metadata":{"id":"POEjvjtAlxJG"},"source":["<br><img width=256 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/nn_w_layers_a_labels.png\"/><br>"]},{"cell_type":"markdown","metadata":{"id":"hfAsyn2Boa6l"},"source":["The figure below illustrates how the weights of a neural network with multiple layers can be learned. Initially, all weights are random numbers. Predictions are computed by applying the above specified transformations node by node, layer by layer. This is called the *forward pass*. Once a prediction is computed, the loss function compares the prediction to the true value of the target and calculates a loss score. The optimizer takes the loss score as a feedback signal to update the weights of the layers in a direction that will lower the loss score for the current example. This adjustment is performed by applying the *backpropagation* algorithm."]},{"cell_type":"markdown","metadata":{"id":"hroW765DqVIZ"},"source":["<br><img width=512 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/big_picture.png\"/><br>\n","<center>Source: Chollet (2021)</center>"]},{"cell_type":"markdown","metadata":{"id":"C6vVpwIFsqps"},"source":["# Import packages\n","\n","As always, we first need to load a number of required Python packages:\n","- `pandas` provides high-performance, easy-to-use data structures and data analysis tools.\n","- `numpy` is a library adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\n","- `SQLAlchemy`, together with `pymysql`, allows to communicate with SQL databases.\n","- `getpass` provides function to safely enter passwords.\n","- `sklearn` is a free software machine learning library for the Python programming language.\n","- `tensorflow` is an end-to-end open source platform for machine learning, especially deep learning.\n","- `matplotlib` is a plotting library for the Python programming language and its numerical mathematics extension NumPy\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5NpTS4Z2Vvzy"},"outputs":[],"source":["# Install packages\n","!pip install pymysql"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mMrhkr83sqpt"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sqlalchemy import create_engine, text\n","import getpass\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import TextVectorization\n","from sklearn import metrics\n","import matplotlib.pyplot as plt"]},{"cell_type":"markdown","metadata":{"id":"UpLf4XJy7fos"},"source":["Check if we are running on GPU."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qDOQCBX17fLW"},"outputs":[],"source":["tf.config.experimental.list_physical_devices('GPU')"]},{"cell_type":"markdown","metadata":{"id":"jZd82t53sqpu"},"source":["# Load documents"]},{"cell_type":"markdown","metadata":{"id":"IsrCafxksqpv"},"source":["As always, we load our data from a MySQL database. For security reasons, we don't store the database credentials here; please have a look at Panda to get them."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vSIGfKcXsqpv"},"outputs":[],"source":["# Get credentials\n","user = input(\"Username: \")\n","passwd = getpass.getpass(\"Password: \")\n","server = input(\"Server: \")\n","db = input(\"Database: \")\n","\n","# Create an engine instance (SQLAlchemy)\n","engine = create_engine(\"mysql+pymysql://{}:{}@{}/{}\".format(user, passwd ,server, db))\n","\n","# Define SQL query\n","sql_query = \"SELECT * FROM WineDataset\"\n","\n","# Query dataset (pandas)\n","corpus = pd.DataFrame(engine.connect().execute(text(sql_query)))"]},{"cell_type":"markdown","metadata":{"id":"LoihghYqIQun"},"source":["Display `shape` of the data."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7NsIQrLiSEak"},"outputs":[],"source":["corpus.shape"]},{"cell_type":"markdown","metadata":{"id":"CureExnsIS-p"},"source":["Split data into three sets: training, validation, and test. Note that we draw 10.000 random documents from the training set to speed up the training process."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BBpPCg5zILlu"},"outputs":[],"source":["train_corpus = corpus[corpus[\"testset\"] == 0]\n","val_corpus = train_corpus.iloc[80000:100000,]\n","train_corpus = train_corpus.iloc[0:80000,].sample(10000)\n","test_corpus = corpus[corpus[\"testset\"] == 1]"]},{"cell_type":"markdown","metadata":{"id":"n5Mygm5yIYnh"},"source":["For each dataset, store features and targets in separate variables"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"18PF5d6ZIN-U"},"outputs":[],"source":["train_corpus_features = train_corpus[[\"description\"]]\n","train_corpus_target = train_corpus[[\"points\"]]\n","val_corpus_features = val_corpus[[\"description\"]]\n","val_corpus_target = val_corpus[[\"points\"]]\n","test_corpus_features = test_corpus[[\"description\"]]\n","test_corpus_target = test_corpus[[\"points\"]]"]},{"cell_type":"markdown","metadata":{"id":"gvhh11WuIeyk"},"source":["Create [TensorFlow `Datasets`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) from the Pandas Dataframes. The use of TensorFlow Datasets follows a common pattern:\n","\n","1.   Create a dataset from raw data (e.g., a Pandas dataframe, a CSV file, multiple text files).\n","2.   Apply transformations to preprocess the data in the dataset (e.g., vectorize text data).\n","3. Iterate over the dataset and process its elements (e.g., for training or making predictions). Iteration happens in a streaming fashion, so the full dataset does not need to fit into memory.\n","\n","Here, we use the `from_tensor_slices` constructor to create datasets from dataframes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QOOszgPrQVVw"},"outputs":[],"source":["train_ds = tf.data.Dataset.from_tensor_slices((tf.cast(train_corpus_features.values, tf.string),\n","                                               tf.cast(train_corpus_target.values, tf.int32)))\n","\n","val_ds = tf.data.Dataset.from_tensor_slices((tf.cast(val_corpus_features.values, tf.string),\n","                                             tf.cast(val_corpus_target.values, tf.int32)))\n","\n","test_ds = tf.data.Dataset.from_tensor_slices((tf.cast(test_corpus_features.values, tf.string),\n","                                              tf.cast(test_corpus_target.values, tf.int32)))"]},{"cell_type":"markdown","metadata":{"id":"dXbjrq6NJ1Qk"},"source":["Display some stats and examples from the created datasets. Because `train_ds` is usually processed in a streaming fashion, we need to use a loop to access its contents."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"j9LXPIY-XxDH"},"outputs":[],"source":["for inputs, targets in train_ds:\n","    print(\"inputs.shape:\", inputs.shape)\n","    print(\"inputs.dtype:\", inputs.dtype)\n","    print(\"targets.shape:\", targets.shape)\n","    print(\"targets.dtype:\", targets.dtype)\n","    print(\"===\")\n","    print(\"inputs[0]:\", inputs[0])\n","    print(\"targets[0]:\", targets[0])\n","    break"]},{"cell_type":"markdown","metadata":{"id":"Z81zya70VKDy"},"source":["# Vectorize documents"]},{"cell_type":"markdown","metadata":{"id":"rM7gEBL8KHzg"},"source":["We will now use [TensorFlow's `TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) function to transform raw texts into numerical vectors (e.g., frequency counts, tf-idf)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oyJW-JetIN2w"},"outputs":[],"source":["max_tokens = 10000\n","text_vectorization = TextVectorization(\n","    max_tokens = max_tokens,\n","    output_mode = \"count\"\n",")"]},{"cell_type":"markdown","metadata":{"id":"T4ElfvWtKj0Q"},"source":["Some apects of the `TextVectorization` function (e.g., the size and contents of the vocabulary) have to be fit using training data, which can be done with the `adapt` function (which can only be applied to the features (x) of the dataset). "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s6xCAUAOMdYH"},"outputs":[],"source":["train_ds_features_only = train_ds.map(lambda x, y: x)\n","text_vectorization.adapt(train_ds_features_only)"]},{"cell_type":"markdown","metadata":{"id":"DkQu4KlNLftU"},"source":["Show some of the vocabulary that our vectorizer knows after being fit to the training data. When we reuse this vectorizer on new data (e.g., test set), only the words in this vocabulary will be considered."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Dn0lt-7CRaCq"},"outputs":[],"source":["text_vectorization.get_vocabulary()[0:20]"]},{"cell_type":"markdown","metadata":{"id":"VjeZpApaLvyx"},"source":["Next, we apply our `text_vectorization` function to all three datasets. This corresponds to step 2 mentioned above."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jHGn2peqMYuP"},"outputs":[],"source":["vectorized_train_ds = train_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls = 4)\n","\n","vectorized_val_ds = val_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls = 4)\n","\n","vectorized_test_ds = test_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls = 4)"]},{"cell_type":"markdown","metadata":{"id":"K_WzVu2WMEn-"},"source":["Show results."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0etTA2eAUB6G"},"outputs":[],"source":["for inputs, targets in vectorized_train_ds:\n","    print(\"inputs.shape:\", inputs.shape)\n","    print(\"inputs.dtype:\", inputs.dtype)\n","    print(\"targets.shape:\", targets.shape)\n","    print(\"targets.dtype:\", targets.dtype)\n","    print(\"===\")\n","    print(\"inputs[0]:\", inputs[0])\n","    print(\"targets[0]:\", targets[0])\n","    break"]},{"cell_type":"markdown","metadata":{"id":"h9QH9PcrSa7J"},"source":["# Train model"]},{"cell_type":"markdown","metadata":{"id":"G_K1l5L-MQ7z"},"source":["We are now ready to specify a neural network and feed it with the vectroized datasets. For convenience, we define a custome function `get_model` which defines the network architecture, creates a model from it, and compiles this model (by defining, e.g., an otpimizer and loss function)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZMB14gBYSblz"},"outputs":[],"source":["def get_model(max_tokens=10000, hidden_dim=32):\n","    inputs = keras.Input(shape = (max_tokens,))\n","    hidden = layers.Dense(hidden_dim, activation = \"relu\")(inputs)\n","    outputs = layers.Dense(1, activation = \"linear\")(hidden)\n","    model = keras.Model(inputs, outputs)\n","    model.compile(optimizer = tf.optimizers.Adam(),\n","                  loss = \"mean_absolute_error\",\n","                  metrics = [\"mean_absolute_error\"])\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"pWZtLzduT_sH"},"source":["Instantiate model and show it's architecture."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"02bcqmcvTIDW"},"outputs":[],"source":["model = get_model(max_tokens)\n","model.summary()"]},{"cell_type":"markdown","metadata":{"id":"0W4SzaRqUExq"},"source":["Fit model on training data and save best model to disk."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aQZ8zbxbSzZb"},"outputs":[],"source":["callbacks = [keras.callbacks.ModelCheckpoint(\"bow.tf\", save_best_only = True)]\n","\n","history = model.fit(vectorized_train_ds.cache(),\n","          validation_data = vectorized_val_ds.cache(),\n","          epochs = 3,\n","          batch_size = 64,\n","          callbacks = callbacks)"]},{"cell_type":"markdown","metadata":{"id":"ebyRcAuv0LAi"},"source":["Plot the learning process."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xSeBQ7Ho5aBL"},"outputs":[],"source":["plt.plot(history.history['mean_absolute_error'])\n","plt.plot(history.history['val_mean_absolute_error'])\n","plt.title('Model accuracy')\n","plt.ylabel('Mean Absolute Error')\n","plt.xlabel('Epoch')\n","plt.legend(['train', 'validation'], loc='upper left')\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"wJ347rt_Tx0G"},"source":["# Make predictions"]},{"cell_type":"markdown","metadata":{"id":"XKR36ArkT2Gq"},"source":["Load best model from training phase."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zA_ON4yBTb2a"},"outputs":[],"source":["model = keras.models.load_model(\"bow.tf\")"]},{"cell_type":"markdown","metadata":{"id":"8LRhBvwPT1q-"},"source":["Make predictions on test set."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yckI-BTWSTFm"},"outputs":[],"source":["preds = model.predict(vectorized_test_ds)"]},{"cell_type":"markdown","metadata":{"id":"pMFi18uLT7tG"},"source":["Calculate accuracy metrics."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"afIwDkWYS_i4"},"outputs":[],"source":["print(metrics.mean_absolute_error(test_corpus_target, preds))"]}],"metadata":{"accelerator":"GPU","colab":{"private_outputs":true,"provenance":[{"file_id":"1kHkaqxz9sVdaOC2i4FJVOOFW9cCiBkYu","timestamp":1636383829898}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}
