{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"},"colab":{"private_outputs":true,"provenance":[{"file_id":"1kHkaqxz9sVdaOC2i4FJVOOFW9cCiBkYu","timestamp":1636383829898}]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"LJrKCbjZsqpo"},"source":["# <font color=\"#003660\">Applied Machine Learning for Text Analysis (M.184.5331)</font>\n"]},{"cell_type":"markdown","metadata":{"id":"V-DU0hkyVyPi"},"source":["# <font color=\"#003660\">Week 4: From MLP over RNN to Transformer</font>"]},{"cell_type":"markdown","metadata":{"id":"mhy42GjRV3ON"},"source":["# <font color=\"#003660\">Notebook 3: Long-Short Term Memory (LSTM) Networks with Embeddings</font>\n","\n","<center><br><img width=256 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/dag.png\"/><br></center>\n","\n","<p>\n","<center>\n","<div>\n","    <font color=\"#085986\"><b>By the end of this lesson, you ...</b><br><br>\n","        ... understand the logic behind recurrent neural networks, especially LSTMs. and <br>\n","        ... are able to train a LSTM with word embeddings as features.\n","    </font>\n","</div>\n","</center>\n","</p>"]},{"cell_type":"markdown","metadata":{"id":"Hr1lI3joctE6"},"source":["# What are Recurrent Neural Networks?"]},{"cell_type":"markdown","metadata":{"id":"lx6TSeClcvWb"},"source":["## Simple Recurrent Neural Network (RNN)\n","\n","A RNN processes sequences by iterating through the sequence elements and maintaining a *state* containing information relative to what it has seen so far. In effect, a RNN layer is a neural network layer with an internal loop, as shown in the figure below."]},{"cell_type":"markdown","metadata":{"id":"ZNlB5mtDdLzn"},"source":["<br><img width=256 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/rnn1.png\"/><br>\n","<center>Source: Chollet (2021)</center>"]},{"cell_type":"markdown","metadata":{"id":"jOobrDD2dMBz"},"source":["The figure below shows a simple RNN unrolled over time. As can be seen from the figure the output of a layer is a combination of\n","\n","1. its direct data input (`input_t`),\n","2. the layer's state from the previous timestep (`state_t`), and\n","3. a bias term (`bo`)."]},{"cell_type":"markdown","metadata":{"id":"Qn6fuh8HdmTo"},"source":["<br><img width=700 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/rnn2.png\"/><br>\n","\n","<center>Source: Chollet (2021)</center>"]},{"cell_type":"markdown","metadata":{"id":"xrYClhKAduA3"},"source":["## Long-Short Term Memory (LSTM) Networks"]},{"cell_type":"markdown","metadata":{"id":"TQny-Crkd0cJ"},"source":["Compared to a simple RNN layer, a LSTM layer contains one central innovation: A *carry track* that allows to carry over information over time from any previous timestep to the current timestep. Consequently, the output of a layer is a combination of\n","\n","\n","\n","1. its direct data input (`input_t`),\n","2. the layer's state from the previous timestep (`state_t`),\n","3. the input from the carry track (`c_t`), and\n","4. a bias term (`bo`)."]},{"cell_type":"markdown","metadata":{"id":"QlZU3v9xd_ia"},"source":["<br><img width=700 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/lstm.png\"/><br>\n","<center>Source: Chollet (2021)</center>"]},{"cell_type":"markdown","source":["## Comparing RNNs with MLPs"],"metadata":{"id":"3VpSWKNzXDbv"}},{"cell_type":"markdown","source":["Andrej Karpathy's legendary blog post \"The Unreasonable Effectiveness of Recurrent Neural Networks\" contains a very informative comparison of RNNs with traditional neural networks: http://karpathy.github.io/2015/05/21/rnn-effectiveness/"],"metadata":{"id":"OmKWTO8DXTAq"}},{"cell_type":"markdown","metadata":{"id":"C6vVpwIFsqps"},"source":["# Import packages\n","\n","As always, we first need to load a number of required Python packages:\n","- `pandas` provides high-performance, easy-to-use data structures and data analysis tools.\n","- `numpy` is a library adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\n","- `SQLAlchemy`, together with `pymysql`, allows to communicate with SQL databases.\n","- `getpass` provides function to safely enter passwords.\n","- `sklearn` is a free software machine learning library for the Python programming language.\n","- `tensorflow` is an end-to-end open source platform for machine learning, especially deep learning.\n","- `matplotlib` is a plotting library for the Python programming language and its numerical mathematics extension NumPy\n"]},{"cell_type":"code","metadata":{"id":"5NpTS4Z2Vvzy"},"source":["# Install packages\n","!pip install pymysql"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mMrhkr83sqpt"},"source":["import pandas as pd\n","import numpy as np\n","from sqlalchemy import create_engine\n","import getpass\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import TextVectorization\n","from sklearn import metrics\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Check if we are running on GPU."],"metadata":{"id":"0cT264vl78Zo"}},{"cell_type":"code","source":["tf.config.experimental.list_physical_devices('GPU')"],"metadata":{"id":"RHONHA667__1"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jZd82t53sqpu"},"source":["# Load documents"]},{"cell_type":"markdown","metadata":{"id":"IsrCafxksqpv"},"source":["We load our data from a MySQL database. For security reasons, we don't store the database credentials here; please have a look at Panda to get them."]},{"cell_type":"code","metadata":{"id":"vSIGfKcXsqpv"},"source":["# Get credentials\n","user = input(\"Username: \")\n","passwd = getpass.getpass(\"Password: \")\n","server = input(\"Server: \")\n","db = input(\"Database: \")\n","\n","# Create an engine instance (SQLAlchemy)\n","engine = create_engine(\"mysql+pymysql://{}:{}@{}/{}\".format(user, passwd ,server, db))\n","\n","# Define SQL query\n","sql_query = \"SELECT * FROM WineDataset\"\n","\n","# Query dataset (pandas)\n","corpus = pd.read_sql(sql=sql_query, con=engine)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LoihghYqIQun"},"source":["Display `shape` of the data."]},{"cell_type":"code","metadata":{"id":"7NsIQrLiSEak"},"source":["corpus.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CureExnsIS-p"},"source":["Split data into three sets: training, validation, and test."]},{"cell_type":"code","metadata":{"id":"BBpPCg5zILlu"},"source":["train_corpus = corpus[corpus[\"testset\"] == 0]\n","val_corpus = train_corpus.iloc[80000:100000,]\n","train_corpus = train_corpus.iloc[0:80000,].sample(10000)\n","test_corpus = corpus[corpus[\"testset\"] == 1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n5Mygm5yIYnh"},"source":["For each dataset, store features and targets in separate variables"]},{"cell_type":"code","metadata":{"id":"18PF5d6ZIN-U"},"source":["train_corpus_features = train_corpus[[\"description\"]]\n","train_corpus_target = train_corpus[[\"points\"]]\n","val_corpus_features = val_corpus[[\"description\"]]\n","val_corpus_target = val_corpus[[\"points\"]]\n","test_corpus_features = test_corpus[[\"description\"]]\n","test_corpus_target = test_corpus[[\"points\"]]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gvhh11WuIeyk"},"source":["Create [TensorFlow `Datasets`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) from the Pandas Dataframes. The use of TensorFlow Datasets follows a common pattern:\n","\n","1.   Create a dataset from raw data (e.g., a Pandas dataframe, a CSV file, multiple text files).\n","2.   Apply transformations to preprocess the data in the dataset (e.g., vectorize text data).\n","3. Iterate over the dataset and process its elements. Iteration happens in a streaming fashion, so the full dataset does not need to fit into memory.\n","\n","Here, we use the `from_tensor_slices` constructor to create datasets from dataframes."]},{"cell_type":"code","metadata":{"id":"QOOszgPrQVVw"},"source":["train_ds = tf.data.Dataset.from_tensor_slices((tf.cast(train_corpus_features.values, tf.string),\n","                                               tf.cast(train_corpus_target.values, tf.int32)))\n","\n","val_ds = tf.data.Dataset.from_tensor_slices((tf.cast(val_corpus_features.values, tf.string),\n","                                             tf.cast(val_corpus_target.values, tf.int32)))\n","\n","test_ds = tf.data.Dataset.from_tensor_slices((tf.cast(test_corpus_features.values, tf.string),\n","                                              tf.cast(test_corpus_target.values, tf.int32)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dXbjrq6NJ1Qk"},"source":["Display some stats and examples from the created datasets."]},{"cell_type":"code","metadata":{"id":"j9LXPIY-XxDH"},"source":["for inputs, targets in train_ds:\n","    print(\"inputs.shape:\", inputs.shape)\n","    print(\"inputs.dtype:\", inputs.dtype)\n","    print(\"targets.shape:\", targets.shape)\n","    print(\"targets.dtype:\", targets.dtype)\n","    print(\"===\")\n","    print(\"inputs[0]:\", inputs[0])\n","    print(\"targets[0]:\", targets[0])\n","    break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uwh4TWJbVFY9"},"source":["# Vectorize documents"]},{"cell_type":"markdown","metadata":{"id":"rM7gEBL8KHzg"},"source":["We will now use [TensorFlow's `TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) function to transform raw texts into numerical vectors. Again, we map unique words to integers."]},{"cell_type":"code","metadata":{"id":"oyJW-JetIN2w"},"source":["max_tokens = 10000\n","max_length = 100\n","\n","text_vectorization = TextVectorization(\n","    max_tokens = max_tokens,\n","    output_mode = \"int\",\n","    output_sequence_length = max_length\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T4ElfvWtKj0Q"},"source":["Some apects of the `TextVectorization` function (e.g., the size and contents of the vocabulary) have to be fit using training data, which can be done with the `adapt` function."]},{"cell_type":"code","metadata":{"id":"s6xCAUAOMdYH"},"source":["train_ds_features_only = train_ds.map(lambda x, y: x)\n","text_vectorization.adapt(train_ds_features_only)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DkQu4KlNLftU"},"source":["Show the vocabulary that our vectorizer knows after being fit to the training data."]},{"cell_type":"code","metadata":{"id":"Dn0lt-7CRaCq"},"source":["text_vectorization.get_vocabulary()[0:10]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VjeZpApaLvyx"},"source":["Next, we apply our `text_vectorization` function to all three datasets. This corresponds to step 2 mentioned above."]},{"cell_type":"code","metadata":{"id":"jHGn2peqMYuP"},"source":["vectorized_train_ds = train_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls = 4)\n","\n","vectorized_val_ds = val_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls = 4)\n","\n","vectorized_test_ds = test_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls = 4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K_WzVu2WMEn-"},"source":["Show results."]},{"cell_type":"code","metadata":{"id":"0etTA2eAUB6G"},"source":["for inputs, targets in vectorized_train_ds:\n","    print(\"inputs.shape:\", inputs.shape)\n","    print(\"inputs.dtype:\", inputs.dtype)\n","    print(\"targets.shape:\", targets.shape)\n","    print(\"targets.dtype:\", targets.dtype)\n","    print(\"===\")\n","    print(\"inputs[0]:\", inputs[0])\n","    print(\"targets[0]:\", targets[0])\n","    break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h9QH9PcrSa7J"},"source":["# Train model"]},{"cell_type":"markdown","metadata":{"id":"G_K1l5L-MQ7z"},"source":["We are now ready to specify a neural network and feed it with the vectroized datasets. For convenience, we define a custome function `get_model` which defines the network architecture, creates a model from it, and compiles this model (by defining, e.g., an otpimizer and loss function).\n","\n","Instead of averaging or flattening the outputs of the embedding layer, a LSTM layer can directly process its 2D output (i.e., it takes a sequence of vectors as input instead of a single vector).\n","\n","In the example below we stack two bi-directional LSTM layers on top of each other."]},{"cell_type":"code","metadata":{"id":"ZMB14gBYSblz"},"source":["def get_model(max_tokens=10000):\n","    inputs = keras.Input(shape=(max_length,), dtype=\"int64\")\n","    embedded = layers.Embedding(input_dim=max_tokens, output_dim=200, mask_zero=True)(inputs)\n","    hidden1 = layers.Bidirectional(layers.LSTM(32, return_sequences = True))(embedded)\n","    hidden2 = layers.Bidirectional(layers.LSTM(32))(hidden1)\n","    outputs = layers.Dense(1, activation = \"linear\")(hidden2)\n","    model = keras.Model(inputs, outputs)\n","    model.compile(optimizer = tf.optimizers.Adam(),\n","                  loss = \"mean_absolute_error\",\n","                  metrics = [\"mean_absolute_error\"])\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pWZtLzduT_sH"},"source":["Instantiate model and show it's architecture."]},{"cell_type":"code","metadata":{"id":"02bcqmcvTIDW"},"source":["model = get_model(max_tokens)\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0W4SzaRqUExq"},"source":["Fit model on training data and save best model to disk."]},{"cell_type":"code","metadata":{"id":"aQZ8zbxbSzZb"},"source":["callbacks = [keras.callbacks.ModelCheckpoint(\"lstm.keras\", save_best_only=True)]\n","\n","history = model.fit(vectorized_train_ds.cache(),\n","          validation_data = vectorized_val_ds.cache(),\n","          epochs = 3,\n","          batch_size = 64,\n","          callbacks = callbacks)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s7NOcSDY0Y6I"},"source":["Plot the learning process."]},{"cell_type":"code","metadata":{"id":"BoHl0JaJ5fJm"},"source":["plt.plot(history.history['mean_absolute_error'])\n","plt.plot(history.history['val_mean_absolute_error'])\n","plt.title('Model accuracy')\n","plt.ylabel('Mean Absolute Error')\n","plt.xlabel('Epoch')\n","plt.legend(['train', 'validation'], loc='upper left')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wJ347rt_Tx0G"},"source":["# Make predictions"]},{"cell_type":"markdown","metadata":{"id":"XKR36ArkT2Gq"},"source":["Load best model from training phase."]},{"cell_type":"code","metadata":{"id":"zA_ON4yBTb2a"},"source":["model = keras.models.load_model(\"lstm.keras\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8LRhBvwPT1q-"},"source":["Make predictions on test set."]},{"cell_type":"code","metadata":{"id":"yckI-BTWSTFm"},"source":["preds = model.predict(vectorized_test_ds)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pMFi18uLT7tG"},"source":["Calculate accuracy metrics."]},{"cell_type":"code","metadata":{"id":"afIwDkWYS_i4"},"source":["print(metrics.mean_absolute_error(test_corpus_target, preds))"],"execution_count":null,"outputs":[]}]}