{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"},"colab":{"private_outputs":true,"provenance":[{"file_id":"1kHkaqxz9sVdaOC2i4FJVOOFW9cCiBkYu","timestamp":1636383829898}]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"LJrKCbjZsqpo"},"source":["# <font color=\"#003660\">Applied Machine Learning for Text Analysis (M.184.5331)</font>\n"]},{"cell_type":"markdown","metadata":{"id":"V-DU0hkyVyPi"},"source":["# <font color=\"#003660\">Week 4: From MLP over RNN to Transformer</font>"]},{"cell_type":"markdown","metadata":{"id":"mhy42GjRV3ON"},"source":["# <font color=\"#003660\">Notebook 2: MLP with Embeddings</font>\n","\n","<center><br><img width=256 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/dag.png\"/><br></center>\n","\n","<p>\n","<center>\n","<div>\n","    <font color=\"#085986\"><b>By the end of this lesson, you ...</b><br><br>\n","        ... are able to train a neural network with word embeddings as features.\n","    </font>\n","</div>\n","</center>\n","</p>"]},{"cell_type":"markdown","metadata":{"id":"uiF_1GU1RMNR"},"source":["# Using Word Embeddings as Features"]},{"cell_type":"markdown","metadata":{"id":"WTmVLl7fRRGT"},"source":["Instead of using word counts as features, we can also represent sentences as sequences of word embeddings. This results in a 2D data structure (sequence_length*embedding_dim). As \"normal\" neural networks cannot process such 2D tensors, we have to find a way to reduce the dimensionality of this structure."]},{"cell_type":"markdown","metadata":{"id":"C6vVpwIFsqps"},"source":["# Import packages\n","\n","As always, we first need to load a number of required Python packages:\n","- `pandas` provides high-performance, easy-to-use data structures and data analysis tools.\n","- `numpy` is a library adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\n","- `SQLAlchemy`, together with `pymysql`, allows to communicate with SQL databases.\n","- `getpass` provides function to safely enter passwords.\n","- `sklearn` is a free software machine learning library for the Python programming language.\n","- `tensorflow` is an end-to-end open source platform for machine learning, especially deep learning.\n","- `matplotlib` is a plotting library for the Python programming language and its numerical mathematics extension NumPy\n"]},{"cell_type":"code","metadata":{"id":"5NpTS4Z2Vvzy"},"source":["# Install packages\n","!pip install pymysql"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mMrhkr83sqpt"},"source":["import pandas as pd\n","import numpy as np\n","from sqlalchemy import create_engine\n","import getpass\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers\n","from tensorflow.keras.layers import TextVectorization\n","from sklearn import metrics\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Check if we are running on GPU."],"metadata":{"id":"NDzj8w5e7DaY"}},{"cell_type":"code","source":["tf.config.experimental.list_physical_devices('GPU')"],"metadata":{"id":"FhHhJiZD6yQG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jZd82t53sqpu"},"source":["# Load documents"]},{"cell_type":"markdown","metadata":{"id":"IsrCafxksqpv"},"source":["We load our data from a MySQL database. For security reasons, we don't store the database credentials here; please have a look at Panda to get them."]},{"cell_type":"code","metadata":{"id":"vSIGfKcXsqpv"},"source":["# Get credentials\n","user = input(\"Username: \")\n","passwd = getpass.getpass(\"Password: \")\n","server = input(\"Server: \")\n","db = input(\"Database: \")\n","\n","# Create an engine instance (SQLAlchemy)\n","engine = create_engine(\"mysql+pymysql://{}:{}@{}/{}\".format(user, passwd ,server, db))\n","\n","# Define SQL query\n","sql_query = \"SELECT * FROM WineDataset\"\n","\n","# Query dataset (pandas)\n","corpus = pd.read_sql(sql=sql_query, con=engine)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LoihghYqIQun"},"source":["Display `shape` of the data."]},{"cell_type":"code","metadata":{"id":"7NsIQrLiSEak"},"source":["corpus.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CureExnsIS-p"},"source":["Split data into three sets: training, validation, and test."]},{"cell_type":"code","metadata":{"id":"BBpPCg5zILlu"},"source":["train_corpus = corpus[corpus[\"testset\"] == 0]\n","val_corpus = train_corpus.iloc[80000:100000,]\n","train_corpus = train_corpus.iloc[0:80000,].sample(10000)\n","test_corpus = corpus[corpus[\"testset\"] == 1]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n5Mygm5yIYnh"},"source":["For each dataset, store features and targets in separate variables"]},{"cell_type":"code","metadata":{"id":"18PF5d6ZIN-U"},"source":["train_corpus_features = train_corpus[[\"description\"]]\n","train_corpus_target = train_corpus[[\"points\"]]\n","val_corpus_features = val_corpus[[\"description\"]]\n","val_corpus_target = val_corpus[[\"points\"]]\n","test_corpus_features = test_corpus[[\"description\"]]\n","test_corpus_target = test_corpus[[\"points\"]]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gvhh11WuIeyk"},"source":["Create [TensorFlow `Datasets`](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) from the Pandas Dataframes. The use of TensorFlow Datasets follows a common pattern:\n","\n","1.   Create a dataset from raw data (e.g., a Pandas dataframe, a CSV file, multiple text files).\n","2.   Apply transformations to preprocess the data in the dataset (e.g., vectorize text data).\n","3. Iterate over the dataset and process its elements. Iteration happens in a streaming fashion, so the full dataset does not need to fit into memory.\n","\n","Here, we use the `from_tensor_slices` constructor to create datasets from dataframes."]},{"cell_type":"code","metadata":{"id":"QOOszgPrQVVw"},"source":["train_ds = tf.data.Dataset.from_tensor_slices((tf.cast(train_corpus_features.values, tf.string),\n","                                               tf.cast(train_corpus_target.values, tf.int32)))\n","\n","val_ds = tf.data.Dataset.from_tensor_slices((tf.cast(val_corpus_features.values, tf.string),\n","                                             tf.cast(val_corpus_target.values, tf.int32)))\n","\n","test_ds = tf.data.Dataset.from_tensor_slices((tf.cast(test_corpus_features.values, tf.string),\n","                                              tf.cast(test_corpus_target.values, tf.int32)))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dXbjrq6NJ1Qk"},"source":["Display some stats and examples from the created datasets."]},{"cell_type":"code","metadata":{"id":"j9LXPIY-XxDH"},"source":["for inputs, targets in train_ds:\n","    print(\"inputs.shape:\", inputs.shape)\n","    print(\"inputs.dtype:\", inputs.dtype)\n","    print(\"targets.shape:\", targets.shape)\n","    print(\"targets.dtype:\", targets.dtype)\n","    print(\"===\")\n","    print(\"inputs[0]:\", inputs[0])\n","    print(\"targets[0]:\", targets[0])\n","    break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Uwh4TWJbVFY9"},"source":["# Vectorize documents"]},{"cell_type":"markdown","metadata":{"id":"rM7gEBL8KHzg"},"source":["We will now use [TensorFlow's `TextVectorization`](https://www.tensorflow.org/api_docs/python/tf/keras/layers/TextVectorization) function to transform raw texts into numerical vectors. Instead of counting word appearances (like in the BOW model), we simply map words to integers (`output_mode = 'int'`) this time."]},{"cell_type":"code","metadata":{"id":"oyJW-JetIN2w"},"source":["max_tokens = 10000\n","max_length = 100\n","\n","text_vectorization = TextVectorization(\n","    max_tokens = max_tokens,\n","    output_mode = \"int\",\n","    output_sequence_length = max_length\n",")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"T4ElfvWtKj0Q"},"source":["Some apects of the `TextVectorization` function (e.g., the size and contents of the vocabulary) have to be fit using training data, which can be done with the `adapt` function."]},{"cell_type":"code","metadata":{"id":"s6xCAUAOMdYH"},"source":["train_ds_features_only = train_ds.map(lambda x, y: x)\n","text_vectorization.adapt(train_ds_features_only)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DkQu4KlNLftU"},"source":["Show the vocabulary that our vectorizer knows after being fit to the training data."]},{"cell_type":"code","metadata":{"id":"Dn0lt-7CRaCq"},"source":["text_vectorization.get_vocabulary()[0:10]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VjeZpApaLvyx"},"source":["Next, we apply our `text_vectorization` function to all three datasets. This corresponds to step 2 mentioned above."]},{"cell_type":"code","metadata":{"id":"jHGn2peqMYuP"},"source":["vectorized_train_ds = train_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls = 4)\n","\n","vectorized_val_ds = val_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls = 4)\n","\n","vectorized_test_ds = test_ds.map(\n","    lambda x, y: (text_vectorization(x), y),\n","    num_parallel_calls = 4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"K_WzVu2WMEn-"},"source":["Show results."]},{"cell_type":"code","metadata":{"id":"0etTA2eAUB6G"},"source":["for inputs, targets in vectorized_train_ds:\n","    print(\"inputs.shape:\", inputs.shape)\n","    print(\"inputs.dtype:\", inputs.dtype)\n","    print(\"targets.shape:\", targets.shape)\n","    print(\"targets.dtype:\", targets.dtype)\n","    print(\"===\")\n","    print(\"inputs[0]:\", inputs[0])\n","    print(\"targets[0]:\", targets[0])\n","    break"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"h9QH9PcrSa7J"},"source":["# Train model"]},{"cell_type":"markdown","metadata":{"id":"G_K1l5L-MQ7z"},"source":["We are now ready to specify a neural network and feed it with the vectroized datasets. For convenience, we define a custome function `get_model` which defines the network architecture, creates a model from it, and compiles this model (by defining, e.g., an otpimizer and loss function). Note that we have to somehow reduce the dimensionality of the output of the embedding layer (sequence_length*embedding_dim). Here, we simply perform a global average pooling (i.e., average each element of the embedding vector over all tokens).\n","\n","*Question*: Can you think of another way to reduce the dimensionality of the output of the embedding layer?"]},{"cell_type":"code","metadata":{"id":"ZMB14gBYSblz"},"source":["def get_model(max_tokens=10000, hidden_dim=16):\n","    inputs = keras.Input(shape=(max_length,), dtype=\"int64\")\n","    embedded = layers.Embedding(input_dim=max_tokens, output_dim=200, mask_zero=True)(inputs)\n","    #hidden1 = layers.GlobalAveragePooling1D()(embedded)\n","    hidden1 = layers.Flatten()(embedded)\n","    hidden2 = layers.Dense(hidden_dim, activation = \"relu\")(hidden1)\n","    outputs = layers.Dense(1, activation = \"linear\")(hidden2)\n","    model = keras.Model(inputs, outputs)\n","    model.compile(optimizer = tf.optimizers.Adam(),\n","                  loss = \"mean_absolute_error\",\n","                  metrics = [\"mean_absolute_error\"])\n","    return model"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pWZtLzduT_sH"},"source":["Instantiate model and show it's architecture."]},{"cell_type":"code","metadata":{"id":"02bcqmcvTIDW"},"source":["model = get_model(max_tokens)\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0W4SzaRqUExq"},"source":["Fit model on training data and save best model to disk."]},{"cell_type":"code","metadata":{"id":"aQZ8zbxbSzZb"},"source":["callbacks = [keras.callbacks.ModelCheckpoint(\"embed.keras\", save_best_only=True)]\n","\n","history = model.fit(vectorized_train_ds.cache(),\n","          validation_data = vectorized_val_ds.cache(),\n","          epochs = 3,\n","          batch_size = 64,\n","          callbacks = callbacks)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"s7NOcSDY0Y6I"},"source":["Plot the learning process."]},{"cell_type":"code","metadata":{"id":"BoHl0JaJ5fJm"},"source":["plt.plot(history.history['mean_absolute_error'])\n","plt.plot(history.history['val_mean_absolute_error'])\n","plt.title('Model accuracy')\n","plt.ylabel('Mean Absolute Error')\n","plt.xlabel('Epoch')\n","plt.legend(['train', 'validation'], loc='upper left')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wJ347rt_Tx0G"},"source":["# Make predictions"]},{"cell_type":"markdown","metadata":{"id":"XKR36ArkT2Gq"},"source":["Load best model from training phase."]},{"cell_type":"code","metadata":{"id":"zA_ON4yBTb2a"},"source":["model = keras.models.load_model(\"embed.keras\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8LRhBvwPT1q-"},"source":["Make predictions on test set."]},{"cell_type":"code","metadata":{"id":"yckI-BTWSTFm"},"source":["preds = model.predict(vectorized_test_ds)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pMFi18uLT7tG"},"source":["Calculate accuracy metrics."]},{"cell_type":"code","metadata":{"id":"afIwDkWYS_i4"},"source":["print(metrics.mean_absolute_error(test_corpus_target, preds))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Use pre-trained word embeddings"],"metadata":{"id":"Vpi_fbUr6d-U"}},{"cell_type":"markdown","source":["In the above model, we learn word embeddings on-the-fly as a by-product of the regression task. This will result in word embeddings that are tuned for regression. Yet, these word embeddings have been learned from a relatively small dataset (here: 10.000 short online reviews).\n","\n","*Question*: Can we improve our model by using pre-trained word embeddings?"],"metadata":{"id":"nLgKGTem6icI"}},{"cell_type":"markdown","source":["Let's first create a dictionary to retrieve the integer index of a word in our vocabulary."],"metadata":{"id":"eedifwlNBRJp"}},{"cell_type":"code","source":["voc = text_vectorization.get_vocabulary()\n","word_index = dict(zip(voc, range(len(voc))))"],"metadata":{"id":"3WSOsmzjBQZE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["word_index[\"and\"]"],"metadata":{"id":"k03kKST0CKJU"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Next, we need a function to load the embeddings from a file and transform the contents into a matrix with as many rows as we have words in the vocabulary and as many columns as we have dimensions in the embeddings."],"metadata":{"id":"4dRFtz8zBaiC"}},{"cell_type":"code","source":["def create_embedding_matrix(filepath, word_index, embedding_dim):\n","    # create a matrix with the right dimensions and fill it with zeros\n","    vocab_size = len(word_index)\n","    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n","\n","    # open embeddings file, read it line by line, and\n","    # for every word in the vocabulary that is in the embeddings file\n","    # fill the matrix with the pre-trained embedding values\n","    # YOUR CODE GOES HERE\n","\n","    # return embedding_matrix\n","    return embedding_matrix"],"metadata":{"id":"WHD3OjkF95uC"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now we can load embeddings from a file."],"metadata":{"id":"CVjYAV80SZr3"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"JnYW1dty7NV_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embedding_dim = 300\n","embedding_matrix = create_embedding_matrix(\n","    \"/content/drive/MyDrive/colab_notebooks/AML4TA2022/Session_03/data/wine_300dim_10minwords_4context\",\n","    word_index, embedding_dim)"],"metadata":{"id":"jqtjDthK8b8-"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["embedding_matrix[word_index[\"and\"]]"],"metadata":{"id":"1DoR1iWg-ztp"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["How many entries of the matrix are non-zero?"],"metadata":{"id":"fXQkscL4Chjg"}},{"cell_type":"code","source":["np.count_nonzero(np.count_nonzero(embedding_matrix, axis=1)) / len(word_index)"],"metadata":{"id":"51tzVHDNCgql"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Go back to the model definition above and initialize the embedding layer with embedding_matrix..."],"metadata":{"id":"DeRX6boQSNYU"}}]}