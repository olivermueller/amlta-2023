{"cells":[{"cell_type":"markdown","metadata":{"id":"LJrKCbjZsqpo"},"source":["# <font color=\"#003660\">Applied Machine Learning for Text Analysis (M.184.5331)</font>\n"]},{"cell_type":"markdown","metadata":{"id":"V-DU0hkyVyPi"},"source":["# <font color=\"#003660\">Session 7: Generating Texts with Transformers</font>"]},{"cell_type":"markdown","metadata":{"id":"mhy42GjRV3ON"},"source":["# <font color=\"#003660\">Notebook 2: Fine-tuning a Masked Language Model</font>\n","\n","<center><br><img width=256 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/dag.png\"/><br></center>\n","\n","<p>\n","<center>\n","<div>\n","    <font color=\"#085986\"><b>By the end of this lesson, you ...</b><br><br>\n","        ... are able to fine-tune a masked language model on your own data, which is useful to train a decoder model.\n","    </font>\n","</div>\n","</center>\n","</p>"]},{"cell_type":"markdown","metadata":{"id":"B8WyaSVOepeR"},"source":["The following content is heavily inspired by the following excellent sources:\n","\n","\n","*   Tunstall et al. (2021): Natural Language Processing with Transformers. O'Reilly. https://www.oreilly.com/library/view/natural-language-processing/9781098103231/\n","*   Hugging Face (2021): Transformer Models - Hugging Face Course. https://huggingface.co/course/\n","\n"]},{"cell_type":"markdown","metadata":{"id":"euTwZ6Bne4E3"},"source":["# How to Fine-tune a Masked Language Model?"]},{"cell_type":"markdown","metadata":{"id":"LPYWtUIVQLrc"},"source":["For many NLP applications, you can simply take a pre-trained model from the Hugging Face Hub and fine-tune it directly on your data for the task at hand (e.g., sentiment analysis). This approach will usually produce good results, provided that the corpus used for pretraining is not too different from the corpus used for fine-tuning.\n","\n","However, if your dataset is very different from the dataset used for pre-training, this approach might fail. In such cases, you can boost the performance of many downstream tasks by first fine-tuning *the language model* (not the model for the actual task of interest!) on in-domain data.\n","\n","The figure below illustrates this process, which was first proposed by [Howard and Ruder in 2018](https://arxiv.org/abs/1801.06146).\n","\n","<center><img width=600 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/ulmfit.png\"/><br></center>"]},{"cell_type":"markdown","metadata":{"id":"xUCJOLf9Rqne"},"source":["In this notebook, we go through this process for fine-tuning a [masked langugae model](https://youtu.be/mqElG5QJWUg). "]},{"cell_type":"markdown","metadata":{"id":"QT_dOs1v-J8Q"},"source":["# Import Packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OFZTIYq-Nlbp"},"outputs":[],"source":["!pip install transformers[sentencepiece]\n","!pip install datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mMrhkr83sqpt"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import math\n","import torch\n","from transformers import AutoTokenizer, AutoModelForMaskedLM\n","from datasets import load_dataset\n","from transformers import DataCollatorForLanguageModeling\n","from transformers import TrainingArguments\n","from transformers import Trainer"]},{"cell_type":"markdown","metadata":{"id":"2jQsLKn7eUOz"},"source":["# Load Pre-trained Model"]},{"cell_type":"markdown","metadata":{"id":"HvbQyVC3mqHu"},"source":["First, we load a model for mask language modeling and a corresponding tokenizer from the model hub."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3-jKbucA0aqX"},"outputs":[],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xc4dKH1W0fR5"},"outputs":[],"source":["model_name = \"distilbert-base-uncased\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w6MugtY50fZG"},"outputs":[],"source":["model = AutoModelForMaskedLM.from_pretrained(model_name).to(device)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"]},{"cell_type":"markdown","metadata":{"id":"4Il-svkt_iuS"},"source":["# Testdrive the Model ðŸš—"]},{"cell_type":"markdown","metadata":{"id":"1aphVRslPPAK"},"source":["Let's see what missing words the pre-trained model generates."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yPOSSpa7_yEX"},"outputs":[],"source":["text = \"This is a great [MASK].\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qMxRD3Ba_lQ0"},"outputs":[],"source":["input_ids = tokenizer(text, return_tensors=\"pt\").to(device)\n","input_ids"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZYfCmEnj_2TS"},"outputs":[],"source":["token_logits = model(**input_ids).logits\n","token_logits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Zj8hKEBEdI2q"},"outputs":[],"source":["token_logits.shape"]},{"cell_type":"markdown","metadata":{"id":"cHTE4_auPjXs"},"source":["Identify the location of the [MASK] and retrieve its logits. We then pick the [MASK] candidates with the highest logits."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AQJCRg1O_pya"},"outputs":[],"source":["mask_token_index = torch.where(input_ids[\"input_ids\"] == tokenizer.mask_token_id)[1]\n","mask_token_logits = token_logits[0, mask_token_index, :]\n","mask_token_logits"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Qc5fxJvK_qE1"},"outputs":[],"source":["top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n","top_5_tokens"]},{"cell_type":"markdown","metadata":{"id":"ukAt1Z-GP5dc"},"source":["Replace the [MASK] by the top candidates."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"x82Ckrf8_tES"},"outputs":[],"source":["for token in top_5_tokens:\n","    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"]},{"cell_type":"markdown","metadata":{"id":"FlUfJ6d_Aq7y"},"source":["# Prepare a Dataset for Fine-tuning"]},{"cell_type":"markdown","metadata":{"id":"XG6uEhn7SLF-"},"source":["Now let's fine tune the model on domain-specific texts. We will use the famous IMDB movie reviews dataset for this purpose."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xfTJVsXKAvFo"},"outputs":[],"source":["imdb_dataset = load_dataset(\"imdb\")\n","imdb_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"juHaypVQeTAv"},"outputs":[],"source":["imdb_dataset[\"train\"][0]"]},{"cell_type":"markdown","metadata":{"id":"5dXO7gefTuVD"},"source":["Tokenize the texts and remove unneeded columns."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"99ffcJmtA7CS"},"outputs":[],"source":["def tokenize_function(examples):\n","    result = tokenizer(examples[\"text\"])\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-AkA-0xPBRsy"},"outputs":[],"source":["tokenized_datasets = imdb_dataset.map(\n","    tokenize_function, batched=True, remove_columns=[\"text\", \"label\"]\n",")\n","tokenized_datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IN6byawKedRH"},"outputs":[],"source":["tokenized_datasets[\"train\"][0]"]},{"cell_type":"markdown","metadata":{"id":"rVm9VvoFT6hg"},"source":["For masked language modeling, a [common preprocessing step](https://youtu.be/8PmhEIXhBvI) is to concatenate all the examples and then split the whole corpus into chunks of equal size. This is quite different from our usual approach, where we simply tokenize individual examples. Why concatenate everything together? The reason is that individual examples might get truncated if theyâ€™re too long, and that would result in losing information that might be useful for the language modeling task! "]},{"cell_type":"markdown","metadata":{"id":"BVumsBdqUVnB"},"source":["The function below, taken from https://huggingface.co/course/chapter7/3?fw=pt, does exactly this, and some other preprocessing steps."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_qFzWOpGBwF1"},"outputs":[],"source":["chunk_size = 128\n","\n","def group_texts(examples):\n","    # Concatenate all texts\n","    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n","    # Compute length of concatenated texts\n","    total_length = len(concatenated_examples[list(examples.keys())[0]])\n","    # We drop the last chunk if it's smaller than chunk_size\n","    total_length = (total_length // chunk_size) * chunk_size\n","    # Split by chunks of max_len\n","    result = {\n","        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n","        for k, t in concatenated_examples.items()\n","    }\n","    # Create a new labels column\n","    result[\"labels\"] = result[\"input_ids\"].copy()\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VME0h5-vBySX"},"outputs":[],"source":["lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n","lm_datasets"]},{"cell_type":"markdown","metadata":{"id":"ntsB-10_Ukds"},"source":["During the above preprocessing, we have added a new column `labels` to the dataset. The labels are simply the IDs of the tokens from the input sequence. As you will see shortly, during training we will replace some IDs of the input sequences by [MASK]. After the replacement, the labels column will still contain the \"truth\"."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NwTucienDH6J"},"outputs":[],"source":["lm_datasets[\"train\"][1][\"input_ids\"][0:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2d9DeLm1CShX"},"outputs":[],"source":["lm_datasets[\"train\"][1][\"labels\"][0:10]"]},{"cell_type":"markdown","metadata":{"id":"RbavaRlmCEcj"},"source":["# Fine-tune with Trainer API"]},{"cell_type":"markdown","metadata":{"id":"npH6MuqEVOwF"},"source":["To replace some input tokens by [MASK], we can use `DataCollatorForLanguageModeling()` function, which will perform the replacement on the fly during training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5b3FprccCEni"},"outputs":[],"source":["data_collator = DataCollatorForLanguageModeling(tokenizer=tokenizer, mlm_probability=0.15)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"78T-Ycn0CbLg"},"outputs":[],"source":["samples = [lm_datasets[\"train\"][i] for i in range(2)]\n","\n","for chunk in data_collator(samples)[\"input_ids\"]:\n","  print(f\"\\n'>>> {tokenizer.decode(chunk)}'\")"]},{"cell_type":"markdown","metadata":{"id":"TBpiMBKqVbOA"},"source":["Let's downsample our dataset so that we don't have to wait tooo long."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1-xfcZ0nF1HZ"},"outputs":[],"source":["train_size = 10000\n","test_size = int(0.1 * train_size)\n","\n","downsampled_dataset = lm_datasets[\"train\"].train_test_split(\n","    train_size=train_size, test_size=test_size, seed=42\n",")\n","\n","downsampled_dataset"]},{"cell_type":"markdown","metadata":{"id":"tSCin3luVfYx"},"source":["Now we can finally start fine-tuning our model with the Trainer API."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZnW51vPvEzuZ"},"outputs":[],"source":["batch_size = 128\n","logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n","\n","training_args = TrainingArguments(\n","    output_dir=f\"{model_name}-mlm-finetuned-imdb\",\n","    overwrite_output_dir=True,\n","    evaluation_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    weight_decay=0.01,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    fp16=True,\n","    logging_steps=logging_steps,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WT3_8iHrFxk6"},"outputs":[],"source":["trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=downsampled_dataset[\"train\"],\n","    eval_dataset=downsampled_dataset[\"test\"],\n","    data_collator=data_collator,\n",")"]},{"cell_type":"markdown","metadata":{"id":"Nf_cx7DvVuOs"},"source":["Before we start fine-tuning, we calculate the original model's (pre-trained, but not fine-tuned) [perplexity](https://youtu.be/NURcDHhYe98) as a benchmark. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qucBz0VWGeyW"},"outputs":[],"source":["eval_results = trainer.evaluate()\n","print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"]},{"cell_type":"markdown","metadata":{"id":"4FQAfvXNV5c0"},"source":["Perform the fine-tuning!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wWuvTYHiKKjG"},"outputs":[],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"JOYz6QinV8bW"},"source":["Calculate perplexity again."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oxUUMnk-KT34"},"outputs":[],"source":["eval_results = trainer.evaluate()\n","print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"]},{"cell_type":"markdown","metadata":{"id":"-twj2CnCKtQv"},"source":["# Testdrive the Fine-tuned Model ðŸ›«"]},{"cell_type":"markdown","metadata":{"id":"A96fkU2dV-vN"},"source":["Let's see what missing tokens our fine-tuned model predicts (the code below is a copy&paste from above)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NolwQ2HbK3hw"},"outputs":[],"source":["text = \"This is a great [MASK].\"\n","input_ids = tokenizer(text, return_tensors=\"pt\").to(device)\n","token_logits = model(**input_ids).logits\n","mask_token_index = torch.where(input_ids[\"input_ids\"] == tokenizer.mask_token_id)[1]\n","mask_token_logits = token_logits[0, mask_token_index, :]\n","top_5_tokens = torch.topk(mask_token_logits, 5, dim=1).indices[0].tolist()\n","for token in top_5_tokens:\n","    print(f\"'>>> {text.replace(tokenizer.mask_token, tokenizer.decode([token]))}'\")"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","private_outputs":true,"provenance":[{"file_id":"1kHkaqxz9sVdaOC2i4FJVOOFW9cCiBkYu","timestamp":1636383829898}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}
