{"cells":[{"cell_type":"markdown","metadata":{"id":"LJrKCbjZsqpo"},"source":["# <font color=\"#003660\">Applied Machine Learning for Text Analysis (M.184.5331)</font>\n"]},{"cell_type":"markdown","metadata":{"id":"V-DU0hkyVyPi"},"source":["# <font color=\"#003660\">Session 7: Generating Texts with Transformers</font>"]},{"cell_type":"markdown","metadata":{"id":"mhy42GjRV3ON"},"source":["# <font color=\"#003660\">Notebook 3: Domain Adaptation of a Causal Language Model</font>\n","\n","<center><br><img width=256 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/dag.png\"/><br></center>\n","\n","<p>\n","<center>\n","<div>\n","    <font color=\"#085986\"><b>By the end of this lesson, you ...</b><br><br>\n","        ... will be able to fine-tune a causal language model on your own data, which is useful for training decoder models.\n","    </font>\n","</div>\n","</center>\n","</p>"]},{"cell_type":"markdown","metadata":{"id":"B8WyaSVOepeR"},"source":["The following content is heavily inspired by the following excellent sources:\n","\n","\n","*   Tunstall et al. (2021): Natural Language Processing with Transformers. O'Reilly. https://www.oreilly.com/library/view/natural-language-processing/9781098103231/\n","*   Hugging Face (2021): Transformer Models - Hugging Face Course. https://huggingface.co/course/\n","\n"]},{"cell_type":"markdown","metadata":{"id":"euTwZ6Bne4E3"},"source":["# Recall: What is a Causal Language Model?"]},{"cell_type":"markdown","metadata":{"id":"pbh9AOUcaEZM"},"source":["In the previous notebook we domain-adapted a **masked language model**, where the task is to predict a missing token in a sequence of tokens. This training task is useful **for training encoder models**.\n","\n","<center><img width=300 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/mlm.png\"/><br></center>\n","\n","In this notebook, we will domain-adapt a **causal language model**, which has the task to predict the next token in a sequence of tokens. This is useful **for training decoder models**.\n","\n","<center><img width=400 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/clm.png\"/><br></center>"]},{"cell_type":"markdown","metadata":{"id":"QT_dOs1v-J8Q"},"source":["# Import Packages"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OFZTIYq-Nlbp"},"outputs":[],"source":["!pip install transformers[sentencepiece]\n","!pip install datasets\n","!pip install accelerate -U"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mMrhkr83sqpt"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import math\n","import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from datasets import load_dataset\n","from transformers import TrainingArguments\n","from transformers import Trainer"]},{"cell_type":"markdown","metadata":{"id":"2jQsLKn7eUOz"},"source":["# Load Pre-trained Model"]},{"cell_type":"markdown","metadata":{"id":"HvbQyVC3mqHu"},"source":["First, we load a model for causal language modeling and a corresponding tokenizer from the model hub."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3-jKbucA0aqX"},"outputs":[],"source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xc4dKH1W0fR5"},"outputs":[],"source":["model_name = \"distilgpt2\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w6MugtY50fZG"},"outputs":[],"source":["model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"]},{"cell_type":"markdown","metadata":{"id":"4Il-svkt_iuS"},"source":["# Testdrive the Model ðŸš—"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ebiAflHnWUU1"},"outputs":[],"source":["input_txt = \"Bob and Clara are great\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Paubng0vVvKz"},"outputs":[],"source":["input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n","output = model.generate(input_ids, max_length=64, do_sample=True)\n","print(tokenizer.decode(output[0]))"]},{"cell_type":"markdown","metadata":{"id":"FlUfJ6d_Aq7y"},"source":["# Prepare a Dataset for Domain Adaptation"]},{"cell_type":"markdown","metadata":{"id":"kZTx2gvRWiJM"},"source":["The following data preparation steps are the same as for masked language modeling."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xfTJVsXKAvFo"},"outputs":[],"source":["imdb_dataset = load_dataset(\"imdb\")\n","imdb_dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"99ffcJmtA7CS"},"outputs":[],"source":["def tokenize_function(examples):\n","    result = tokenizer(examples[\"text\"])\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-AkA-0xPBRsy"},"outputs":[],"source":["tokenized_datasets = imdb_dataset.map(\n","    tokenize_function, batched=True, remove_columns=[\"text\", \"label\"]\n",")\n","tokenized_datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_qFzWOpGBwF1"},"outputs":[],"source":["chunk_size = 128\n","\n","def group_texts(examples):\n","    # Concatenate all texts\n","    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n","    # Compute length of concatenated texts\n","    total_length = len(concatenated_examples[list(examples.keys())[0]])\n","    # We drop the last chunk if it's smaller than chunk_size\n","    total_length = (total_length // chunk_size) * chunk_size\n","    # Split by chunks of max_len\n","    result = {\n","        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n","        for k, t in concatenated_examples.items()\n","    }\n","    # Create a new labels column\n","    result[\"labels\"] = result[\"input_ids\"].copy()\n","    return result"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VME0h5-vBySX"},"outputs":[],"source":["lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n","lm_datasets"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NwTucienDH6J"},"outputs":[],"source":["lm_datasets[\"train\"][1][\"input_ids\"][0:10]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2d9DeLm1CShX"},"outputs":[],"source":["lm_datasets[\"train\"][1][\"labels\"][0:10]"]},{"cell_type":"markdown","metadata":{"id":"RbavaRlmCEcj"},"source":["# Domain-Adapt with Trainer API"]},{"cell_type":"markdown","metadata":{"id":"lkZ9ETuFWpYv"},"source":["Let's draw a sample of the original dataset, so that we don't have to wait toooo long."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1-xfcZ0nF1HZ"},"outputs":[],"source":["train_size = 10000\n","test_size = int(0.1 * train_size)\n","\n","downsampled_dataset = lm_datasets[\"train\"].train_test_split(\n","    train_size=train_size, test_size=test_size, seed=42\n",")\n","\n","downsampled_dataset"]},{"cell_type":"markdown","metadata":{"id":"y8o-5zVwWxOp"},"source":["For causal language modeling, we don't need a data collator. During training, the labels will be automatically shifted to right by one position so that the task is to predict the token at timestep `t+1`, using all tokens up to `t`."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ZnW51vPvEzuZ"},"outputs":[],"source":["batch_size = 32\n","logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n","\n","training_args = TrainingArguments(\n","    output_dir=f\"{model_name}-clm-finetuned-imdb\",\n","    overwrite_output_dir=True,\n","    evaluation_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    weight_decay=0.01,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    logging_steps=logging_steps,\n",")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WT3_8iHrFxk6"},"outputs":[],"source":["trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=downsampled_dataset[\"train\"],\n","    eval_dataset=downsampled_dataset[\"test\"]\n",")"]},{"cell_type":"markdown","metadata":{"id":"kRWCKhXrXNJZ"},"source":["Perplexity of the pre-trained, but not domain-adapted model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qucBz0VWGeyW"},"outputs":[],"source":["eval_results = trainer.evaluate()\n","print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"]},{"cell_type":"markdown","metadata":{"id":"wLL1CdVTXRud"},"source":["Perform domain adaptation!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wWuvTYHiKKjG"},"outputs":[],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"pCJedTVWXT10"},"source":["Calculate perplexity for the domain-adapetd model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oxUUMnk-KT34"},"outputs":[],"source":["eval_results = trainer.evaluate()\n","print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"]},{"cell_type":"markdown","metadata":{"id":"-twj2CnCKtQv"},"source":["# Testdrive the Domain-adapted Model ðŸ›«"]},{"cell_type":"markdown","metadata":{"id":"dry4Jk4lXa0X"},"source":["Let's see if the text generated by the domain-adapted model differs from the text generated by the original model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4BxqEtEUXJUI"},"outputs":[],"source":["input_txt = \"Bob and Clara are great\"\n","input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n","output = model.generate(input_ids, max_length=64, do_sample=True)\n","print(tokenizer.decode(output[0]))"]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","private_outputs":true,"provenance":[{"file_id":"1kHkaqxz9sVdaOC2i4FJVOOFW9cCiBkYu","timestamp":1636383829898}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}
