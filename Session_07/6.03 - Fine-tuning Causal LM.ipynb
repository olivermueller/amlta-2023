{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"},"colab":{"private_outputs":true,"provenance":[{"file_id":"1kHkaqxz9sVdaOC2i4FJVOOFW9cCiBkYu","timestamp":1636383829898}],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"LJrKCbjZsqpo"},"source":["# <font color=\"#003660\">Applied Machine Learning for Text Analysis (M.184.5331)</font>\n"]},{"cell_type":"markdown","metadata":{"id":"V-DU0hkyVyPi"},"source":["# <font color=\"#003660\">Week 6: Generating Texts with Transformers</font>"]},{"cell_type":"markdown","metadata":{"id":"mhy42GjRV3ON"},"source":["# <font color=\"#003660\">Notebook 3: Fine-tuning a Causal Language Model</font>\n","\n","<center><br><img width=256 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/dag.png\"/><br></center>\n","\n","<p>\n","<center>\n","<div>\n","    <font color=\"#085986\"><b>By the end of this lesson, you ...</b><br><br>\n","        ... will be able to fine-tune a causal language model on your own data, which is useful for training decoder models.\n","    </font>\n","</div>\n","</center>\n","</p>"]},{"cell_type":"markdown","source":["The following content is heavily inspired by the following excellent sources:\n","\n","\n","*   Tunstall et al. (2021): Natural Language Processing with Transformers. O'Reilly. https://www.oreilly.com/library/view/natural-language-processing/9781098103231/\n","*   Hugging Face (2021): Transformer Models - Hugging Face Course. https://huggingface.co/course/\n","\n"],"metadata":{"id":"B8WyaSVOepeR"}},{"cell_type":"markdown","source":["# Recall: What is a Causal Language Model?"],"metadata":{"id":"euTwZ6Bne4E3"}},{"cell_type":"markdown","source":["In the previous notebook we fine-tuned a **masked language model**, where the task is to predict a missing token in a sequence of tokens. This training task is useful **for training encoder models**.\n","\n","<center><img width=300 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/mlm.png\"/><br></center>\n","\n","In this notebook, we will fine-tune a **causal language model**, which has the task to predict the next token in a sequence of tokens. This is useful **for training decoder models**.\n","\n","<center><img width=400 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/clm.png\"/><br></center>"],"metadata":{"id":"pbh9AOUcaEZM"}},{"cell_type":"markdown","source":["# Import Packages"],"metadata":{"id":"QT_dOs1v-J8Q"}},{"cell_type":"code","metadata":{"id":"OFZTIYq-Nlbp"},"source":["!pip install transformers[sentencepiece]\n","!pip install datasets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mMrhkr83sqpt"},"source":["import pandas as pd\n","import numpy as np\n","import math\n","import torch\n","from transformers import AutoTokenizer, AutoModelForCausalLM\n","from datasets import load_dataset\n","from transformers import TrainingArguments\n","from transformers import Trainer"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load Pre-trained Model"],"metadata":{"id":"2jQsLKn7eUOz"}},{"cell_type":"markdown","source":["First, we load a model for causal language modeling and a corresponding tokenizer from the model hub."],"metadata":{"id":"HvbQyVC3mqHu"}},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"],"metadata":{"id":"3-jKbucA0aqX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_name = \"distilgpt2\""],"metadata":{"id":"Xc4dKH1W0fR5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n","tokenizer = AutoTokenizer.from_pretrained(model_name)"],"metadata":{"id":"w6MugtY50fZG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Testdrive the Model ðŸš—"],"metadata":{"id":"4Il-svkt_iuS"}},{"cell_type":"code","source":["input_txt = \"Bob and Clara are great\""],"metadata":{"id":"ebiAflHnWUU1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n","output = model.generate(input_ids, max_length=64, do_sample=True)\n","print(tokenizer.decode(output[0]))"],"metadata":{"id":"Paubng0vVvKz"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Prepare a Dataset for Fine-tuning"],"metadata":{"id":"FlUfJ6d_Aq7y"}},{"cell_type":"markdown","source":["The following data preparation steps are the same as for masked language modeling."],"metadata":{"id":"kZTx2gvRWiJM"}},{"cell_type":"code","source":["imdb_dataset = load_dataset(\"imdb\")\n","imdb_dataset"],"metadata":{"id":"xfTJVsXKAvFo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def tokenize_function(examples):\n","    result = tokenizer(examples[\"text\"])\n","    return result"],"metadata":{"id":"99ffcJmtA7CS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenized_datasets = imdb_dataset.map(\n","    tokenize_function, batched=True, remove_columns=[\"text\", \"label\"]\n",")\n","tokenized_datasets"],"metadata":{"id":"-AkA-0xPBRsy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["chunk_size = 128\n","\n","def group_texts(examples):\n","    # Concatenate all texts\n","    concatenated_examples = {k: sum(examples[k], []) for k in examples.keys()}\n","    # Compute length of concatenated texts\n","    total_length = len(concatenated_examples[list(examples.keys())[0]])\n","    # We drop the last chunk if it's smaller than chunk_size\n","    total_length = (total_length // chunk_size) * chunk_size\n","    # Split by chunks of max_len\n","    result = {\n","        k: [t[i : i + chunk_size] for i in range(0, total_length, chunk_size)]\n","        for k, t in concatenated_examples.items()\n","    }\n","    # Create a new labels column\n","    result[\"labels\"] = result[\"input_ids\"].copy()\n","    return result"],"metadata":{"id":"_qFzWOpGBwF1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lm_datasets = tokenized_datasets.map(group_texts, batched=True)\n","lm_datasets"],"metadata":{"id":"VME0h5-vBySX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lm_datasets[\"train\"][1][\"input_ids\"][0:10]"],"metadata":{"id":"NwTucienDH6J"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["lm_datasets[\"train\"][1][\"labels\"][0:10]"],"metadata":{"id":"2d9DeLm1CShX"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Fine-tune with Trainer API"],"metadata":{"id":"RbavaRlmCEcj"}},{"cell_type":"markdown","source":["Let's draw a sample of the original dataset, so that we don't have to wait toooo long."],"metadata":{"id":"lkZ9ETuFWpYv"}},{"cell_type":"code","source":["train_size = 10000\n","test_size = int(0.1 * train_size)\n","\n","downsampled_dataset = lm_datasets[\"train\"].train_test_split(\n","    train_size=train_size, test_size=test_size, seed=42\n",")\n","\n","downsampled_dataset"],"metadata":{"id":"1-xfcZ0nF1HZ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For causal language modeling, we don't need a data collator. During training, the labels will be automatically shifted to right by one position so that the task is to predict the token at timestep `t+1`, using all tokens up to `t`."],"metadata":{"id":"y8o-5zVwWxOp"}},{"cell_type":"code","source":["batch_size = 32\n","logging_steps = len(downsampled_dataset[\"train\"]) // batch_size\n","\n","training_args = TrainingArguments(\n","    output_dir=f\"{model_name}-clm-finetuned-imdb\",\n","    overwrite_output_dir=True,\n","    evaluation_strategy=\"epoch\",\n","    learning_rate=2e-5,\n","    weight_decay=0.01,\n","    per_device_train_batch_size=batch_size,\n","    per_device_eval_batch_size=batch_size,\n","    logging_steps=logging_steps,\n",")"],"metadata":{"id":"ZnW51vPvEzuZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=downsampled_dataset[\"train\"],\n","    eval_dataset=downsampled_dataset[\"test\"]\n",")"],"metadata":{"id":"WT3_8iHrFxk6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Perplexity of the pre-trained, but not fine-tuned model."],"metadata":{"id":"kRWCKhXrXNJZ"}},{"cell_type":"code","source":["eval_results = trainer.evaluate()\n","print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"],"metadata":{"id":"qucBz0VWGeyW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Perform fine-tuning!"],"metadata":{"id":"wLL1CdVTXRud"}},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"wWuvTYHiKKjG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Calculate perplexity for the fine-tuned model."],"metadata":{"id":"pCJedTVWXT10"}},{"cell_type":"code","source":["eval_results = trainer.evaluate()\n","print(f\">>> Perplexity: {math.exp(eval_results['eval_loss']):.2f}\")"],"metadata":{"id":"oxUUMnk-KT34"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Testdrive the Fine-tuned Model ðŸ›«"],"metadata":{"id":"-twj2CnCKtQv"}},{"cell_type":"markdown","source":["Let's see if the text generated by the fine-tuned model differs from the text generated by the original model."],"metadata":{"id":"dry4Jk4lXa0X"}},{"cell_type":"code","source":["input_txt = \"Bob and Clara are great\"\n","input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n","output = model.generate(input_ids, max_length=64, do_sample=True)\n","print(tokenizer.decode(output[0]))"],"metadata":{"id":"4BxqEtEUXJUI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"85e-2TFhePHn"},"execution_count":null,"outputs":[]}]}