{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"},"colab":{"private_outputs":true,"provenance":[{"file_id":"1kHkaqxz9sVdaOC2i4FJVOOFW9cCiBkYu","timestamp":1636383829898}],"machine_shape":"hm","collapsed_sections":["V-DU0hkyVyPi"]},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"LJrKCbjZsqpo"},"source":["# <font color=\"#003660\">Applied Machine Learning for Text Analysis (M.184.5331)</font>\n"]},{"cell_type":"markdown","metadata":{"id":"V-DU0hkyVyPi"},"source":["# <font color=\"#003660\">Week 7: Named Entity Recognition (NER) with Transformers</font>"]},{"cell_type":"markdown","metadata":{"id":"mhy42GjRV3ON"},"source":["# <font color=\"#003660\">Notebook 1: Fine-tuning a NER model</font>\n","\n","<center><br><img width=256 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/dag.png\"/><br></center>\n","\n","<p>\n","<center>\n","<div>\n","    <font color=\"#085986\"><b>By the end of this lesson, you ...</b><br><br>\n","        ... you understand the differences between sequence and token classification, <br>\n","        ... know how to fine-tune a NER model on labelled data.\n","    </font>\n","</div>\n","</center>\n","</p>"]},{"cell_type":"markdown","source":["The following content is heavily inspired by the following excellent sources:\n","\n","\n","*   Tunstall et al. (2021): Natural Language Processing with Transformers. O'Reilly. https://www.oreilly.com/library/view/natural-language-processing/9781098103231/\n","*   Hugging Face (2021): Transformer Models - Hugging Face Course. https://huggingface.co/course/\n","\n"],"metadata":{"id":"B8WyaSVOepeR"}},{"cell_type":"markdown","source":["# Token vs. Sequence Classification"],"metadata":{"id":"euTwZ6Bne4E3"}},{"cell_type":"markdown","source":["Token classification assigns a label to individual tokens in a sentence. One of the most common token classification tasks is Named Entity Recognition (NER). NER attempts to find a label for each entity in a sentence, such as a person, location, or organization.\n","\n","Watch the Hugging Face YouTube video below to learn more about token classification / NER."],"metadata":{"id":"nPnOBt7jDs5i"}},{"cell_type":"code","source":["from IPython.display import YouTubeVideo\n","YouTubeVideo('wVHdVlPScxA')"],"metadata":{"id":"LmjG9wVOXX3O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The figures below show the main differences of using encoders like BERT and its sibblings and cousins for sequence and token classification. When doing **sequence classification** (e.g., sentiment analysis), we feed a sequence into the model and only work with the contextual embedding of the [CLS] token when training the classification head of the model. In contrast, when doing **token classification** we feed the contextual embeddings of all tokens through the classification head of the model. "],"metadata":{"id":"Z8NxJHRgVp8H"}},{"cell_type":"markdown","source":["<center><img width=500 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/seq_class.png\"/><br></center>\n","\n","<hr>\n","\n","<center><img width=500 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/token_class.png\"/><br></center>"],"metadata":{"id":"uvBIe3QmXyS-"}},{"cell_type":"markdown","metadata":{"id":"C6vVpwIFsqps"},"source":["# Import Packages\n","\n","As always, we first need to load a number of required Python packages:\n","- `pandas` provides high-performance, easy-to-use data structures and data analysis tools.\n","- `numpy` is a library adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\n","- `torch` is an open source machine learning library used for applications such as computer vision and natural language processing, primarily developed by Facebook's AI Research lab. \n","- `transformers` provides general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages.\n","- `datasets` and `evaluate` are libraries from Hugging Face to feed Transformers with data and evaluate their predictive accuracy."]},{"cell_type":"code","metadata":{"id":"OFZTIYq-Nlbp"},"source":["!pip install transformers datasets evaluate seqeval"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mMrhkr83sqpt"},"source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","from huggingface_hub import notebook_login\n","from transformers import AutoTokenizer\n","from transformers import DataCollatorForTokenClassification\n","from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n","from transformers import pipeline\n","from datasets import load_dataset, load_metric\n","import evaluate"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load data"],"metadata":{"id":"Qy8yT9LPEmQY"}},{"cell_type":"markdown","source":["In this notebook we will use the WNUT 17 dataset, whcih is a NER dataset focusing on emerging and of rare entities. You can find more information about the dataset here: https://huggingface.co/datasets/wnut_17"],"metadata":{"id":"pB91kYyKZ1eB"}},{"cell_type":"code","source":["wnut = load_dataset(\"wnut_17\")"],"metadata":{"id":"baRzuVjMEw3z"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wnut"],"metadata":{"id":"-VOpvQmjE1xW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["wnut[\"train\"][0]"],"metadata":{"id":"Z9LjjdRwE2X3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Each number in `ner_tags` represents a type of named entity (e.g., location, person). We can convert the numbers to textual labels to learn more about those entities."],"metadata":{"id":"ilA4t0ydFMJQ"}},{"cell_type":"code","source":["label_list = wnut[\"train\"].features[\"ner_tags\"].feature.names\n","label_list"],"metadata":{"id":"K82K22FVFByE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The prefixes of the tags indicate whether a given token signifies the beginning or middle/end of a named entity:\n","\n","* **B-**: indicates the beginning of an entity.\n","* **I-**: indicates a token is contained inside the same entity (for example, the State token is a part of an entity like Empire State Building).\n","* **O**: indicates that the token doesn’t correspond to any entity."],"metadata":{"id":"1kVuOiTWFJlj"}},{"cell_type":"markdown","source":["# Preprocess data"],"metadata":{"id":"Yo8znf3rEuoJ"}},{"cell_type":"markdown","source":["As always, the first thing to do when processing raw texts with Transformers is to tokenize the sequences. First, we need to load a tokenizer that is compatible with the architecture we want to use (here: DistilBERT)."],"metadata":{"id":"KT87Q8TiGCrX"}},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"],"metadata":{"id":"GRKsApeBEwAG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's re-join one example from the training set and feed it through the tokenizer. This mimics how we would use the tokenizer on new data. "],"metadata":{"id":"oi22MiALauGE"}},{"cell_type":"code","source":["tokenized_input = tokenizer(\" \".join(wnut[\"train\"][0][\"tokens\"]))\n","tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n","tokens"],"metadata":{"id":"eSPZ9yBbGOSa"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As expected, the tokenizer performed sub-word tokenization, splitting, for example, `ESB` into `es` and `##b`. In addition, the tokenizer added the usual `[CLS]` and `[SEP]` tokens."],"metadata":{"id":"CZ8fil5oGkeu"}},{"cell_type":"markdown","source":["As a result, the tokenized sequence and the labels of our training data (which has NOT been tokenized with sub-word tokenization) are not aligend anymore."],"metadata":{"id":"DihpmRxPbOA5"}},{"cell_type":"code","source":["len(tokens)"],"metadata":{"id":"FzBLy_UhGboT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["len(wnut[\"train\"][0][\"tokens\"])"],"metadata":{"id":"944Ck3W_GdiY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Hence, we need to realign the tokens and labels following the following process:\n","\n","1. Mapping all tokens to their corresponding word with the `word_ids` method.\n","2. Assigning the label `-100` to the special tokens `[CLS]` and `[SEP]`, so that they will be ignored by the loss function.\n","3. Only labeling the first token of a given word. Assign `-100` to other subtokens from the same word."],"metadata":{"id":"j7d_UuZFHJBA"}},{"cell_type":"markdown","source":["\n","Below is a function to realign the tokens and labels, and truncate sequences that are longer than our model's maximum sequence length."],"metadata":{"id":"UgwsGOD6HRCA"}},{"cell_type":"code","source":["def tokenize_and_align_labels(examples):\n","    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n","\n","    labels = []\n","    for i, label in enumerate(examples[f\"ner_tags\"]):\n","        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n","        previous_word_idx = None\n","        label_ids = []\n","        for word_idx in word_ids:  # Set the special tokens to -100.\n","            if word_idx is None:\n","                label_ids.append(-100)\n","            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n","                label_ids.append(label[word_idx])\n","            else:\n","                label_ids.append(-100)\n","            previous_word_idx = word_idx\n","        labels.append(label_ids)\n","\n","    tokenized_inputs[\"labels\"] = labels\n","    return tokenized_inputs"],"metadata":{"id":"ncJI_vFSH1mf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Apply the function to the whole dataset (i.e., train, validation, and test)."],"metadata":{"id":"Qhwa2rcZbfi5"}},{"cell_type":"code","source":["tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True)"],"metadata":{"id":"PLkhx6YwH595"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Check whether the sequences are aligned now."],"metadata":{"id":"xs9Lj5wSbmZb"}},{"cell_type":"code","source":["tokenized_wnut[\"train\"][0]"],"metadata":{"id":"9fNik2uPH94C"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["To preprocess our texts on-the-fly while training our model, we can use a data collator (more information: https://huggingface.co/docs/transformers/main_classes/data_collator)."],"metadata":{"id":"5AP8vru5bwPu"}},{"cell_type":"code","source":["data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"],"metadata":{"id":"NnPHiajMOBHD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Fine-tune model"],"metadata":{"id":"FvQn7RWeKTMM"}},{"cell_type":"markdown","source":["Now we are (almost) ready to fine-tune a pre-trained DistilBERT on our emerging and rare named entities dataset. We will also see how to publish our model (checkpoint + tokenizer) on the Hugging Face dataset hub and download it again."],"metadata":{"id":"q_aHYIqjb4lv"}},{"cell_type":"markdown","source":["Let's first log into the Hugging Face dataset hub."],"metadata":{"id":"78xkVk18cOi-"}},{"cell_type":"code","source":["notebook_login()"],"metadata":{"id":"KTTE2Bs5Yzhj"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["As always, when training a model we need a compute_metrics function to calculate and display selected accuracy metrics during training. Here we use precision, recall, F1, and accuracy (they are all saved together in the seqeval evaluation object)."],"metadata":{"id":"J2cCrNtrcShN"}},{"cell_type":"code","source":["seqeval = evaluate.load(\"seqeval\")\n","\n","def compute_metrics(p):\n","    predictions, labels = p\n","    predictions = np.argmax(predictions, axis=2)\n","\n","    true_predictions = [\n","        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    true_labels = [\n","        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","\n","    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n","    return {\n","        \"precision\": results[\"overall_precision\"],\n","        \"recall\": results[\"overall_recall\"],\n","        \"f1\": results[\"overall_f1\"],\n","        \"accuracy\": results[\"overall_accuracy\"],\n","    }"],"metadata":{"id":"vB4HeCstKjVH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["In order to be able to interpret the model's predictions, we also need two dictionaries to be able to translate between the textual labels and their indices."],"metadata":{"id":"8WkJ8AMHcrDq"}},{"cell_type":"code","source":["id2label = {\n","    0: \"O\",\n","    1: \"B-corporation\",\n","    2: \"I-corporation\",\n","    3: \"B-creative-work\",\n","    4: \"I-creative-work\",\n","    5: \"B-group\",\n","    6: \"I-group\",\n","    7: \"B-location\",\n","    8: \"I-location\",\n","    9: \"B-person\",\n","    10: \"I-person\",\n","    11: \"B-product\",\n","    12: \"I-product\",\n","}\n","\n","label2id = {\n","    \"O\": 0,\n","    \"B-corporation\": 1,\n","    \"I-corporation\": 2,\n","    \"B-creative-work\": 3,\n","    \"I-creative-work\": 4,\n","    \"B-group\": 5,\n","    \"I-group\": 6,\n","    \"B-location\": 7,\n","    \"I-location\": 8,\n","    \"B-person\": 9,\n","    \"I-person\": 10,\n","    \"B-product\": 11,\n","    \"I-product\": 12,\n","}"],"metadata":{"id":"XcIof8DJK3Qu"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["🚀 🚀 🚀 Now we are really ready to fine-tune... 🚀 🚀 🚀"],"metadata":{"id":"zLWqEVeVc04e"}},{"cell_type":"markdown","source":["Load the pre-trained model, configure the classification head (how many labels?), and pass the functions to translate between label IDs and textual labels."],"metadata":{"id":"agR3A0wkc5EY"}},{"cell_type":"code","source":["model = AutoModelForTokenClassification.from_pretrained(\n","    \"distilbert-base-uncased\", num_labels=len(label2id), id2label=id2label, label2id=label2id\n",")"],"metadata":{"id":"IFGaZp6dK6WQ"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Configure the Trainer. Here, we will only train for 2 epochs. In reality, you should train much longer! And with more data!"],"metadata":{"id":"J7txAFUGdVGy"}},{"cell_type":"code","source":["training_args = TrainingArguments(\n","    output_dir=\"my_awesome_wnut_model\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=2,\n","    weight_decay=0.01,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    load_best_model_at_end=True,\n","    push_to_hub=True,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_wnut[\"train\"],\n","    eval_dataset=tokenized_wnut[\"test\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")"],"metadata":{"id":"Yc1ifPK8N5Y6"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Go! 🏁"],"metadata":{"id":"rJUieJIzdl-p"}},{"cell_type":"code","source":["trainer.train()"],"metadata":{"id":"m-7CvqpmT5tI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Save and publish the results on the Hugging Face ☁️"],"metadata":{"id":"eYzau5Fgdorb"}},{"cell_type":"code","source":["trainer.push_to_hub()"],"metadata":{"id":"uojBHzC_PEZD"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Inference"],"metadata":{"id":"pTc0_H2rT_2-"}},{"cell_type":"markdown","source":["## Using a Pipeline"],"metadata":{"id":"SKwFkGbCUkEc"}},{"cell_type":"markdown","source":["In the following cells, we instantiate a pipeline, which integrates a checkpoint (=trained model) and compatible tokenizer, and feed it with a sample sentence."],"metadata":{"id":"yqKA8Kpzdz-K"}},{"cell_type":"code","source":["text = \"The Golden State Warriors are an American professional basketball team based in San Francisco.\""],"metadata":{"id":"WLiCAVjzPQCJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ner_classifier = pipeline(\"ner\", model=\"olivermueller/my_awesome_wnut_model\")"],"metadata":{"id":"TF4Rn93mJF6B"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["ner_classifier(text)"],"metadata":{"id":"WwSox4_2ZAud"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## By Hand"],"metadata":{"id":"TeVcDvp7UmIJ"}},{"cell_type":"markdown","source":["Instead of using a pipeline, we can also compute predictions step-by-step. Reproducing each of the steps by hand ✍️ will increase your understanding of the underlying logic."],"metadata":{"id":"GePBKSCvVAaX"}},{"cell_type":"markdown","source":["Load only the tokenizer."],"metadata":{"id":"8n2aGqXweMD9"}},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(\"olivermueller/my_awesome_wnut_model\")"],"metadata":{"id":"Z1P5Wk1lUr4A"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Tokenize the example sentence."],"metadata":{"id":"FxFSyooQeO1V"}},{"cell_type":"code","source":["inputs = tokenizer(text, return_tensors=\"pt\")\n","inputs"],"metadata":{"id":"Hod_oPTKUtcr"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load the checkpoint."],"metadata":{"id":"MKUAzdSBeQ2o"}},{"cell_type":"code","source":["model = AutoModelForTokenClassification.from_pretrained(\"olivermueller/my_awesome_wnut_model\")"],"metadata":{"id":"M8Ppwvp4UuqP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Do a forward pass through the network."],"metadata":{"id":"qIfVUJ66eTwq"}},{"cell_type":"code","source":["with torch.no_grad():\n","    logits = model(**inputs).logits\n","logits"],"metadata":{"id":"3n76RB6lUwoE"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["For each token, get the index of the label with the highest score."],"metadata":{"id":"0RpWM84ReYBn"}},{"cell_type":"code","source":["predictions = torch.argmax(logits, dim=2)\n","predictions"],"metadata":{"id":"5FuJWnhJUx4o"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Translate the indices to textual labels."],"metadata":{"id":"dMtc-L1celF5"}},{"cell_type":"code","source":["predicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]\n","predicted_token_class"],"metadata":{"id":"yrsyEd6nUypj"},"execution_count":null,"outputs":[]}]}