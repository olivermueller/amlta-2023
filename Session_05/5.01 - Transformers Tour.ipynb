{"cells":[{"cell_type":"markdown","metadata":{"id":"LJrKCbjZsqpo"},"source":["# <font color=\"#003660\">Applied Machine Learning for Text Analysis (M.184.5331)</font>\n"]},{"cell_type":"markdown","metadata":{"id":"V-DU0hkyVyPi"},"source":["# <font color=\"#003660\">Week 5: Transformer Architecture</font>"]},{"cell_type":"markdown","metadata":{"id":"mhy42GjRV3ON"},"source":["# <font color=\"#003660\">Notebook 1: A Tour of Hugging Face Transformers</font>\n","\n","<center><br><img width=256 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/dag.png\"/><br></center>\n","\n","<p>\n","<center>\n","<div>\n","    <font color=\"#085986\"><b>By the end of this lesson, you ...</b><br><br>\n","        ... will have a high-level understanding of the Transformer architecture, <br>\n","        ... will know the basic types of Transformers (i.e., encoder, decoder, encoder-decoder), and <br>\n","        ... will know how to use pre-trained NLP pipeline (e.g., sentiment analysis, NER, translation) from the Hugging Face Transformers library.\n","    </font>\n","</div>\n","</center>\n","</p>"]},{"cell_type":"markdown","metadata":{"id":"gfuL8MTlTSjy"},"source":["The following content is heavily inspired by the following excellent sources:\n","\n","\n","*   Tunstall et al. (2021): Natural Language Processing with Transformers. O'Reilly. https://www.oreilly.com/library/view/natural-language-processing/9781098103231/\n","*   Hugging Face (2021): Transformer Models - Hugging Face Course. https://huggingface.co/course/\n","\n"]},{"cell_type":"markdown","metadata":{"id":"Hr1lI3joctE6"},"source":["# What are Transformers?"]},{"cell_type":"markdown","metadata":{"id":"-3SqfNuOSBT1"},"source":["## Overview"]},{"cell_type":"markdown","metadata":{"id":"9y5H7UKKYJi_"},"source":["The general **Transformer** architecture consists of two components:\n","\n","*   **Encoder**: The encoder processes an input sequence and builds a numerical representation (feature vector) of it. This means that this component is optimized for understanding the input.\n","*   **Decoder**: The decoder processes the feature vector produced by the encoder, plus other inputs, to generate an output sequence. This means that this component is optimized for generating outputs.\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"WgYksqATWO_1"},"source":["<center><br><img width=256 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/architecture_high-level.png\"/><br></center>"]},{"cell_type":"markdown","metadata":{"id":"b9aPRtWUZLj5"},"source":["Depending on the task at hand, the components of the Transformer architecture can be used independently or in combination:\n","\n","*   **Encoders**: Good for tasks that require understanding of the input, such as *text classification* and *named entity recognition*. Watch [this video](https://youtu.be/MUqNwgPjJvQ) to learn more.\n","*   **Decoders**: Good for generative tasks such as *text generation*. Watch [this video](https://youtu.be/d_ixlCubqQw) to learn more.\n","*   **Encoder-decoder Models** (aka sequence-to-sequence models): Good for generative tasks that require an input, such as *translation* or *summarization*. Watch [this video](https://youtu.be/0_4KEb08xrE) to learn more.\n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"1nfI55UFWyV7"},"source":["<center><br><img width=500 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/transformer_models.png\"/><br></center>"]},{"cell_type":"markdown","metadata":{"id":"FEvS6t7-SWom"},"source":["## Attention is all you need!"]},{"cell_type":"markdown","metadata":{"id":"wn_xYT53al8n"},"source":["In general, the encoder and decoder components can be any kind of neural network architecture that is suited for modeling sequences. For example, in the diagram below, simple recurrent neural networks (RNN) are used to implement the encoder and decoder components.\n","\n","In this example, the English sentence “Transformers are great!” is encoded into a hidden state vector that is then decoded to produce the German translation “Transformer sind grossartig!” (Note: besides the hidden state, the decoder normally also uses the already generated output tokens as an additional input).\n","\n","Note how this network processes inputs and generates outputs in a sequential way, indicated the vertical lines between the RNN cells. This sequential processing is slow, as it cannot take full advantage of the parallel processing capabilities of a GPU."]},{"cell_type":"markdown","metadata":{"id":"4yXrToPYW_Xs"},"source":["<center><br><img width=600 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/transformer_rnn.png\"/><br></center>"]},{"cell_type":"markdown","metadata":{"id":"vDULSC7ocLbw"},"source":["One weakness with the above architecture is that only the final hidden state of the encoder is passed to the decoder, which creates an information bottleneck. The meaning of the whole input sequence has to be captured in in just one vector, which is especially challenging for long sequences where information at the start of the sequence might be lost in the process of creating a single vector representation.\n","\n","Of course, a straight forward way to avoid this bottleneck is to pass all of the encoder’s hidden states to the decorder (not shown in the diagrams). This way, almost no information would be lost. Yet, at the same time we would probably pass a lot of irrelevant information to the decoder.\n","\n","A meet-in-the-middle approach between the two extremes (passing just one hidden state vs. passing all hidden states) is a mechanism called **Attention**. Attention allows the decoder to assign a weight to each individual hidden input state (see diagram below), indicating which states are important or unimportant for producing the next element of the output sequence. Like all other weights of the network, the attention weights are learned during training."]},{"cell_type":"markdown","metadata":{"id":"ILfPSxnjcLvG"},"source":["<center><br><img width=600 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/transformer_att1.png\"/><br></center>"]},{"cell_type":"markdown","metadata":{"id":"XS1wGsUOjdVD"},"source":["The figure below visualizes the attention weights for an English-to-French translation model. Note how the decoder is able to correctly align the words “zone” and “Area”, which are ordered differently in the two languages."]},{"cell_type":"markdown","metadata":{"id":"E49JFt-ejoTt"},"source":["<center><br><img width=500 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/att_translation.png\"/><br></center>"]},{"cell_type":"markdown","metadata":{"id":"IA95n4phbwdP"},"source":["In the famous \"[Attention is all you need paper](http://papers.nips.cc/paper/7181-attention-is-all-you-%0Aneed.pdf)\", Vaswani et al. showed that the RNNs inside the encoder and decoder components can be replaced entirely with Attention and feed-forward layers.\n","\n","This architecture has three main advantages:\n","\n","* First, without RNN cells, all tokens can be fed in parallel through the model, which makes the model faster and allows to train it on larger corpora.\n","\n","* Second, the Attention mechanism makes the network more effective on tasks that require memorizing information over long time sequences.\n","\n","* Third, the Attention mechanisms creates a representation for each token that is dependent on its surrounding tokens. This makes the representation of each token context aware, such that the representation of the word “apple” (fruit) is different from “apple” (computer manufacturer)."]},{"cell_type":"markdown","metadata":{"id":"-cFRZMu1XNJL"},"source":["<center><br><img width=600 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/transformer_att2.png\"/><br></center>"]},{"cell_type":"markdown","metadata":{"id":"5MLUJO9FgT6Z"},"source":["In modern Transformer architectures, several blocks of Attention and feed-forward layers are stacked in the encoder to produce rich hidden states which are then passed to the decoder.\n","\n","This information should be enough to build an intuitive understanding of Attention. If you want to learn more about its technical details, read Chapter 3 of Tunstall et al. (2021)."]},{"cell_type":"markdown","metadata":{"id":"fSlAVQz2TQb7"},"source":["## Transfer Learning"]},{"cell_type":"markdown","metadata":{"id":"OqDJmt2h-QKH"},"source":["Another feature of Transformers is this use of transfer learning. Transfer learning is a machine learning approach that involves applying knowledge gained from solving one problem to solve a different but related problem.\n","\n","This usually works by splitting a model in terms of a **body** and **head**. During pretraining, the weights of the body are optimized to represent broad features of the source domain (e.g., vocabulary, grammar). These weights are then used to initialize the new model for the new task. The head is a task-specific network that is only trained during fine-tuning.\n","\n","The figure below illustrates this idea and contrasts it with traditional machine learning. For example, in the models on the right Body A could be pretrained with a language modeling task in Domain A and the used to initialize Body B of a sentiment analysis model in Domain B. The weights of Head B, the sentiment classifier, are learned from scratch.\n","\n","<center><br><img width=700 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/transfer_learning.png\"/><br></center>\n","\n","Transfer learning typically produces models that can be fine-tuned efficiently on a variety of downstream tasks (i.e., with less time and data)."]},{"cell_type":"markdown","metadata":{"id":"C6vVpwIFsqps"},"source":["# Import Packages\n","\n","As always, we first need to load a number of required Python packages:\n","- `pandas` provides high-performance, easy-to-use data structures and data analysis tools.\n","- `numpy` is a library adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\n","- `sklearn` is a free software machine learning library for the Python programming language.\n","- `transformers` provides general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OFZTIYq-Nlbp"},"outputs":[],"source":["!pip install transformers[sentencepiece]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mMrhkr83sqpt"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn import metrics\n","from transformers import pipeline"]},{"cell_type":"markdown","metadata":{"id":"UcjGHY-JPxvp"},"source":["# Pre-trained Pipelines"]},{"cell_type":"markdown","metadata":{"id":"40nQxzBjFNVp"},"source":["In the following, we will use some pre-trained NLP pipelines from the Hugging Face Transformers library.\n","\n","These pipelines are an easy way to use models for inference. They abstract most of the complex code from the library (e.g., code for tokenizing input sequences or for training loops), offering a simple API dedicated to specific tasks.\n","\n","Below are some examples of pre-trained Transformer pipelines. For more, see: https://huggingface.co/transformers/main_classes/pipelines.html"]},{"cell_type":"markdown","metadata":{"id":"4hE7EoJ8Nb23"},"source":["## Sentiment Analysis"]},{"cell_type":"markdown","metadata":{"id":"YZqbsHIlBX3j"},"source":["The sentiment analysis pipeline uses a model that was fine-tuned on the Stanford Sentiment Treebank, which is an English corpus of annotated movie reviews. This is an example of an encoder model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wtWngfscOCt1"},"outputs":[],"source":["sa_classifier = pipeline(\"sentiment-analysis\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ACE37--bNi0d"},"outputs":[],"source":["sa_classifier([\n","            \"This is the best movie I have ever seen.\",\n","            \"I hate this movie so much!\",\n","            \"I don't like the new iPhone.\",\n","            \"Becks is not bad.\",\n","     ])"]},{"cell_type":"markdown","metadata":{"id":"Prrsvz2jP4tE"},"source":["## Named Entity Recognition"]},{"cell_type":"markdown","metadata":{"id":"BU1rS_UWCEgE"},"source":["Named entities are names of products, places or people and detecting and extracting them from text is called named entity recognition (NER). This is an example of an encoder model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PkSc8zTZP8VR"},"outputs":[],"source":["ner = pipeline(\"ner\", grouped_entities=True)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Kml2jjTCP9nh"},"outputs":[],"source":["ner(\"My name is Oliver and I work at Paderborn University.\")"]},{"cell_type":"markdown","metadata":{"id":"5LljwfGDQRLn"},"source":["## Question Answering"]},{"cell_type":"markdown","metadata":{"id":"3bSEb6pcChEW"},"source":["In question answering we provide the model with a passage of text called the context, along with a question whose answer we’d like to extract. The model then returns the span of text corresponding to the answer. This is an example of an encoder model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NDh2yAlbQTIP"},"outputs":[],"source":["question_answerer = pipeline(\"question-answering\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z3IeHN_-QUj_"},"outputs":[],"source":["question_answerer(\n","    question=\"What's my name?\",\n","    context=\"My name is Oliver and I work at Paderborn University.\",\n",")"]},{"cell_type":"markdown","metadata":{"id":"1IP8Ujv7OhW8"},"source":["## Text Generation"]},{"cell_type":"markdown","metadata":{"id":"vSeWRN2wEmBV"},"source":["Text generation is the task of generating text with the goal of appearing indistinguishable to human-written text. This is an example of a decoder model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R-qugDlLOi-m"},"outputs":[],"source":["txt_generator = pipeline(\"text-generation\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pC7S90jQO6hP"},"outputs":[],"source":["txt_generator(\"In this course, you will learn how to\",\n","              num_return_sequences = 2, max_length = 20)"]},{"cell_type":"markdown","metadata":{"id":"5a8uwt0sQsPB"},"source":["## Translation"]},{"cell_type":"markdown","metadata":{"id":"iz2y9w_eD07z"},"source":["Translation is the task of translating text or speech from one language to another. This is an example of an encoder-decoder model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XIYf8ukHQtnH"},"outputs":[],"source":["translator = pipeline(\"translation\", model=\"Helsinki-NLP/opus-mt-fr-en\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oRaS5x3hQt7T"},"outputs":[],"source":["translator(\"La bibliothèque Hugging Face est tout simplement géniale.\")"]},{"cell_type":"markdown","metadata":{"id":"oh-mTZ1AQgvZ"},"source":["## Summarization"]},{"cell_type":"markdown","metadata":{"id":"YHuDAVyyCvwk"},"source":["The goal of text summarization is to take a long text as input and generate a short version with all relevant facts. This is an example of an encoder-decoder model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"R4GSywLCQlUC"},"outputs":[],"source":["summarizer = pipeline(\"summarization\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7mHA48S7QfuX"},"outputs":[],"source":["summarizer(\n","  \"\"\"\n","  Darth Vader is a fictional character in the Star Wars franchise. The character\n","  is the primary antagonist in the original trilogy and, as Anakin Skywalker,\n","  is a primary protagonist in the prequel trilogy. Star Wars creator George\n","  Lucas has collectively referred to the first six episodic films of the\n","  franchise as \"the tragedy of Darth Vader\". He has become one of the most\n","  iconic villains in popular culture, and has been listed among the greatest\n","  villains and fictional characters ever.\n","  \"\"\",\n","  max_length = 60\n",")"]},{"cell_type":"markdown","metadata":{"id":"z2AOW8p3fquf"},"source":["## Text-to-Image Generation"]},{"cell_type":"markdown","metadata":{"id":"nloaoYLegTdc"},"source":["Conditional image generation is the task of generating new images from a dataset conditional on their class. Instead of specifying one of a pre-defined set of classes, we can also specify the \"class\" through a textual prompt."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JIdtC5jPf1VI"},"outputs":[],"source":["!pip install diffusers[\"torch\"]\n","!pip install transformers"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"48S5jkpgfuX-"},"outputs":[],"source":["from diffusers import DiffusionPipeline\n"]},{"cell_type":"markdown","metadata":{"id":"my-uL_OfgnL7"},"source":["Load diffusion model and move it to the GPU."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"obGnNksvf7U1"},"outputs":[],"source":["generator = DiffusionPipeline.from_pretrained(\"CompVis/ldm-text2im-large-256\")"]},{"cell_type":"markdown","metadata":{"id":"K0mDxl09hmtz"},"source":["Pass a prompt to the model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q7IzKg8hf-Z3"},"outputs":[],"source":["image = generator(\"An image of a dog in Van Gogh style\").images[0]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9GXzpNIqgAZ8"},"outputs":[],"source":["image.save(\"image_of_dog_painting.png\")"]}],"metadata":{"accelerator":"GPU","colab":{"private_outputs":true,"provenance":[{"file_id":"1kHkaqxz9sVdaOC2i4FJVOOFW9cCiBkYu","timestamp":1636383829898}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}
