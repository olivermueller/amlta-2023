{"cells":[{"cell_type":"markdown","metadata":{"id":"LJrKCbjZsqpo"},"source":["# <font color=\"#003660\">Applied Machine Learning for Text Analysis (M.184.5331)</font>\n"]},{"cell_type":"markdown","metadata":{"id":"V-DU0hkyVyPi"},"source":["# <font color=\"#003660\">Week 5: Transformer Architecture</font>"]},{"cell_type":"markdown","metadata":{"id":"mhy42GjRV3ON"},"source":["# <font color=\"#003660\">Notebook 2: Text Classification with Transformers</font>\n","\n","<center><br><img width=256 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/dag.png\"/><br></center>\n","\n","<p>\n","<center>\n","<div>\n","    <font color=\"#085986\"><b>By the end of this lesson, you ...</b><br><br>\n","        ... will know the difference between the feature extraction and fine.-tuning approach for text classification with Transformers, <br>\n","        ... will know how to train a feature extraction model for text classification, <br>\n","        ... will know how to train a fine-tuned model for text classification, <br>,\n","        ... will get to know the main libraries of the Hugging Face ecosystem (i.e., Datasets, Tokenizers, Transformers).\n","    </font>\n","</div>\n","</center>\n","</p>"]},{"cell_type":"markdown","metadata":{"id":"IPvSfoxIngjS"},"source":["The following content is heavily inspired by the following excellent sources:\n","\n","\n","*   Tunstall et al. (2021): Natural Language Processing with Transformers. O'Reilly. https://www.oreilly.com/library/view/natural-language-processing/9781098103231/\n","*   Hugging Face (2021): Transformer Models - Hugging Face Course. https://huggingface.co/course/\n","\n"]},{"cell_type":"markdown","metadata":{"id":"C6vVpwIFsqps"},"source":["# Import Packages\n","\n","As always, we first need to load a number of required Python packages:\n","- `pandas` provides high-performance, easy-to-use data structures and data analysis tools.\n","- `numpy` is a library adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\n","- `sklearn` is a free software machine learning library for the Python programming language.\n","- `transformers` provides general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages.\n","- `datasets` is an API for datasets from the makers of transformers.\n","- `torch` is an open source machine learning library used for applications such as computer vision and natural language processing, primarily developed by Facebook's AI Research lab."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OFZTIYq-Nlbp"},"outputs":[],"source":["!pip install transformers[sentencepiece]\n","!pip install datasets\n","!pip install accelerate -U"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mMrhkr83sqpt"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.metrics import accuracy_score, f1_score\n","from sklearn.metrics import classification_report\n","from datasets import list_datasets, load_dataset\n","from transformers import AutoTokenizer\n","from transformers import AutoModel\n","from transformers import AutoModelForSequenceClassification\n","from transformers import Trainer, TrainingArguments\n","import torch"]},{"cell_type":"markdown","metadata":{"id":"HF9NcQIOF38V"},"source":["# Overview"]},{"cell_type":"markdown","metadata":{"id":"mTmR0vobGR0l"},"source":["<center><br><img width=400 src=\"https://upload.wikimedia.org/wikipedia/en/f/f1/Bert_and_Ernie.JPG\"/><br></center>"]},{"cell_type":"markdown","metadata":{"id":"3l-voX_EF5Qw"},"source":["\n","\n","In this notebook we will use a famous Transformer model called **BERT**, short for Bidirectional Encoder Representations from Transformers.\n","\n","We will use the three core libraries from the Hugging Face ecosystem: **Datasets**, **Tokenizers**, and **Transformers**. These libraries will allow us to quickly go from raw text to a fine-tuned model that can be used for predictions on new texts."]},{"cell_type":"markdown","metadata":{"id":"8oufSV6RpCvd"},"source":["# Load and Prepare Dataset"]},{"cell_type":"markdown","metadata":{"id":"AeEBELLPGp2C"},"source":["In the next code chunks we will use the **Datasets** library to retrieve a dataset from Hugging Face's datasets hub. This library is designed to load and process large datasets efficiently and share them with the community."]},{"cell_type":"markdown","metadata":{"id":"xHu5ZrqLG0W1"},"source":["Let's start by getting a list of datasets from the hub."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WdWlrokmpDOm"},"outputs":[],"source":["datasets = list_datasets()\n","print(f\"There are {len(datasets)} datasets currently available on the Hub.\")\n","print(f\"The first 10 are: {datasets[:10]}\")"]},{"cell_type":"markdown","metadata":{"id":"VSlhyYVxG-Hb"},"source":["Load the \"emotions\" dataset, a dataset of English Twitter messages labelled with six basic emotions: anger, fear, joy, love, sadness, and surprise. More: https://huggingface.co/datasets/emotion"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rwJVxFPvpzT_"},"outputs":[],"source":["emotions = load_dataset(\"emotion\")"]},{"cell_type":"markdown","metadata":{"id":"ByhadfPTHYz-"},"source":["Explore the structure of the dataset."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B6SAG2pbp7Qs"},"outputs":[],"source":["emotions"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"faDfu2M0qMKz"},"outputs":[],"source":["emotions[\"train\"].features"]},{"cell_type":"markdown","metadata":{"id":"xqsbo3kyHb4b"},"source":["Show the first Tweet and its label."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YZMHgTVzqCyq"},"outputs":[],"source":["emotions[\"train\"][0]"]},{"cell_type":"markdown","metadata":{"id":"Ax0-WcUpHeQY"},"source":["Get the name of the first label."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3z2F2bsg7oGM"},"outputs":[],"source":["emotions[\"train\"].features[\"label\"].int2str(emotions[\"train\"][0][\"label\"])"]},{"cell_type":"markdown","metadata":{"id":"tIByOSDzHpsb"},"source":["Store a list of all label names for later."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WF8gOTUb8pA5"},"outputs":[],"source":["labels = emotions[\"train\"].features[\"label\"].names\n","labels"]},{"cell_type":"markdown","metadata":{"id":"5YhYjl4xqyTV"},"source":["# Tokenize Texts"]},{"cell_type":"markdown","metadata":{"id":"0lSWAfhxHuD8"},"source":["Like other neural networks or machine learning models, Transformer models cannot process raw strings as input; instead they assume the text has been tokenized and turned into numerical vectors.\n","\n","Most Transformers use a subword **tokenizer**. The idea behind subword tokenization is a meet-in-the-middle between character and word tokenization. On one hand we want to use characters since they allow the model to deal with rare character combinations and misspellings. On the other hand, we want to keep frequent words and word parts as unique entities.\n","\n","There are many different subword tokenization strategies. Using the right tokenizer for a given pretrained model is crucial for getting sensible results. The Transformers library provides convenient tools (e.g., AutoTokenizer, from_pretrained()) to load both objects (model and tokenizer) from the Hugging Face model hub."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8d8UYAoNrY2w"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IFsJMcZ8rgq1"},"outputs":[],"source":["tokenizer.vocab_size"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gKvG2UEtrjrt"},"outputs":[],"source":["tokenizer.model_max_length"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9khZudEcrl8j"},"outputs":[],"source":["encoded_str = tokenizer.encode(\"this is a complicatedtest\")\n","encoded_str"]},{"cell_type":"markdown","metadata":{"id":"C6rkFNq4JLUt"},"source":["Show special tokens used by the tokenizer. BERT uses the [MASK] token for the primary objective of masked language modeling and the [CLS] and [SEP] tokens for the secondary pretraining objective of predicting if two sentences are consecutive.\n","\n","Below, we can observe two things. First, the [CLS] and [SEP] tokens have been added automatically to the start and end of the sequence. Second, the long word `complicatedtest` has been split into two tokens. The ## prefix in ##test signifies that the preceding string is not a whitespace and that it should be merged with the previous token."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ilmFb1RVrpvi"},"outputs":[],"source":["for token in encoded_str:\n","    print(token, tokenizer.decode([token]))"]},{"cell_type":"markdown","metadata":{"id":"vYL980hHJtUH"},"source":["# BERT for Text Classification"]},{"cell_type":"markdown","metadata":{"id":"8N9njIOlJvmQ"},"source":["BERT has been pretrained to predict masked words and next sentences in texts. Hence, we can’t use the model as-is for text classification and have to modify its architecture slightly."]},{"cell_type":"markdown","metadata":{"id":"GAIRsBuyJ97p"},"source":["The figure below illustrates the general architecture of a BERT text classification model.\n","\n","<center><br><img width=600 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/bert.png\"/><br></center>\n","\n","First, the text is tokenized and represented as one-hot vectors whose dimension is the size of the tokenizer vocabulary (not shown in the diagram). Next, these token encodings are embedded in lower dimensions and passed through stacks of encoder layers to yield a hidden state for each input token. During pretraining, the hidden states are used for the task of language modeling. For the classification task, we replace the language modeling layer with a classification model."]},{"cell_type":"markdown","metadata":{"id":"uykU2peUJ-L0"},"source":["We have two options to train the classification model:\n","\n","* Feature extraction: We use BERT in inference mode take the hidden states as features to train an \"external\" classifier on them.\n","\n","* Fine-tuning: We add a classification head to the model and train the whole model end-to-end, which also updates the parameters of the pretrained BERT model."]},{"cell_type":"markdown","metadata":{"id":"Zr_LiWpCot7a"},"source":["# The Feature Extraction Approach"]},{"cell_type":"markdown","metadata":{"id":"A20HvS5ELJYY"},"source":["The figure below illustrates the idea behind the feature extraction approach. To use a Transformer as a feature extractor we use BERT just for inference and use the generated hidden states as features for a downstream classifier (e.g., logistic regression, random forest). The advantage of this approach is that we can quickly train a shallow model. This method is especially convenient if GPUs are unavailable since the hidden states can be computed relatively fast on a CPU.\n","\n","<center><br><img width=600 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/feature_extraction.png\"/><br></center>\n"]},{"cell_type":"markdown","metadata":{"id":"GlWiBzYenTgy"},"source":["See also Jay Alammar's blog post for using the feature-extraction approach for classification: https://jalammar.github.io/a-visual-guide-to-using-bert-for-the-first-time/"]},{"cell_type":"markdown","metadata":{"id":"V4nqFlbRrBnV"},"source":["## Model: Load and Inference"]},{"cell_type":"markdown","metadata":{"id":"rr0CvfmlL2AC"},"source":["Check if GPU is available."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GeXhxwclsHcc"},"outputs":[],"source":["device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","device"]},{"cell_type":"markdown","metadata":{"id":"vw0E67odL4_R"},"source":["(Down)Load a pretrained model and move it to the GPU. The AutoModel class initalizes an input encoder that translates the one-hot vectors to embeddings with positional encodings and feeds them through the encoder stack to return the hidden states. Note that the language model head that normally takes the hidden states and decodes them to the masked token prediction is excluded since it is only needed for pretraining."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ANW4qnDiZD-D"},"outputs":[],"source":["model_name = \"distilbert-base-uncased\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"h33LOIMc9hs7"},"outputs":[],"source":["model = AutoModel.from_pretrained(model_name).to(device)"]},{"cell_type":"markdown","metadata":{"id":"vigbRRZNMBiC"},"source":["Test the model with a single short document."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fwgGorPo9oh5"},"outputs":[],"source":["text = \"this is a test\"\n","text_tensor = tokenizer.encode(text, return_tensors=\"pt\").to(device)"]},{"cell_type":"markdown","metadata":{},"source":["The tokenized text is a tensor with the dimensionality [1, 6] (number of documents, length of the documents). "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ldSpgvWM-aaN"},"outputs":[],"source":["text_tensor.shape"]},{"cell_type":"markdown","metadata":{},"source":["The values of the tensor are integers, which are the indices of the tokens in the vocabulary (101 and 102 are special tokens that have been added during tokenization)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["text_tensor"]},{"cell_type":"markdown","metadata":{},"source":["Let's feed the tensor through the model to get the hidden states."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hm3nltvTCL80"},"outputs":[],"source":["output = model(text_tensor)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["output"]},{"cell_type":"markdown","metadata":{},"source":["The output are the hidden states of the last encoder layer for each token. Hence, they have the dimensionality [1, 6, 768] (number of documents, length of the documents, dimension of the hidden states). They are the raw contextual embeddings generated by BERT."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VQQ8GqjxMjUt"},"outputs":[],"source":["output.last_hidden_state.shape"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fxvRM5_3MeO9"},"outputs":[],"source":["output.last_hidden_state"]},{"cell_type":"markdown","metadata":{"id":"80xF_JyEMmyU"},"source":["Let’s repeat the process for the whole dataset. First, we write a simple function that will tokenize our texts. The padding=True parameter will pad the examples with zeroes to the longest one in a batch, and truncation=True will truncate the examples to the model’s maximum sequence length."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kccbnYLuCzEa"},"outputs":[],"source":["def tokenize(batch):\n","    return tokenizer(batch[\"text\"], padding=True, truncation=True)"]},{"cell_type":"markdown","metadata":{"id":"nlhVecn-NEFz"},"source":["Apply to the first three texts."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BfVQMr2ZCzHl"},"outputs":[],"source":["tokenize(emotions[\"train\"][:3])"]},{"cell_type":"markdown","metadata":{"id":"gSALNDtpNG-g"},"source":["Repeat with the whole corpus."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"bf7zFgfQCzKk"},"outputs":[],"source":["emotions_encoded = emotions.map(tokenize, batched=True, batch_size=None)"]},{"cell_type":"markdown","metadata":{"id":"yRV7TMy-NMCy"},"source":["The above function call has added two new features to the dataset: input_ids and the attention mask."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5EKgbeMCCzNw"},"outputs":[],"source":["emotions_encoded[\"train\"].features"]},{"cell_type":"markdown","metadata":{"id":"UY3E9ADUNfh6"},"source":["With `hidden_states = model(input_ids, attention_mask)` we could now generate the hidden states for each token of each document. For convenience, we define a custom function that takes a batch of tokenized documents, feeds them through the model, and adds hidden_state features to the batch. Instead of using the hidden states of all tokens, in the original BERT paper it was recommended to only use the hidden states of the CLS token as features for classification. An alternative would be to average over the hidden states of all tokens."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BelGgNdUiLr4"},"outputs":[],"source":["def extract_features_cls(batch):\n","  # store inputs in separate variables\n","  input_ids = torch.tensor(batch[\"input_ids\"]).to(device)\n","  attention_mask = torch.tensor(batch[\"attention_mask\"]).to(device)\n","\n","  # feed inputs into model and save outputs\n","  with torch.no_grad():\n","      last_hidden_state = model(input_ids, attention_mask).last_hidden_state\n","      last_hidden_state = last_hidden_state.cpu().numpy()\n","\n","  # extract the hidden states for the CLS token (i.e., the first token)\n","  cls_lhs = last_hidden_state[:,0,:]\n","\n","  # return results\n","  batch[\"cls_hidden_state\"] = cls_lhs\n","  return batch"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zDeUKiXLFSb-"},"outputs":[],"source":["emotions_encoded = emotions_encoded.map(extract_features_cls,\n","                                        batched=True,\n","                                        batch_size=16)"]},{"cell_type":"markdown","metadata":{"id":"Pg6CbLybN4jX"},"source":["Show the dataset again."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HTo6ZpqACzTX"},"outputs":[],"source":["emotions_encoded[\"train\"].features"]},{"cell_type":"markdown","metadata":{"id":"GnFfhrX3OiNW"},"source":["Check the dimensions of the hidden states."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zX4m0yQdFYvC"},"outputs":[],"source":["len(emotions_encoded[\"train\"][\"cls_hidden_state\"][0])"]},{"cell_type":"markdown","metadata":{"id":"VF4W231oN9bL"},"source":["The dataset now contains all the information we need to train a classifier on it. We will use the extracted hidden states of the CLS tokens as input features and the labels as targets. We can easily create the corresponding arrays in the well known Scikit-Learn format as follows:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9TTYuwbeWRuJ"},"outputs":[],"source":["y_valid = np.array(emotions_encoded[\"validation\"][\"label\"])"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U6xd4jmy-XHr"},"outputs":[],"source":["X_train = np.array(emotions_encoded[\"train\"][\"cls_hidden_state\"])\n","X_valid = np.array(emotions_encoded[\"validation\"][\"cls_hidden_state\"])\n","y_train = np.array(emotions_encoded[\"train\"][\"label\"])\n","y_valid = np.array(emotions_encoded[\"validation\"][\"label\"])\n","X_train.shape, X_valid.shape"]},{"cell_type":"markdown","metadata":{"id":"yWdarGxMOKDD"},"source":["And now we can train a standard classifier on these data structures."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TKhnWJMkuZrX"},"outputs":[],"source":["clf = LogisticRegression(max_iter=3000)\n","clf.fit(X_train, y_train)"]},{"cell_type":"markdown","metadata":{"id":"d98HpqT9rIP-"},"source":["## Evaluate Model"]},{"cell_type":"markdown","metadata":{"id":"P4xoQYscOSbB"},"source":["Make predictions on validation set and evaluate predictive accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8tRNCCJOut7O"},"outputs":[],"source":["y_preds = clf.predict(X_valid)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aG93pkwkukIv"},"outputs":[],"source":["print(classification_report(y_valid, y_preds, target_names=labels))"]},{"cell_type":"markdown","metadata":{"id":"O5wxuEsgoxOK"},"source":["# The Fine-tuning Approach"]},{"cell_type":"markdown","metadata":{"id":"HH6OjPg-Ol-S"},"source":["Let’s now fine-tune a Transformer end-to-end. With the fine-tuning approach we do not use the hidden states as fixed features, but instead train them as shown in the figure below. Since we retrain all the parameters, this approach requires more compute than the feature extraction approach and typically requires a GPU."]},{"cell_type":"markdown","metadata":{"id":"pTMO-UzOOyNN"},"source":["<center><br><img width=600 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/fine_tuning.png\"/><br></center>\n"]},{"cell_type":"markdown","metadata":{"id":"uamJyUWArP2C"},"source":["## Model: Load Pre-trained Model and Fine-tune"]},{"cell_type":"markdown","metadata":{"id":"R8-IdNAfPCIk"},"source":["In the following, we will use the Trainer API to simplify the training loop.\n","\n","We need is a pretrained model like the one we used in the feature-based approach. The only difference is that we use the AutoModelForSequenceClassification model instead of AutoModel. This model has a classification head on top of the model outputs which can be easily trained with the base model."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nTUjcMuLZFNl"},"outputs":[],"source":["model_name = \"distilbert-base-uncased\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kXlWZARF3FZy"},"outputs":[],"source":["model = (AutoModelForSequenceClassification\n","         .from_pretrained(model_name, num_labels = 6)\n","         .to(device))"]},{"cell_type":"markdown","metadata":{"id":"VjvcMv6NP8mk"},"source":["Furthermore, we define a function to monitor some metrics during training."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"gE0ePh_4P85A"},"outputs":[],"source":["def compute_metrics(pred):\n","    labels = pred.label_ids\n","    preds = pred.predictions.argmax(-1)\n","    f1 = f1_score(labels, preds, average=\"weighted\")\n","    acc = accuracy_score(labels, preds)\n","    return {\"accuracy\": acc, \"f1\": f1}"]},{"cell_type":"markdown","metadata":{"id":"nOM8x2gYP4lU"},"source":["With the dataset and metrics ready we can now instantiate a Trainer class. The main ingredient here is the TrainingArguments class to specify all the parameters of the training run, one of which is the output directory for the model checkpoints."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PWTgI6Dv3ZvD"},"outputs":[],"source":["batch_size = 64\n","logging_steps = len(emotions_encoded[\"train\"]) // batch_size\n","\n","training_args = TrainingArguments(output_dir=\"results\",\n","                                  num_train_epochs=2,\n","                                  learning_rate=3e-5,\n","                                  per_device_train_batch_size=batch_size,\n","                                  per_device_eval_batch_size=batch_size,\n","                                  load_best_model_at_end=True,\n","                                  metric_for_best_model=\"f1\",\n","                                  weight_decay=0.01,\n","                                  evaluation_strategy=\"epoch\",\n","                                  save_strategy=\"epoch\",\n","                                  disable_tqdm=False,\n","                                  logging_steps=logging_steps)"]},{"cell_type":"markdown","metadata":{"id":"ESgn9VNIQaeS"},"source":["We can now instantiate and fine-tune our model with the Trainer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XdiUuJsP3oz7"},"outputs":[],"source":["trainer = Trainer(model=model,\n","                  args=training_args,\n","                  compute_metrics=compute_metrics,\n","                  train_dataset=emotions_encoded[\"train\"],\n","                  eval_dataset=emotions_encoded[\"validation\"])\n","trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"lQYRp0-9rQT7"},"source":["## Evaluate Model"]},{"cell_type":"markdown","metadata":{"id":"D82JH6g9Qicy"},"source":["Make predictions on validation set and evaluate predictive accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KEsMi6xu6tBa"},"outputs":[],"source":["preds_output = trainer.predict(emotions_encoded[\"validation\"])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["preds_output"]},{"cell_type":"markdown","metadata":{"id":"3_Rvr5d1Qn0D"},"source":["Above, you can see that the model outputs are raw logits, not probabilities. Nonetheless, we can simply take the highest value as the predicted label."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IRHVLH_j67Hj"},"outputs":[],"source":["y_preds = np.argmax(preds_output.predictions, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K92-yKIq62Wa"},"outputs":[],"source":["print(classification_report(y_valid, y_preds, target_names=labels))"]},{"cell_type":"markdown","metadata":{"id":"_vBFltOiR_zj"},"source":["The classification report reveals that the fine-tune approach is performing much better than the feature extraction approach."]}],"metadata":{"accelerator":"GPU","colab":{"gpuClass":"premium","private_outputs":true,"provenance":[{"file_id":"1kHkaqxz9sVdaOC2i4FJVOOFW9cCiBkYu","timestamp":1636383829898}]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}
