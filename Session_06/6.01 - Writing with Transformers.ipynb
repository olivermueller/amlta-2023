{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"},"colab":{"private_outputs":true,"provenance":[{"file_id":"1kHkaqxz9sVdaOC2i4FJVOOFW9cCiBkYu","timestamp":1636383829898}],"machine_shape":"hm"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"LJrKCbjZsqpo"},"source":["# <font color=\"#003660\">Applied Machine Learning for Text Analysis (M.184.5331)</font>\n"]},{"cell_type":"markdown","metadata":{"id":"V-DU0hkyVyPi"},"source":["# <font color=\"#003660\">Week 6: Generating Texts with Transformers</font>"]},{"cell_type":"markdown","metadata":{"id":"mhy42GjRV3ON"},"source":["# <font color=\"#003660\">Notebook 1: Writing with Transformers</font>\n","\n","<center><br><img width=256 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/dag.png\"/><br></center>\n","\n","<p>\n","<center>\n","<div>\n","    <font color=\"#085986\"><b>By the end of this lesson, you ...</b><br><br>\n","        ... you know how to generate texts with a decoder model, <br>\n","        ... understand how we can select tokens using search (greedy or beam) or sampling.\n","    </font>\n","</div>\n","</center>\n","</p>"]},{"cell_type":"markdown","source":["The following content is heavily inspired by the following excellent sources:\n","\n","\n","*   Tunstall et al. (2021): Natural Language Processing with Transformers. O'Reilly. https://www.oreilly.com/library/view/natural-language-processing/9781098103231/\n","*   Hugging Face (2021): Transformer Models - Hugging Face Course. https://huggingface.co/course/\n","\n"],"metadata":{"id":"B8WyaSVOepeR"}},{"cell_type":"markdown","source":["# How does text generation work?"],"metadata":{"id":"euTwZ6Bne4E3"}},{"cell_type":"markdown","source":["One of the most fascinating things about Transformers is their ability to generate natural language text. A famous example is OpenAI‚Äôs GPT-2, which when given the prompt (Tunstall et al. (2021))\n","\n","> *In a shocking finding, scientist discovered a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains. Even more surprising to the researchers was the fact that the unicorns spoke perfect English.*\n","\n","was able to generate a compelling news article about talking unicorns:\n","\n","> *The scientist named the population, after their distinctive horn, Ovid‚Äôs Unicorn. These four-horned, silver-white unicorns were previously unknown to science. Now, after almost two centuries, the mystery of what sparked this odd phenomenon is finally solved. Dr. Jorge P√©rez, an evolutionary biologist from the University of La Paz, and several companions, were exploring the Andes Mountains when they found a small valley, with no other animals or humans. P√©rez noticed that the valley had what appeared to be a natural fountain, surrounded by two peaks of rock and silver snow. P√©rez and the others then ventured further into the valley. ‚ÄúBy the time we reached the top of one peak, the water looked blue, with some crystals on top,‚Äù said P√©rez. P√©rez and his friends were astonished to see the unicorn herd. These creatures could be seen from the air without having to move too much to see them - they were so close they could touch their horns. While examining these bizarre creatures the scientists discovered that the creatures also spoke some fairly regular English*\n","\n"],"metadata":{"id":"wOWkF8awe6pT"}},{"cell_type":"markdown","source":["It is remarkable that this text was generated by a model trained without explicit supervision. Through a self-supervised way of learning to predict next words in millions of documents from the Internet, languages models like GPT-2 are able to generate new texts, which are almost indistinguishable from human-written texts."],"metadata":{"id":"_Nvu_lhke6Bf"}},{"cell_type":"markdown","source":["For NLP tasks like sequence classification, generating predictions is fairly straightforward; the model encodes an input text in a vector and uses this vectors as features for a standard classification task, i.e., it outputs a single or multiple logits, probabilities, or class labels.\n","\n","The output of a text generation model, in contrast, is not a vector of probabilities or class labels, but again a text sequence. Producing a coherent text sequence requires an extra decoding step (probabilities to text) and poses a number of challenges. For example, instead of simply passing inputs once through the forward pass of a model, the prediction and decoding must be done iteratively and in an autoregressive way. \n","\n","Language models are typically pretrained to estimate the probability of a sequence of tokens `y1, y2, ... yt` occurring in the text, given some initial text prompt `x`:\n","\n","<center><img width=350 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/lm_conditional_prob.png\"/><br></center>"],"metadata":{"id":"GS4vIe7Zgth2"}},{"cell_type":"markdown","source":["We can easily extend the next-token prediction task to generate text sequences of arbitrary length by iteratively repeating the process, taking the output of step `t` as the input of step `t+1`. The Figure below illustrates this process. We start with a prompt like ‚ÄúTransformers are the‚Äù and use the model to predict the next token. Once we have determined the next token, we append it to the prompt and then use the new input sequence to generate another token. We do this until we have reached a special end of sequence (EOS) token - for example a full stop (.) - or a predefined maximum length.\n","\n","<center><img width=600 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/transformers_are_the.png\"/><br></center>"],"metadata":{"id":"KggoYDWQkKg5"}},{"cell_type":"markdown","source":["For the above process we need a decoding method that determines which token is selected at each timestep. Since the language model head produces a logit $z_{ti}$ per token in the vocabulary at each step, we can get the probability distribution over the next possible token  by taking the softmax of this distribution:\n","\n","<center><img width=300 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/softmax_zti.png\"/><br></center>\n","\n","Yet, the goal of most decoding methods is to find the most likely *overall* sequence. Since there does not exist an algorithm that can find the optimal decoded sequence in polynomial time, we have to rely on heuristics such as **greedy search** or **beam search** to find likely sequences.\n"],"metadata":{"id":"D6_thAD4lFWH"}},{"cell_type":"markdown","source":["Let's try this out with Hugging Face ü§ó. We will use a **GPT-2** decoder model for this.\n","\n","<center><img width=600 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/decoder.png\"/><br></center>"],"metadata":{"id":"MQ6Yrs0amgpx"}},{"cell_type":"markdown","metadata":{"id":"C6vVpwIFsqps"},"source":["# Import Packages\n","\n","As always, we first need to load a number of required Python packages:\n","- `pandas` provides high-performance, easy-to-use data structures and data analysis tools.\n","- `numpy` is a library adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\n","- `transformers` provides general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet‚Ä¶) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages.\n","- `torch` is an open source machine learning library used for applications such as computer vision and natural language processing, primarily developed by Facebook's AI Research lab. "]},{"cell_type":"code","metadata":{"id":"OFZTIYq-Nlbp"},"source":["!pip install transformers[sentencepiece]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mMrhkr83sqpt"},"source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","from transformers import AutoTokenizer, AutoModelForCausalLM"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Load Pre-trained Model"],"metadata":{"id":"2jQsLKn7eUOz"}},{"cell_type":"markdown","source":["First, we load a model for causal language modeling and a corresponding tokenizer from the model hub."],"metadata":{"id":"HvbQyVC3mqHu"}},{"cell_type":"code","source":["device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n","device"],"metadata":{"id":"3-jKbucA0aqX"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["model_name = \"gpt2-xl\""],"metadata":{"id":"Xc4dKH1W0fR5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer = AutoTokenizer.from_pretrained(model_name)\n","model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"],"metadata":{"id":"w6MugtY50fZG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# Greedy Search Decoding"],"metadata":{"id":"T1d98Sgb0ZG5"}},{"cell_type":"markdown","source":["The simplest decoding method to yield sequences of tokens from a model‚Äôs raw predictions is to greedily select the token with the highest probability at each timestep:\n","\n","<center><img width=300 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/greedy_search.png\"/><br></center>"],"metadata":{"id":"DZ6XhOo6niG4"}},{"cell_type":"markdown","source":["The following diagram illustrates greedy search decoding with a simple example (Source: https://huggingface.co/blog/how-to-generate). Note that in reality we consider all known words at each step (and not only 3). Here, greedy search would decode the sequence \"The nice woman\".\n","\n","<center><img width=500 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/greedy_ex.png\"/><br></center>"],"metadata":{"id":"CDKlGOr9KHCW"}},{"cell_type":"markdown","source":["To generate new text, a language model needs a user-specified input prompt."],"metadata":{"id":"REvJx8jn4zNn"}},{"cell_type":"code","source":["input_txt = \"Transformers are the\""],"metadata":{"id":"0fdvfbui0ppn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)"],"metadata":{"id":"vUam3QvS0r_o"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["input_ids"],"metadata":{"id":"sAVA_jcH3CzY"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Although the Hugging Face library provides a `generate()` function for autoregressive models like GPT-2, let‚Äôs implement the decoding step using greedy search by hand to understand what happens in the backstage."],"metadata":{"id":"WsewODQfn7Uy"}},{"cell_type":"markdown","source":["First, let's process the tokenized input with a simple forward pass through the model."],"metadata":{"id":"XfbUOewv47Qf"}},{"cell_type":"code","source":["output = model(input_ids=input_ids)"],"metadata":{"id":"R0L76Tjy36WP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The `output` contains an element named `logits`, which is a vector of raw (non-normalized) predictions generated by the model. In the code below, we extract the logits after the last token (index -1) of the first input batch (index 0) and normalize it to probabilities using the `softmax()` function."],"metadata":{"id":"N_E9LB8vo8jQ"}},{"cell_type":"code","source":["next_token_probs = torch.softmax(output.logits[0, -1, :], dim=-1)\n","next_token_probs"],"metadata":{"id":"brJuyA3o36ci"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This vector of probabilities has as many entries as we have words in the model's vocabulary."],"metadata":{"id":"i0VOfAbKqEVo"}},{"cell_type":"code","source":["len(next_token_probs)"],"metadata":{"id":"1fmH6hgV36fW"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's sort the probabilities and display the five most likely next words after the prompt..."],"metadata":{"id":"bC1i6NR1qRNO"}},{"cell_type":"code","source":["next_token_probs_sorted = torch.argsort(next_token_probs, dim=-1, descending=True)\n","next_token_probs_sorted[0:5]"],"metadata":{"id":"v69vChFM3ozK"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer.decode(next_token_probs_sorted[0:5])"],"metadata":{"id":"-dYdeyWD4oo8"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The five words above are the most likely options for the first token of the generated text. Now let's repeat the process in an autoregressive way to generate a whole sequence of tokens."],"metadata":{"id":"XSEoc9eF4v4M"}},{"cell_type":"code","source":["iterations = []\n","n_steps = 8\n","choices_per_step = 5\n","\n","with torch.no_grad():\n","    for _ in range(n_steps):\n","      \n","        # Feed input prompt (plus already generated tokens) into model \n","        iteration = dict()\n","        iteration[\"Input\"] = tokenizer.decode(input_ids[0])\n","        output = model(input_ids=input_ids)\n","\n","        # Select logits of the last token of the first batch and apply softmax\n","        next_token_logits = output.logits[0, -1, :]\n","        next_token_probs = torch.softmax(next_token_logits, dim=-1)\n","        sorted_ids = torch.argsort(next_token_probs, dim=-1, descending=True)\n","        \n","        # Store tokens with highest probabilities\n","        for choice_idx in range(choices_per_step):\n","            token_id = sorted_ids[choice_idx]\n","            token_prob = next_token_probs[token_id].cpu().numpy()\n","            token_choice = (\n","                f\"{tokenizer.decode(token_id)} ({100 * token_prob:.2f}%)\"\n","            )\n","            iteration[f\"Choice {choice_idx+1}\"] = token_choice\n","        \n","        # Append most likely next token to input, and repeat\n","        input_ids = torch.cat([input_ids, sorted_ids[None, 0, None]], dim=-1)\n","        iterations.append(iteration)\n","\n","pd.DataFrame.from_records(iterations)"],"metadata":{"id":"Udgpt6C-0oje"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["The above loop is useful for understanding what happens under the hood. In real uses cases, however, we would rather use Hugging Face's `generate()` function."],"metadata":{"id":"tUcJtCF55h-v"}},{"cell_type":"code","source":["input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n","output = model.generate(input_ids, max_length=11, do_sample=False)\n","print(tokenizer.decode(output[0]))"],"metadata":{"id":"UQ8luY8n5lKk"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let's do the same with the ü¶Ñ prompt from above..."],"metadata":{"id":"JJd5fQjasITK"}},{"cell_type":"code","source":["input_txt = \"\"\"In a shocking finding, scientist discovered \\\n","a herd of unicorns living in a remote, previously unexplored \\\n","valley, in the Andes Mountains. Even more surprising to the \\\n","researchers was the fact that the unicorns spoke perfect English.\\n\\n\n","\"\"\"\n","\n","input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)\n","\n","output = model.generate(input_ids, max_length=256, do_sample=False)\n","\n","print(tokenizer.decode(output[0]))"],"metadata":{"id":"huXBU9Qz5nfO"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["One common issue with greedy search decoding is that the output tends to comprise repetitive sequences. For instance, in the above example the fragment \"The researchers were surprised to find that the unicorns were able\" appears multiple times."],"metadata":{"id":"1EyCmcfOsVZZ"}},{"cell_type":"markdown","source":["# Beam Search Decoding"],"metadata":{"id":"88pJ_LVn658k"}},{"cell_type":"markdown","source":["Instead of selecting the individual token with the highest probability at each step, beam search keeps track of the top-`b` most probable next tokens at each step. The next set of beams are chosen by considering all possible next token extensions of the existing set and selecting the `b` most likely extensions. The process is repeated until we reach the maximum length or an EOS token, and the most likely overall sequence is selected by ranking the beams according to their log-probabilities.\n","\n","Using the example from above, the following diagram illustrates beam search decoding with b=2 and a maximum sequence length of 3 (Source: https://huggingface.co/blog/how-to-generate). Again, note that in reality we would consider all known words at each step (and not only 3). In contrast to greedy search, beam search would decode the sequence \"The dog has\".\n","\n","<center><img width=500 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/beam_ex.png\"/><br></center>"],"metadata":{"id":"YrcywM3jtTzF"}},{"cell_type":"markdown","source":["Let‚Äôs calculate and compare the log-probabilities (for mathematical reasons, we work with log-probabilities instead of probbailities here) of the text generated by greedy and beam search to see if beam search can improve the overall sequence probability. "],"metadata":{"id":"e0DrrcKauFFl"}},{"cell_type":"markdown","source":["We first need two helper functions to calculate the overall sequence probability. The function below calculates the log-probability for a single token."],"metadata":{"id":"QWcG0jGFulEs"}},{"cell_type":"code","source":["# Calculate the log-probability of a single token (from Chapter 8 of Tunstall et al. (2021))\n","def token_logprob(logits, predictions):\n","    logp = F.log_softmax(logits, dim=-1)\n","    logp_prediction = torch.gather(logp, 2, predictions.unsqueeze(2)).squeeze(-1)\n","    return logp_prediction"],"metadata":{"id":"sYc5FoBv669O"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["And the following function calculates the total log-probability of a sequence by just summing up the log-probabilities for each token."],"metadata":{"id":"vbGhEV1Uul4H"}},{"cell_type":"code","source":["# Calculate the log-probability of a whole sequence (from Chapter 8 of Tunstall et al. (2021))\n","def sequence_logprob(model, predictions, input_len=0):\n","    with torch.no_grad():\n","        output = model(predictions)\n","        log_probs = token_logprob(output.logits[:, :-1, :], predictions[:, 1:])\n","        seq_log_prob = torch.sum(log_probs[:, input_len:])\n","    return seq_log_prob.cpu().numpy()"],"metadata":{"id":"v8mrXrwb7QTm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Let‚Äôs use these functions to first calculate the log-probability of the Greedy search decoder for the ü¶Ñ prompt."],"metadata":{"id":"8U0j9aJ3u0d7"}},{"cell_type":"code","source":["input_txt = \"\"\"In a shocking finding, scientist discovered \\\n","a herd of unicorns living in a remote, previously unexplored \\\n","valley, in the Andes Mountains. Even more surprising to the \\\n","researchers was the fact that the unicorns spoke perfect English.\\n\\n\n","\"\"\"\n","input_ids = tokenizer(input_txt, return_tensors=\"pt\")[\"input_ids\"].to(device)"],"metadata":{"id":"rcldByxy9uOJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["output_greedy = model.generate(input_ids, max_length=128, do_sample=False)\n","print(tokenizer.decode(output_greedy[0]))"],"metadata":{"id":"AUK_NfjM9wmA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["logp = sequence_logprob(model, output_greedy, input_len=len(input_ids[0]))\n","logp"],"metadata":{"id":"9rGc0Y697WnK"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Now let's repeat the same with Beam search..."],"metadata":{"id":"x0v31Lk0vMu4"}},{"cell_type":"code","source":["output_beam = model.generate(input_ids, max_length=256, num_beams=5,\n","                             do_sample=False)\n","print(tokenizer.decode(output_beam[0]))"],"metadata":{"id":"Cibj3Bov99OA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\n","logp"],"metadata":{"id":"e4s03wbX7pw_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["We can see that we get a better overall log-probability (higher is better) with Beam search than we did with Greedy search. \n","\n","However we can see that Beam search also suffers from repetitive text (\"a herd of unicorns living in a remote, previously unexplored valley, in the Andes Mountains\"). One way to address this is to impose an n-gram penalty with the `no_repeat_ngram_size` parameter that tracks which n-grams have been seen and sets the next-token probability to zero if it would produce a previously seen n-gram."],"metadata":{"id":"miq_VXicvdWU"}},{"cell_type":"code","source":["output_beam = model.generate(input_ids, max_length=256, num_beams=5,\n","                             do_sample=False, no_repeat_ngram_size=2)\n","print(tokenizer.decode(output_beam[0]))"],"metadata":{"id":"pe4fTJ7m7sPO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["logp = sequence_logprob(model, output_beam, input_len=len(input_ids[0]))\n","logp"],"metadata":{"id":"P61G7SYk-TM3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Although the log-probability went down, the text reads much better!"],"metadata":{"id":"-RL0II7vv7Kv"}},{"cell_type":"markdown","source":["# Decoding with Sampling"],"metadata":{"id":"C9cIjNKS8EyE"}},{"cell_type":"markdown","source":["One solution to reduce repetitions while improving diversity is to use sampling instead of deterministic greedy/beam search.\n","\n","<center><br><br><img width=600 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/urn_lm.png\"/><br></center>"],"metadata":{"id":"63t5GQE2wUzh"}},{"cell_type":"markdown","source":["## Random Sampling"],"metadata":{"id":"MUonmcLNwtHa"}},{"cell_type":"markdown","source":["The simplest sampling method is to **randomly sample** from the model output‚Äôs probability distribution over the full vocabulary at each timestep (`|V|` is the size of the vocabulary):\n","\n","<center><img width=300 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/random_sampling.png\"/><br></center>"],"metadata":{"id":"RpNkeOeVwu9_"}},{"cell_type":"markdown","source":["We can extend this to control the diversity of the output by adding a **temperature** parameter `T` that rescales the logits before taking the softmax:\n","\n","<center><img width=300 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/random_sampling_w_temp.png\"/><br></center>\n","\n","With temperature we can control the shape of the probability distribution. Low temperature means that the tokens with high probability get boosted, while the probabilities of less likely tokens get damped. When we increase the temperature the distribution smooths out and the probabilities get closer to each other. This effect is illustrated in the figure below.\n","\n","<center><img width=600 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/temp.png\"/><br></center>"],"metadata":{"id":"qupPfBLrw9CR"}},{"cell_type":"markdown","source":["Let's try out random sampling with a relatively high temperature of T=2."],"metadata":{"id":"oE7t64gHzQHw"}},{"cell_type":"code","source":["torch.manual_seed(42)\n","output_temp = model.generate(input_ids, max_length=128, do_sample=True,\n","                             temperature=2.0, top_k=0)\n","print(tokenizer.decode(output_temp[0]))"],"metadata":{"id":"5bLk9lfJ8HHo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["With a high temperature the model seems to produce mostly gibberish. \n","\n","Let‚Äôs see what happens, when we cool down the temperature..."],"metadata":{"id":"9HJTc4tmzHR5"}},{"cell_type":"code","source":["torch.manual_seed(42)\n","output_temp = model.generate(input_ids, max_length=128, do_sample=True,\n","                             temperature=0.5, top_k=0)\n","print(tokenizer.decode(output_temp[0]))"],"metadata":{"id":"h8I786O1zYYP"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["This is significantly more coherent and even includes a quote! \n","\n","The main lesson we can draw from temperature is that it allows us to control the quality of the samples, but there‚Äôs always a trade-off between coherence (low temperature) and diversity (high temperature)."],"metadata":{"id":"img_ThfNz1Lg"}},{"cell_type":"markdown","source":["By the way: Try to turn off the manual random seed in the code above. As you will see, text generation using sampling is not deterministic."],"metadata":{"id":"e31LpL-26npM"}},{"cell_type":"markdown","source":["## Top-k and Nucleus (Top-p) Sampling"],"metadata":{"id":"OiEA3kTMz_n9"}},{"cell_type":"markdown","source":["Top-k and nucleus (top-p) sampling are two popular alternatives or extensions to using random sampling. In both cases the basic idea is to restrict the number of possible tokens we can sample from at each timestep."],"metadata":{"id":"6rwWHeV80GO4"}},{"cell_type":"markdown","source":["To see how this works, let‚Äôs first visualize a cumulative probability distribution of the model‚Äôs outputs."],"metadata":{"id":"nurNwuXp1w6q"}},{"cell_type":"markdown","source":["<center><img width=800 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/topk_nucleus_sampling.png\"/><br></center>"],"metadata":{"id":"XTtmIEA40o-W"}},{"cell_type":"markdown","source":["Looking at the left diagram, we can see that the chance of picking the token with the highest probability (the isolated bar at 10^-1) is 10%. \n","\n","The right diagram shows the cumulative sum of the probabilities of the 10,000 most likely tokens (sorted by descending probabilty). For example, there is a 99% chance of picking any of the 2,000 tokens with the highest probability. \n","\n","Or in other words: There is only a 1% chance of not picking any of the tokens that are not in the top-2,000. Although this number might appear small, they become important because we sample often when generating long texts. So even if there is only a 1% or 0.1% chance, if we sample hundreds of times there is a significant chance of picking an unlikely token at some point. And picking such tokens can badly influence the quality of the generated text. "],"metadata":{"id":"cjxVZiHM1tac"}},{"cell_type":"markdown","source":["The idea of **top-k sampling** is to avoid the low probability choices by only sampling from the `k` tokens with the highest probability at each step. This puts a fixed cutoff on the long tail of the distribution. "],"metadata":{"id":"xnyd6QKH3Pd7"}},{"cell_type":"code","source":["torch.manual_seed(42)\n","output_topk = model.generate(input_ids, max_length=256, do_sample=True,\n","                             top_k=50)\n","print(tokenizer.decode(output_topk[0]))"],"metadata":{"id":"C5ySqnXj14Jq"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Instead of defining a fixed cutoff, we can use a dynamic one with **nucleus or top-p sampling**. Here, we set a *condition* when to cutoff. This condition is when a certain probability mass in the selection is reached. For example, when we set the cutoff condition to 95%, we order all tokens by probability and add one token after another from the top list until the sum of probabilities of the selected tokens is 95%."],"metadata":{"id":"kUW0aWho3uIw"}},{"cell_type":"code","source":["torch.manual_seed(42)\n","output_topp = model.generate(input_ids, max_length=128, do_sample=True,\n","                             top_p=0.90)\n","print(tokenizer.decode(output_topp[0]))"],"metadata":{"id":"Ou66drgl324Y"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["You can also combine the two approaches. Setting `top_k=50` and `top_p=0.9` corresponds to the rule of choosing tokens with a probability mass that is 90% but at most 50 tokens."],"metadata":{"id":"P5pdX1eI4P--"}}]}