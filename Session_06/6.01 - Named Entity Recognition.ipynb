{"cells":[{"cell_type":"markdown","metadata":{"id":"LJrKCbjZsqpo"},"source":["# <font color=\"#003660\">Applied Machine Learning for Text Analysis (M.184.5331)</font>\n"]},{"cell_type":"markdown","metadata":{"id":"V-DU0hkyVyPi"},"source":["# <font color=\"#003660\">Session 6: NER and Data Annotation</font>"]},{"cell_type":"markdown","metadata":{"id":"mhy42GjRV3ON"},"source":["# <font color=\"#003660\">Notebook 1: Named Entity Recognition</font>\n","\n","<center><br><img width=256 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/dag.png\"/><br></center>\n","\n","<p>\n","<center>\n","<div>\n","    <font color=\"#085986\"><b>By the end of this lesson, you ...</b><br><br>\n","        ... you understand the differences between sequence and token classification, <br>\n","        ... know how to fine-tune a NER model on labelled data.\n","    </font>\n","</div>\n","</center>\n","</p>"]},{"cell_type":"markdown","metadata":{"id":"B8WyaSVOepeR"},"source":["The following content is heavily inspired by the following excellent sources:\n","\n","\n","*   Tunstall et al. (2021): Natural Language Processing with Transformers. O'Reilly. https://www.oreilly.com/library/view/natural-language-processing/9781098103231/\n","*   Hugging Face (2021): Transformer Models - Hugging Face Course. https://huggingface.co/course/\n","\n"]},{"cell_type":"markdown","metadata":{"id":"euTwZ6Bne4E3"},"source":["# Token vs. Sequence Classification"]},{"cell_type":"markdown","metadata":{"id":"nPnOBt7jDs5i"},"source":["Token classification assigns a label to individual tokens in a sentence. One of the most common token classification tasks is Named Entity Recognition (NER). NER attempts to find a label for each entity in a sentence, such as a person, location, or organization.\n","\n","Watch the Hugging Face YouTube video below to learn more about token classification (esp., NER)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LmjG9wVOXX3O"},"outputs":[],"source":["from IPython.display import YouTubeVideo\n","YouTubeVideo('wVHdVlPScxA')"]},{"cell_type":"markdown","metadata":{"id":"Z8NxJHRgVp8H"},"source":["The figures below show the main differences of using encoders like BERT and its sibblings and cousins for sequence and token classification. When doing **sequence classification** (e.g., sentiment analysis), we feed a sequence into the model and only work with the contextual embedding of the [CLS] token when training the classification head of the model. In contrast, when doing **token classification** we feed the contextual embeddings of all tokens through the classification head of the model. "]},{"cell_type":"markdown","metadata":{"id":"uvBIe3QmXyS-"},"source":["<center><img width=500 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/seq_class.png\"/><br></center>\n","\n","<hr>\n","\n","<center><img width=500 src=\"https://raw.githubusercontent.com/olivermueller/aml4ta-2021/main/resources/token_class.png\"/><br></center>"]},{"cell_type":"markdown","metadata":{"id":"C6vVpwIFsqps"},"source":["# Import Packages\n","\n","As always, we first need to load a number of required Python packages:\n","- `pandas` provides high-performance, easy-to-use data structures and data analysis tools.\n","- `numpy` is a library adding support for large, multi-dimensional arrays and matrices, along with a large collection of high-level mathematical functions to operate on these arrays.\n","- `torch` is an open source machine learning library used for applications such as computer vision and natural language processing, primarily developed by Facebook's AI Research lab. \n","- `transformers` provides general-purpose architectures (BERT, GPT-2, RoBERTa, XLM, DistilBert, XLNet…) for Natural Language Understanding (NLU) and Natural Language Generation (NLG) with over 32+ pretrained models in 100+ languages.\n","- `datasets` and `evaluate` are libraries from Hugging Face to feed Transformers with data and evaluate their predictive accuracy."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OFZTIYq-Nlbp"},"outputs":[],"source":["!pip install transformers datasets evaluate seqeval\n","!pip install accelerate -U"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mMrhkr83sqpt"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import torch\n","import torch.nn.functional as F\n","from huggingface_hub import notebook_login\n","from transformers import AutoTokenizer\n","from transformers import DataCollatorForTokenClassification\n","from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n","from transformers import pipeline\n","from datasets import load_dataset, load_metric\n","import evaluate"]},{"cell_type":"markdown","metadata":{"id":"Qy8yT9LPEmQY"},"source":["# Load data"]},{"cell_type":"markdown","metadata":{"id":"pB91kYyKZ1eB"},"source":["In this notebook we will use the WNUT 17 dataset, whcih is a NER dataset focusing on emerging and rare entities. You can find more information about the dataset here: https://huggingface.co/datasets/wnut_17"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"baRzuVjMEw3z"},"outputs":[],"source":["wnut = load_dataset(\"wnut_17\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-VOpvQmjE1xW"},"outputs":[],"source":["wnut"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z9LjjdRwE2X3"},"outputs":[],"source":["wnut[\"train\"][0]"]},{"cell_type":"markdown","metadata":{"id":"ilA4t0ydFMJQ"},"source":["Each number in `ner_tags` represents a type of named entity (e.g., location, person). We can convert the numbers to textual labels to learn more about those entities."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K82K22FVFByE"},"outputs":[],"source":["label_list = wnut[\"train\"].features[\"ner_tags\"].feature.names\n","label_list"]},{"cell_type":"markdown","metadata":{"id":"1kVuOiTWFJlj"},"source":["The prefixes of the tags indicate whether a given token signifies the beginning or middle/end of a named entity:\n","\n","* **B-**: indicates the beginning of an entity.\n","* **I-**: indicates a token is contained inside the same entity (for example, the State token is a part of an entity like Empire State Building).\n","* **O**: indicates that the token doesn’t correspond to any entity."]},{"cell_type":"markdown","metadata":{"id":"Yo8znf3rEuoJ"},"source":["# Preprocess data"]},{"cell_type":"markdown","metadata":{"id":"KT87Q8TiGCrX"},"source":["As always, the first thing to do when processing raw texts with Transformers is to tokenize the sequences. First, we need to load a tokenizer that is compatible with the architecture we want to use (here: DistilBERT)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"GRKsApeBEwAG"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")"]},{"cell_type":"markdown","metadata":{"id":"oi22MiALauGE"},"source":["Let's re-join one example from the training set and feed it through the tokenizer. This mimics how we would use the tokenizer on new data. "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eSPZ9yBbGOSa"},"outputs":[],"source":["tokenized_input = tokenizer(\" \".join(wnut[\"train\"][0][\"tokens\"]))\n","tokens = tokenizer.convert_ids_to_tokens(tokenized_input[\"input_ids\"])\n","tokens"]},{"cell_type":"markdown","metadata":{"id":"CZ8fil5oGkeu"},"source":["As expected, the tokenizer performed sub-word tokenization, splitting, for example, `ESB` into `es` and `##b`. In addition, the tokenizer added the usual `[CLS]` and `[SEP]` tokens."]},{"cell_type":"markdown","metadata":{"id":"DihpmRxPbOA5"},"source":["As a result, the tokenized sequence and the labels of our training data (which has NOT been tokenized with sub-word tokenization) are not aligend anymore. A quick look at the length of the two sequences confirms this."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"FzBLy_UhGboT"},"outputs":[],"source":["len(tokens)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"944Ck3W_GdiY"},"outputs":[],"source":["len(wnut[\"train\"][0][\"tokens\"])"]},{"cell_type":"markdown","metadata":{"id":"j7d_UuZFHJBA"},"source":["Hence, we need to realign the tokens and labels using the following process:\n","\n","1. Map all tokens to their corresponding word with the `word_ids` method.\n","2. Assig the label `-100` to the special tokens `[CLS]` and `[SEP]`, so that they can be ignored by the loss function.\n","3. Only label the first token of a given word. Assign `-100` to other subtokens from the same word."]},{"cell_type":"markdown","metadata":{"id":"UgwsGOD6HRCA"},"source":["Below is a function to tokenize the text and, afterwards, realign the tokens and labels."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ncJI_vFSH1mf"},"outputs":[],"source":["def tokenize_and_align_labels(examples):\n","    tokenized_inputs = tokenizer(examples[\"tokens\"], truncation=True, is_split_into_words=True)\n","\n","    labels = []\n","    for i, label in enumerate(examples[f\"ner_tags\"]):\n","        word_ids = tokenized_inputs.word_ids(batch_index=i)  # Map tokens to their respective word.\n","        previous_word_idx = None\n","        label_ids = []\n","        for word_idx in word_ids:  # Set the special tokens to -100.\n","            if word_idx is None:\n","                label_ids.append(-100)\n","            elif word_idx != previous_word_idx:  # Only label the first token of a given word.\n","                label_ids.append(label[word_idx])\n","            else:\n","                label_ids.append(-100)\n","            previous_word_idx = word_idx\n","        labels.append(label_ids)\n","\n","    tokenized_inputs[\"labels\"] = labels\n","    return tokenized_inputs"]},{"cell_type":"markdown","metadata":{"id":"Qhwa2rcZbfi5"},"source":["Apply the function to the whole dataset (i.e., train, validation, and test)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PLkhx6YwH595"},"outputs":[],"source":["tokenized_wnut = wnut.map(tokenize_and_align_labels, batched=True)"]},{"cell_type":"markdown","metadata":{"id":"xs9Lj5wSbmZb"},"source":["Check whether the sequences are aligned now."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9fNik2uPH94C"},"outputs":[],"source":["tokenized_wnut[\"train\"][0]"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["len(tokens) == len(tokenized_wnut[\"train\"][0][\"input_ids\"])"]},{"cell_type":"markdown","metadata":{"id":"5AP8vru5bwPu"},"source":["To preprocess our texts on-the-fly while training our model, we can use a data collator (more information: https://huggingface.co/docs/transformers/main_classes/data_collator)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"NnPHiajMOBHD"},"outputs":[],"source":["data_collator = DataCollatorForTokenClassification(tokenizer=tokenizer)"]},{"cell_type":"markdown","metadata":{"id":"FvQn7RWeKTMM"},"source":["# Fine-tune model"]},{"cell_type":"markdown","metadata":{"id":"q_aHYIqjb4lv"},"source":["Now we are (almost) ready to fine-tune a pre-trained DistilBERT on our emerging and rare named entities dataset. We will also see how to publish our model (checkpoint + tokenizer) on the Hugging Face dataset hub and download it again."]},{"cell_type":"markdown","metadata":{"id":"78xkVk18cOi-"},"source":["Let's first log into the Hugging Face dataset hub. As a prerequisite, you need to create an account and a token on the Hugging Face hub."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"KTTE2Bs5Yzhj"},"outputs":[],"source":["notebook_login()"]},{"cell_type":"markdown","metadata":{"id":"J2cCrNtrcShN"},"source":["As always, when training a model we need a compute_metrics function to calculate and display selected accuracy metrics during training. Here we use precision, recall, F1, and accuracy (they are all saved together in the seqeval evaluation object)."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vB4HeCstKjVH"},"outputs":[],"source":["seqeval = evaluate.load(\"seqeval\")\n","\n","def compute_metrics(p):\n","    predictions, labels = p\n","    predictions = np.argmax(predictions, axis=2)\n","\n","    true_predictions = [\n","        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","    true_labels = [\n","        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n","        for prediction, label in zip(predictions, labels)\n","    ]\n","\n","    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n","    return {\n","        \"precision\": results[\"overall_precision\"],\n","        \"recall\": results[\"overall_recall\"],\n","        \"f1\": results[\"overall_f1\"],\n","        \"accuracy\": results[\"overall_accuracy\"],\n","    }"]},{"cell_type":"markdown","metadata":{"id":"8WkJ8AMHcrDq"},"source":["In order to be able to interpret the model's predictions, we also need two dictionaries to translate between the textual labels and their indices."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XcIof8DJK3Qu"},"outputs":[],"source":["id2label = {\n","    0: \"O\",\n","    1: \"B-corporation\",\n","    2: \"I-corporation\",\n","    3: \"B-creative-work\",\n","    4: \"I-creative-work\",\n","    5: \"B-group\",\n","    6: \"I-group\",\n","    7: \"B-location\",\n","    8: \"I-location\",\n","    9: \"B-person\",\n","    10: \"I-person\",\n","    11: \"B-product\",\n","    12: \"I-product\",\n","}\n","\n","label2id = {\n","    \"O\": 0,\n","    \"B-corporation\": 1,\n","    \"I-corporation\": 2,\n","    \"B-creative-work\": 3,\n","    \"I-creative-work\": 4,\n","    \"B-group\": 5,\n","    \"I-group\": 6,\n","    \"B-location\": 7,\n","    \"I-location\": 8,\n","    \"B-person\": 9,\n","    \"I-person\": 10,\n","    \"B-product\": 11,\n","    \"I-product\": 12,\n","}"]},{"cell_type":"markdown","metadata":{"id":"zLWqEVeVc04e"},"source":["🚀 🚀 🚀 Now we are really ready to fine-tune... 🚀 🚀 🚀"]},{"cell_type":"markdown","metadata":{"id":"agR3A0wkc5EY"},"source":["Load the pre-trained model, configure the classification head (how many labels?), and pass the dictionaries to translate between label IDs and textual labels."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IFGaZp6dK6WQ"},"outputs":[],"source":["model = AutoModelForTokenClassification.from_pretrained(\n","    \"distilbert-base-uncased\", num_labels=len(label2id), id2label=id2label, label2id=label2id\n",")"]},{"cell_type":"markdown","metadata":{"id":"J7txAFUGdVGy"},"source":["Configure the Trainer. Here, we will only train for 2 epochs. In reality, you probably want to train longer!"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Yc1ifPK8N5Y6"},"outputs":[],"source":["training_args = TrainingArguments(\n","    output_dir=\"my_awesome_wnut_model\",\n","    learning_rate=2e-5,\n","    per_device_train_batch_size=16,\n","    per_device_eval_batch_size=16,\n","    num_train_epochs=2,\n","    weight_decay=0.01,\n","    evaluation_strategy=\"epoch\",\n","    save_strategy=\"epoch\",\n","    load_best_model_at_end=True,\n","    push_to_hub=True,\n",")\n","\n","trainer = Trainer(\n","    model=model,\n","    args=training_args,\n","    train_dataset=tokenized_wnut[\"train\"],\n","    eval_dataset=tokenized_wnut[\"test\"],\n","    tokenizer=tokenizer,\n","    data_collator=data_collator,\n","    compute_metrics=compute_metrics,\n",")"]},{"cell_type":"markdown","metadata":{"id":"rJUieJIzdl-p"},"source":["Go! 🏁"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"m-7CvqpmT5tI"},"outputs":[],"source":["trainer.train()"]},{"cell_type":"markdown","metadata":{"id":"eYzau5Fgdorb"},"source":["Save and publish the results on the Hugging Face ☁️"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"uojBHzC_PEZD"},"outputs":[],"source":["trainer.push_to_hub()"]},{"cell_type":"markdown","metadata":{"id":"pTc0_H2rT_2-"},"source":["# Inference"]},{"cell_type":"markdown","metadata":{"id":"SKwFkGbCUkEc"},"source":["## Using a Pipeline"]},{"cell_type":"markdown","metadata":{"id":"yqKA8Kpzdz-K"},"source":["In the following cells, we instantiate a pipeline, which integrates a trained model (also called checkpoint in the Hugging Face world) and compatible tokenizer, and feed it with a sample sentence."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WLiCAVjzPQCJ"},"outputs":[],"source":["text = \"The Golden State Warriors are an American professional basketball team based in San Francisco.\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TF4Rn93mJF6B"},"outputs":[],"source":["ner_classifier = pipeline(\"ner\", model=\"olivermueller/my_awesome_wnut_model\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WwSox4_2ZAud"},"outputs":[],"source":["ner_classifier(text)"]},{"cell_type":"markdown","metadata":{"id":"TeVcDvp7UmIJ"},"source":["## By Hand"]},{"cell_type":"markdown","metadata":{"id":"GePBKSCvVAaX"},"source":["Instead of using a pipeline, we can also compute predictions step-by-step. Reproducing each of the steps by hand ✍️ will increase your understanding of the underlying logic."]},{"cell_type":"markdown","metadata":{"id":"8n2aGqXweMD9"},"source":["Load only the tokenizer."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z1P5Wk1lUr4A"},"outputs":[],"source":["tokenizer = AutoTokenizer.from_pretrained(\"olivermueller/my_awesome_wnut_model\")"]},{"cell_type":"markdown","metadata":{"id":"FxFSyooQeO1V"},"source":["Tokenize the example sentence."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Hod_oPTKUtcr"},"outputs":[],"source":["inputs = tokenizer(text, return_tensors=\"pt\")\n","inputs"]},{"cell_type":"markdown","metadata":{"id":"MKUAzdSBeQ2o"},"source":["Load the checkpoint."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M8Ppwvp4UuqP"},"outputs":[],"source":["model = AutoModelForTokenClassification.from_pretrained(\"olivermueller/my_awesome_wnut_model\")"]},{"cell_type":"markdown","metadata":{"id":"qIfVUJ66eTwq"},"source":["Do a forward pass through the network."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3n76RB6lUwoE"},"outputs":[],"source":["with torch.no_grad():\n","    logits = model(**inputs).logits\n","logits"]},{"cell_type":"markdown","metadata":{"id":"0RpWM84ReYBn"},"source":["For each token, get the index of the label with the highest score."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5FuJWnhJUx4o"},"outputs":[],"source":["predictions = torch.argmax(logits, dim=2)\n","predictions"]},{"cell_type":"markdown","metadata":{"id":"dMtc-L1celF5"},"source":["Translate the indices to textual labels."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yrsyEd6nUypj"},"outputs":[],"source":["predicted_token_class = [model.config.id2label[t.item()] for t in predictions[0]]\n","predicted_token_class"]},{"cell_type":"markdown","metadata":{},"source":["## On the HF Hub"]},{"cell_type":"markdown","metadata":{},"source":["One of the many great features of the HF hub is that you can test drive models directly on the web. Go to https://huggingface.co/olivermueller/my_awesome_wnut_model and try it out!"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["V-DU0hkyVyPi"],"machine_shape":"hm","private_outputs":true,"provenance":[{"file_id":"1kHkaqxz9sVdaOC2i4FJVOOFW9cCiBkYu","timestamp":1636383829898}]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.0"}},"nbformat":4,"nbformat_minor":0}
