{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF USING GOOGLE COLABORATORY -> RUN FIRST!!!\n",
    "# OTHERWISE -> IGNORE ;-)\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "!pip install pymysql"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#003660\">Applied Machine Learning for Text Analysis (M.184.5331)</font>\n",
    "# <font color=\"#003660\">Lesson 8: Creating a Neural Translator with Sequence-to-Sequence Models</font>\n",
    "\n",
    "<center><br><img width=256 src=\"https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/resources/dag.png\"/><br></center>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<div>\n",
    "    <font color=\"#085986\"><b>By the end of this lesson, you will be able to...</b><br><br>\n",
    "        ... understand the inner workings of sequence-to-sequence architectures;<br>\n",
    "        ... implement your own sequence-to-sequence models.<br>\n",
    "    </font>\n",
    "</div>\n",
    "</center>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img width=100 src=\"https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/resources/tip.png\"></center>\n",
    "\n",
    "<p><center><font color=\"red\"><strong><i>This entire tutorial is based on the implementations by <a href=\"https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\">Robertson (n.d.)</a> and <a href=\"https://github.com/bentrevett/pytorch-seq2seq/blob/master/2%20-%20Learning%20Phrase%20Representations%20using%20RNN%20Encoder-Decoder%20for%20Statistical%20Machine%20Translation.ipynb\">Trevett (n.d.)</a> of the architecture proposed by Cho et al. (2014).</i></strong></font></center></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. What is a Sequence-to-Sequence Model?\n",
    "\n",
    "\n",
    "## 1.1 General Idea\n",
    "\n",
    "<table class=\"image\">\n",
    "<center>\n",
    "<caption align=\"bottom\">(Lane et al., 2019,  p.318)</caption>\n",
    "<tr><td><img width=540 src='https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/week_6/images/seq2seq.png'></td></tr>\n",
    "</center>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Applications\n",
    "<center><img width=100 src=\"https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/resources/question.png\"></center>\n",
    "\n",
    "<p><center><b>What are possible applications of sequence-to-sequence models?<br>What are they good for?</b></center></p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset\n",
    "\n",
    "<p>For this tutorial, we will use, akin to <a href=\"https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\">Robertson (n.d.)</a>, a dataset provided by <a href=\"https://tatoeba.org/eng/\">Tatoeba.</a></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "################\n",
    "# Load dataset #\n",
    "################\n",
    "\n",
    "# Import\n",
    "import re\n",
    "import getpass\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Get credentials\n",
    "user = input(\"Username: \")\n",
    "host = input(\"Host: \")\n",
    "db = input(\"Database: \")\n",
    "passwd = getpass.getpass(\"Password: \")\n",
    "\n",
    "# Create an engine instance (SQLAlchemy)\n",
    "engine = create_engine(\"mysql+pymysql://{}:{}@{}/{}\".format(user, passwd, host, db))\n",
    "\n",
    "# Define SQL query\n",
    "sql_query = \"SELECT english, german FROM TatoebaEnglishGerman\"\n",
    "\n",
    "# Query dataset (pandas)\n",
    "data = pd.read_sql(sql=sql_query, con=engine)\n",
    "\n",
    "# Sample\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# Explore dataset #\n",
    "###################\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>For the sake of this tutorial, the documents were already preprocessed and have a maximum length of 10 tokens. However, some more preprocessing is required before we can get started!</p>\n",
    "\n",
    "<table class=\"image\">\n",
    "<center>\n",
    "<caption align=\"bottom\">(Lane et al., 2019,  p.319)</caption>\n",
    "<tr><td><img width=768 src='https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/week_6/images/preprocessing.png'></td></tr>\n",
    "</center>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# Preprocessing #\n",
    "#################\n",
    "\n",
    "# Import\n",
    "import re\n",
    "\n",
    "def preprocessing(doc):\n",
    "    \n",
    "    # Tokenize\n",
    "    doc = re.findall(r'\\w+', doc)\n",
    "    \n",
    "    # Add <sos> token (start of sentence)\n",
    "    # TODO\n",
    "    \n",
    "    # Add <eos> token (end of sentence)\n",
    "    # TODO\n",
    "    \n",
    "    return doc\n",
    "\n",
    "# Apply preprocessing\n",
    "data.english = data.english.apply(preprocessing)\n",
    "data.german = data.german.apply(preprocessing)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# Split dataset #\n",
    "# 80% / 5% / 5% #\n",
    "#################\n",
    "\n",
    "# Import\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Train / Test\n",
    "train, test = train_test_split(\n",
    "    data,\n",
    "    test_size=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Test / Validation\n",
    "test, val = train_test_split(\n",
    "    test,\n",
    "    test_size=0.5,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(train.shape)\n",
    "print(val.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Build vocabularies #\n",
    "######################\n",
    "\n",
    "def vocabulary_generator(corpus):\n",
    "    \n",
    "    # Initialize vocabulary\n",
    "    # TODO\n",
    "    \n",
    "    for doc in corpus:\n",
    "        for token in doc:\n",
    "            if token not in vocab:\n",
    "                vocab[token] = len(vocab)+1\n",
    "                \n",
    "    return vocab\n",
    "    \n",
    "# Source vocabulary – i.e., English\n",
    "src_vocabulary = vocabulary_generator(train.english)\n",
    "\n",
    "# Target vocabulary – i.e., German\n",
    "trg_vocabulary = vocabulary_generator(train.german)\n",
    "\n",
    "print(len(src_vocabulary))\n",
    "print(len(trg_vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# Vectorize documents #\n",
    "#######################\n",
    "\n",
    "# Import\n",
    "import torch\n",
    "\n",
    "MAX_LENGTH = 12\n",
    "\n",
    "def document_vectorizer(corpus, vocab):\n",
    "    \n",
    "    # Initialize output corpus\n",
    "    output_corpus = list()\n",
    "    \n",
    "    for doc in corpus:\n",
    "        \n",
    "        # Initialize output tensor\n",
    "        output_tensor = torch.zeros(MAX_LENGTH, dtype=torch.int64)\n",
    "\n",
    "        for index, token in enumerate(doc):\n",
    "            \n",
    "            # Check if token is in vocabulary\n",
    "            if token in vocab:output_tensor[index] = vocab[token]\n",
    "            else:output_tensor[index] = vocab['<unk>']\n",
    "        \n",
    "        # Append\n",
    "        output_corpus.append(output_tensor)\n",
    "    \n",
    "    return torch.stack(output_corpus)\n",
    "\n",
    "# Train\n",
    "src_train = document_vectorizer(train.english.to_list(), src_vocabulary)\n",
    "trg_train = document_vectorizer(train.german.to_list(), trg_vocabulary)\n",
    "\n",
    "# Validation\n",
    "src_val = document_vectorizer(val.english.to_list(), src_vocabulary)\n",
    "trg_val = document_vectorizer(val.german.to_list(), trg_vocabulary)\n",
    "\n",
    "# Test\n",
    "src_test = document_vectorizer(test.english.to_list(), src_vocabulary)\n",
    "trg_test = document_vectorizer(test.german.to_list(), trg_vocabulary)\n",
    "\n",
    "print(f'>>> Train:\\n• src -> {src_train.shape}\\n• trg -> {trg_train.shape}')\n",
    "print(f'\\n>>> Val.:\\n• src -> {src_val.shape}\\n• trg -> {trg_val.shape}')\n",
    "print(f'\\n>>> Test:\\n• src -> {src_test.shape}\\n• trg -> {trg_test.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Generate DataLoaders #\n",
    "########################\n",
    "\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "def dataloader_generator(src, trg, shuffle):\n",
    "\n",
    "    # Generate TensorDataset\n",
    "    dataset = torch.utils.data.TensorDataset(src, trg)\n",
    "    \n",
    "    # Return DataLoader\n",
    "    return torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        num_workers=8,\n",
    "        shuffle=shuffle,\n",
    "        batch_size=BATCH_SIZE\n",
    "    )\n",
    "\n",
    "train_dataloader = dataloader_generator(src_train, trg_train, True)\n",
    "val_dataloader = dataloader_generator(src_val, trg_val, False)\n",
    "test_dataloader = dataloader_generator(src_test, trg_test, False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##########\n",
    "# Import #\n",
    "##########\n",
    "\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch import optim\n",
    "import torch.nn.functional as F \n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f'>>> Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Encoder\n",
    "\n",
    "<table class=\"image\">\n",
    "<center>\n",
    "<caption align=\"bottom\">(Trevett, n.d.)</caption>\n",
    "<tr><td><img width=512 src='https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/week_6/images/trevett_encoder.png'></td></tr>\n",
    "</center>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# Encoder #\n",
    "###########\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers=1, dropout=0., bidirectional=True):\n",
    "        \n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        # Define attributes\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Define embedding layer (vocab_size, emb_dim)\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        \n",
    "        # Define recurrent layer (emb_dim, hidden_dim, *)\n",
    "        # Example -> Bidirectional GRU\n",
    "        self.rnn = nn.GRU(\n",
    "            emb_dim, \n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "    def forward(self, input):\n",
    "                \n",
    "        # Embedding layer\n",
    "        # Input -> (sequence_length, batch_size)\n",
    "        embedding = # TODO\n",
    "                        \n",
    "        # Recurrent layer\n",
    "        # Input -> (sequence_length, batch_size, embedding_dim)\n",
    "        _, hidden_state = # TODO\n",
    "        \n",
    "        # Return hidden state\n",
    "        # Output -> (n_layers * directions, batch_size, hidden_dim)\n",
    "        return hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# Debug encoder #\n",
    "#################\n",
    "\n",
    "DEBUG_BATCH_SIZE = 5\n",
    "\n",
    "# Initialize encoder\n",
    "debug_encoder = Encoder(len(src_vocabulary)+1, 200, 256, 1, 0., True)\n",
    "\n",
    "# Forward\n",
    "hidden_state = debug_encoder.forward(src_train[:DEBUG_BATCH_SIZE].permute(1,0))\n",
    "\n",
    "# Check shape\n",
    "print(hidden_state.shape)\n",
    "\n",
    "# Debugging\n",
    "# (if required!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Decoder\n",
    "\n",
    "<table class=\"image\">\n",
    "<center>\n",
    "<caption align=\"bottom\">(Trevett, n.d.)</caption>\n",
    "<tr><td><img width=256 src='https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/week_6/images/trevett_decoder.png'></td></tr>\n",
    "</center>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# Decoder #\n",
    "###########\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, vocab_size, emb_dim, hidden_dim, num_layers=1, dropout=0., bidirectional=True):\n",
    "        \n",
    "        super(Decoder, self).__init__()\n",
    "        \n",
    "        # Define attributes\n",
    "        self.vocab_size = vocab_size\n",
    "        self.hidden_dim = hidden_dim\n",
    "        \n",
    "        # Define embedding layer (vocab_size, emb_dim)\n",
    "        self.embedding = nn.Embedding(vocab_size, emb_dim)\n",
    "        \n",
    "        # Define recurrent layer (emb_dim, hidden_dim, *)\n",
    "        # Example -> Bidirectional GRU\n",
    "        self.rnn = nn.GRU(\n",
    "            emb_dim,\n",
    "            hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            dropout=dropout,\n",
    "            bidirectional=bidirectional\n",
    "        )\n",
    "        \n",
    "        # Define fully connected layer (hidden_dim * directions, vocab_size)\n",
    "        self.out = nn.Linear(hidden_dim * (2 if bidirectional else 1), vocab_size)\n",
    "                            \n",
    "    def forward(self, input, hidden_state):\n",
    "        \n",
    "        # Add dimension at position 0\n",
    "        # Input -> (batch_size)\n",
    "        input = input.unsqueeze(0)\n",
    "        \n",
    "        # Embedding layer\n",
    "        # Input -> (1, batch_size)\n",
    "        embedded = # TODO \n",
    "                \n",
    "        # Recurrent layer\n",
    "        # Input -> (sequence_length, batch_size, embedding_dim)\n",
    "        output, hidden_state = # TODO\n",
    "        \n",
    "        # FCL\n",
    "        # Input -> (sequence_length, batch_size, hidden_dim * directions)\n",
    "        predictions = self.out(output).squeeze(0)\n",
    "        \n",
    "        # Return predictions (logits) + hidden_state\n",
    "        # Prediction -> (batch_size, vocab_size)\n",
    "        return predictions, hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# Debug encoder #\n",
    "#################\n",
    "\n",
    "DEBUG_BATCH_SIZE = 5\n",
    "\n",
    "# Initialize encoder + Forward\n",
    "debug_encoder = Encoder(len(src_vocabulary)+1, 200, 256, 1, 0., True)\n",
    "hidden_state = debug_encoder.forward(src_train[:DEBUG_BATCH_SIZE].permute(1,0))\n",
    "\n",
    "# Initialize decoder + Forward\n",
    "debug_decoder = Decoder(len(trg_vocabulary)+1, 200, 256, 1, 0., True)\n",
    "predictions, hidden_state = debug_decoder.forward(trg_train[:DEBUG_BATCH_SIZE].permute(1,0)[0], hidden_state)\n",
    "\n",
    "# Check shape\n",
    "print(predictions.shape); print(hidden_state.shape)\n",
    "\n",
    "# Debugging\n",
    "# (if required!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Model\n",
    "\n",
    "<table class=\"image\">\n",
    "<center>\n",
    "<caption align=\"bottom\">(Trevett, n.d.)</caption>\n",
    "<tr><td><img width=512 src='https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/week_6/images/trevett_seq2seq.png'></td></tr>\n",
    "</center>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# Model #\n",
    "#########\n",
    "\n",
    "class Model(nn.Module):\n",
    "    \n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        # Define attributes\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        \n",
    "        # Initialize output vector (sequence_length, batch_size, vocab_size)\n",
    "        output_vector = torch.zeros(trg.shape[0], trg.shape[1], self.decoder.vocab_size).to(self.device)\n",
    "        \n",
    "        # Encoder\n",
    "        # Input -> (sequence_length, batch_size)\n",
    "        hidden_state = # TODO\n",
    "        \n",
    "        # First token (i.e., <sos>)\n",
    "        # Input -> (sequence_length, batch_size)\n",
    "        # Output -> (batch_size)\n",
    "        decoder_input = # TODO\n",
    "                \n",
    "        for index in range(1, trg.shape[0]):\n",
    "            \n",
    "            # Decoder\n",
    "            # Input -> (batch_size)\n",
    "            # Hidden state -> tensor (e.g., RNN / GRU) or tuple (e.g., LSTM)\n",
    "            predictions, hidden_state = # TODO\n",
    "            \n",
    "            # Append \n",
    "            output_vector[index] = predictions\n",
    "                        \n",
    "            # Teacher forcing\n",
    "            teacher_forcing = True if random.random() < teacher_forcing_ratio else False\n",
    "            decoder_input = trg[index] if teacher_forcing else torch.argmax(predictions, dim=1)\n",
    "            \n",
    "        # Return output_vector\n",
    "        # Output -> (sequence_length, batch_size, trg_vocab_size)\n",
    "        return output_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# Debug model #\n",
    "#################\n",
    "\n",
    "DEBUG_BATCH_SIZE = 5\n",
    "\n",
    "# Initialize encoder / decoder\n",
    "debug_encoder = Encoder(len(src_vocabulary)+1, 200, 256, 1, 0., True)\n",
    "debug_decoder = Decoder(len(trg_vocabulary)+1, 200, 256, 1, 0., True)\n",
    "\n",
    "# Initialize model\n",
    "debug_model = Model(debug_encoder, debug_decoder, 'cpu')\n",
    "\n",
    "# Forward\n",
    "output_vector = debug_model.forward(\n",
    "    src_train[:DEBUG_BATCH_SIZE].permute(1,0),\n",
    "    trg_train[:DEBUG_BATCH_SIZE].permute(1,0), \n",
    "    teacher_forcing_ratio=0.5\n",
    ")\n",
    "\n",
    "# Check shape\n",
    "print(output_vector.shape)\n",
    "\n",
    "# Debugging\n",
    "# (if required!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Initialize model #\n",
    "####################\n",
    "\n",
    "# Hyperparameters\n",
    "EMB_DIM = 300\n",
    "HIDDEN_DIM = 512\n",
    "NUM_LAYERS = 2\n",
    "DROPOUT = 0.\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "# Initialize encoder\n",
    "encoder = Encoder(len(src_vocabulary)+1, EMB_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT, BIDIRECTIONAL)\n",
    "\n",
    "# Initialize decoder\n",
    "decoder = Decoder(len(trg_vocabulary)+1, EMB_DIM, HIDDEN_DIM, NUM_LAYERS, DROPOUT, BIDIRECTIONAL)\n",
    "\n",
    "# Initialize model\n",
    "model = Model(encoder, decoder, device)\n",
    "model.to(device)\n",
    "\n",
    "# Define optimizer\n",
    "optimizer = optim.Adam(model.parameters(),lr=1e-3, weight_decay=1e-5)\n",
    "\n",
    "# Define loss function (a.k.a. criterion)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# Evaluation function #\n",
    "#######################\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    \n",
    "    # Initialize\n",
    "    total_loss = 0\n",
    "    model.eval()\n",
    "    \n",
    "    for batch_id, batch in enumerate(dataloader):\n",
    "\n",
    "        # Get validation/test data\n",
    "        src, trg = batch\n",
    "\n",
    "        # Permute dimensions\n",
    "        # Input -> (batch_size, sequence_length)\n",
    "        # Output -> (sequence_length, batch_size)\n",
    "        src = src.permute(1,0).to(device)\n",
    "        trg = trg.permute(1,0).to(device)\n",
    "         \n",
    "        with torch.no_grad():\n",
    "            \n",
    "            # Encoder\n",
    "            hidden_state = model.encoder(src)\n",
    "\n",
    "            # Initialize output vector (sequence_length, batch_size, vocab_size)\n",
    "            outputs = torch.zeros(src.shape[0], src.shape[1], model.decoder.vocab_size).to(device)\n",
    "\n",
    "            # First token (i.e., <sos>)    \n",
    "            decoder_input = src[0]\n",
    "\n",
    "            for index in range(1, src.shape[0]):\n",
    "                predictions, hidden_state = model.decoder(decoder_input, hidden_state)\n",
    "                outputs[index] = predictions\n",
    "                decoder_input = torch.argmax(predictions, dim=1)\n",
    "\n",
    "        # Skip first token (i.e., <sos>) + reshape output\n",
    "        # Input -> (sequence_length, batch_size, trg_vocab_size)\n",
    "        # Output -> (sequence_length-1 * batch_size, trg_vocab_size)\n",
    "        outputs = outputs[1:].reshape(-1, outputs.shape[2])\n",
    "\n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, trg[1:].reshape(-1))\n",
    "\n",
    "        # Update loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Train model #\n",
    "###############\n",
    "\n",
    "EPOCHS = 20\n",
    "\n",
    "train_history = list()\n",
    "validation_history = list()\n",
    "\n",
    "best_validation_loss = np.inf\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    # Initialize\n",
    "    total_loss = 0\n",
    "    model.train()\n",
    "        \n",
    "    for batch_id, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        # Get training data\n",
    "        src, trg = batch\n",
    "        \n",
    "        # Permute dimensions\n",
    "        # Input -> (batch_size, sequence_length)\n",
    "        # Output -> (sequence_length, batch_size)\n",
    "        src = src.permute(1,0).to(device)\n",
    "        trg = trg.permute(1,0).to(device)\n",
    "  \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Feedforward\n",
    "        # teacher_forcing_ratio >= 0.0\n",
    "        outputs = model(src, trg, teacher_forcing_ratio=0.4)\n",
    "               \n",
    "        # Skip first token (i.e., <sos>) + reshape output\n",
    "        # Input -> (sequence_length, batch_size, trg_vocab_size)\n",
    "        # Output -> (sequence_length-1 * batch_size, trg_vocab_size)\n",
    "        outputs = outputs[1:].reshape(-1, outputs.shape[2])\n",
    "                        \n",
    "        # Compute loss\n",
    "        loss = criterion(outputs, trg[1:].reshape(-1))\n",
    "        \n",
    "        # Backpropagate errors\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "\n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update loss\n",
    "        total_loss =+ loss.item()\n",
    "        \n",
    "    # Validation\n",
    "    validation_loss = evaluate(model, val_dataloader, criterion)\n",
    "    \n",
    "    if validation_loss < best_validation_loss:\n",
    "\n",
    "        best_validation_loss = validation_loss\n",
    "        torch.save(model, 'best_seq2seq_model.pt')\n",
    "    \n",
    "    train_history.append(total_loss)\n",
    "    validation_history.append(validation_loss)\n",
    "    \n",
    "    print({ 'epoch': epoch, 'training loss': total_loss, 'validation loss': validation_loss})\n",
    "\n",
    "print('\\n>>> DONE!')\n",
    "print(f'>>> BEST MODEL (VALIDATION): {best_validation_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# Plot history #\n",
    "################\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "\n",
    "axs[0].plot(train_history)\n",
    "axs[0].set_title('Training Loss')\n",
    "\n",
    "axs[1].plot(validation_history)\n",
    "axs[1].set_title('Validation Loss')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# Evaluate model (test) #\n",
    "#########################\n",
    "\n",
    "# Load model\n",
    "best_model = torch.load('best_seq2seq_model.pt')\n",
    "\n",
    "# Evaluate\n",
    "print(evaluate(best_model, test_dataloader, criterion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############\n",
    "# Translate #\n",
    "#############\n",
    "\n",
    "def get_translation(document, seq2seq_model, greedy_decoding=True, temperature=0.1):\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        # Add dimension\n",
    "        document = document.unsqueeze(1)\n",
    "        \n",
    "        # Encoder\n",
    "        hidden_state = seq2seq_model.encoder(document.to(device))\n",
    "        \n",
    "        # Initialize output document\n",
    "        output_document = [trg_vocabulary['<sos>']]\n",
    "\n",
    "        for _ in range(1, src.shape[0]):\n",
    "            \n",
    "            # Feedforward\n",
    "            outputs, hidden_state = seq2seq_model.decoder(torch.LongTensor([output_document[-1]]).to(device), hidden_state)\n",
    "\n",
    "            if greedy_decoding is True:\n",
    "\n",
    "                # Greedy decoding\n",
    "                output_document.append(outputs.argmax().item())\n",
    "            \n",
    "            else:\n",
    "\n",
    "                # Probabilistic sampling\n",
    "                output_document.append(torch.multinomial(F.softmax(outputs / temperature, dim=1), 1).item()) \n",
    "\n",
    "            if output_document[-1] == trg_vocabulary['<eos>']:\n",
    "                break\n",
    "            \n",
    "        return output_document\n",
    "\n",
    "def tokens_lookup(tokens_idx, vocabulary):\n",
    "    tokens = [list(vocabulary.keys())[list(vocabulary.values()).index(token)] for token in tokens_idx if token != vocabulary['<pad>']]\n",
    "    return ' '.join(tokens)\n",
    "        \n",
    "# Document ID\n",
    "ID = 13 # 166 # 230 # 442 # 178\n",
    "\n",
    "# Translate\n",
    "tokens_idx = get_translation(src_test[ID], best_model, greedy_decoding=True)\n",
    "translation = tokens_lookup(tokens_idx, trg_vocabulary)\n",
    "print(f'>>> Translation: {translation}')\n",
    "\n",
    "# Source & Target\n",
    "print(f'\\n>>> Source: {tokens_lookup(src_test[ID].tolist(), src_vocabulary)}')\n",
    "print(f'>>> Target: {tokens_lookup(trg_test[ID].tolist(), trg_vocabulary)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"list-style-type:round\">\n",
    "<i>\n",
    "    <li>Cho, K., Van Merriënboer, B., Gulcehre, C., Bahdanau, D., Bougares, F., Schwenk, H., &amp; Bengio, Y. (2014). Learning phrase representations using RNN encoder-decoder for statistical machine translation.</li>\n",
    "    <li>Lane, H., Howard, C., &amp; Hapke, H.M. (2019). Natural Language Processing in Action. Shelter Island, NY: Manning Publications Co.</li>\n",
    "    <li>Rao, D., &amp; McMahan, B. (2019). Natural Language Processing with Pytorch. Sebastopol, CA: O'Reilly Media.</li>\n",
    "    <li>Robertson, S. (n.d.). NLP from Scratch: Translation with a Sequence-to-Sequence Network and Attention. <a href=\"https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html\">Link</a>.</li>\n",
    "    <li>Trevett, B. (n.d.). pytorch-seq2seq:  Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation. <a href=\"https://github.com/bentrevett/pytorch-seq2seq/blob/master/2%20-%20Learning%20Phrase%20Representations%20using%20RNN%20Encoder-Decoder%20for%20Statistical%20Machine%20Translation.ipynb\">Link</a>.</li>\n",
    "</i>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
