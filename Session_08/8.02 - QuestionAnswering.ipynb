{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# IF USING GOOGLE COLABORATORY -> RUN FIRST!!!\n",
    "# OTHERWISE -> IGNORE ;-)\n",
    "\n",
    "from google.colab import drive\n",
    "\n",
    "drive.mount('/content/gdrive')\n",
    "\n",
    "!pip install pymysql\n",
    "!pip install transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <font color=\"#003660\">Applied Machine Learning for Text Analysis (M.184.5331)</font>\n",
    "# <font color=\"#003660\">Lesson 9: How Much do Machines Truly Understand? Putting BERT's Reading Comprehension Skills to the Test</font>\n",
    "\n",
    "<center><br><img width=256 src=\"https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/resources/dag.png\"/><br></center>\n",
    "\n",
    "<p>\n",
    "<center>\n",
    "<div>\n",
    "    <font color=\"#085986\"><b>By the end of this lesson, you will be able to...</b><br><br>\n",
    "        ... recognize the benefits of the Transformer approach; and<br>\n",
    "        ... implement and train a question answering model using BERT.<br>\n",
    "    </font>\n",
    "</div>\n",
    "</center>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Transformers & Question Answering: What's All That Hype About?\n",
    "\n",
    "<table class=\"image\">\n",
    "<center>\n",
    "<caption align=\"bottom\">(Vaswani et al., 2017,  p.3)</caption>\n",
    "<tr><td><img width=384 src='https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/week_7/images/transformer_architecture.png'></td></tr>\n",
    "</center>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<table class=\"image\">\n",
    "<center>\n",
    "<caption align=\"bottom\">(Devlin et al., 2018)</caption>\n",
    "<tr><td><img width=640 src='https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/week_7/images/bert.png'></td></tr>\n",
    "</center>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Dataset\n",
    "\n",
    "<p>For this tutorial, we will use the so-called <a href=\"https://rajpurkar.github.io/SQuAD-explorer/\">Stanford Question Answering Dataset (SQuAD)</a>. As revealed by its name, this benchmark dataset is used for training and evaluating models on the task of question answering. Because of the sheer size of the original dataset and the complexity of the task at hand, we will focus on a small sample of the corpus (SQuAD1.1).</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "################\n",
    "# Load dataset #\n",
    "################\n",
    "\n",
    "# Import\n",
    "import re\n",
    "import getpass\n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Get credentials\n",
    "user = input(\"Username: \")\n",
    "host = input(\"Host: \")\n",
    "db = input(\"Database: \")\n",
    "passwd = getpass.getpass(\"Password: \")\n",
    "\n",
    "# Create an engine instance (SQLAlchemy)\n",
    "engine = create_engine(\"mysql+pymysql://{}:{}@{}/{}\".format(user, passwd, host, db))\n",
    "\n",
    "# Define SQL query\n",
    "sql_query = \"SELECT * FROM SQuAD_V1_Sample\"\n",
    "\n",
    "# Query dataset (pandas)\n",
    "data = pd.read_sql(sql=sql_query, con=engine, index_col='index')\n",
    "\n",
    "# Sample\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################\n",
    "# Explore dataset #\n",
    "###################\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################\n",
    "# Train / Val./ Test #\n",
    "######################\n",
    "\n",
    "# Import\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train, test = train_test_split(data, train_size=0.9, random_state=42)\n",
    "test, val = train_test_split(test, test_size=0.5, random_state=42)\n",
    "\n",
    "print(train.shape)\n",
    "print(val.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# Print sample #\n",
    "################\n",
    "\n",
    "print(train.iloc[0]['context'])\n",
    "print(f\"\\n>>> Question: {train.iloc[0]['question']}\")\n",
    "print(f\">>> Answer: {train.iloc[0]['answer_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# Preprocessing #\n",
    "#################\n",
    "\n",
    "def preprocessing(row):\n",
    "    \n",
    "    # Remove special characters\n",
    "    regex = re.compile(r'[^a-z0-9\\-\\'\\s]')\n",
    "    \n",
    "    # Remove multiple spaces\n",
    "    row.context = re.sub(r' +', ' ', re.sub(regex, '', row.context.lower()))\n",
    "    row.question = re.sub(r' +', ' ', re.sub(regex, '', row.question.lower()))\n",
    "    row.answer_text = re.sub(r' +', ' ', re.sub(regex, '', row.answer_text.lower()))\n",
    "    \n",
    "    return row\n",
    "\n",
    "# Apply preprocessing\n",
    "train = train.apply(preprocessing, axis=1).reset_index(drop=True)\n",
    "val = val.apply(preprocessing, axis=1).reset_index(drop=True)\n",
    "test = test.apply(preprocessing, axis=1).reset_index(drop=True)\n",
    "\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Question Answering (Almost) From Scratch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Encoding\n",
    "\n",
    "<table class=\"image\">\n",
    "<center>\n",
    "<caption align=\"bottom\">(Devlin et al., 2018)</caption>\n",
    "<tr><td><img width=384 src='https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/week_7/images/bert_qa.png'></td></tr>\n",
    "</center>\n",
    "</table>\n",
    "\n",
    "<p>As can be seen above, we need to encode our data in a specific manner in order to train a question answering model &mdash; i.e., for every entry in our dataset, we need to encode a single sequence (starting with the so-called <code>[CLS]</code> token) composed of the question and the context divided by a <code>[SEP]</code> token. Keep in mind that different Transformer archictures may require different encodings!</p><br>\n",
    "\n",
    "<center><a href=\"https://huggingface.co/transformers/\"><img width=512 src=\"https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/week_7/images/huggingface_transformers_logo.png\"/></a></center>\n",
    "\n",
    "<p>For the rest of this tutorial, we will be using models, implementations, and tokenizers contained within the <a href=\"https://huggingface.co/transformers/\">Transformers</a> library provided by <a href=\"https://huggingface.co\">huggingface</a> (Wolf et al., 2019). This library offers one of the most extensive collection of NLP resources and is, without a doubt, the reference when it comes to Transformer models.</p>\n",
    "\n",
    "<br>\n",
    "\n",
    "<p><center><a href=\"https://huggingface.co/transformers/pretrained_models.html\">https://huggingface.co/transformers/pretrained_models.html</a></center></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Encoding example #\n",
    "# >>> Question     #\n",
    "####################\n",
    "\n",
    "# Import\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Define model\n",
    "# BERT -> 'bert-base-uncased'\n",
    "# MiniLM (12 layers) -> 'microsoft/MiniLM-L12-H384-uncased'\n",
    "# MiniLM (6 layers) -> 'nreimers/MiniLM-L6-H384-uncased'\n",
    "# MiniLM -> https://arxiv.org/pdf/2002.10957.pdf\n",
    "\n",
    "PATH = 'nreimers/MiniLM-L6-H384-uncased'\n",
    "\n",
    "# Initialize tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(PATH, use_fast=True)\n",
    "\n",
    "# Question\n",
    "sample_question = train.iloc[0]['question']\n",
    "\n",
    "# Tokenizer\n",
    "sample_question_encoded = tokenizer(\n",
    "    # TODO\n",
    ")\n",
    "\n",
    "print(f'Question:\\n{sample_question}\\n')\n",
    "print(f'Token:\\n{tokenizer.convert_ids_to_tokens(sample_question_encoded[\"input_ids\"][0])}\\n')\n",
    "print(f'input_ids:\\n{sample_question_encoded[\"input_ids\"]}\\n')\n",
    "print(f'token_type_ids:\\n{sample_question_encoded[\"token_type_ids\"]}\\n')\n",
    "print(f'attention_mask:\\n{sample_question_encoded[\"attention_mask\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "##########################\n",
    "# Encoding example       #\n",
    "# >>> Question + Context #\n",
    "##########################\n",
    "\n",
    "# Question + Context\n",
    "sample_question = train.iloc[0]['question']\n",
    "sample_context = train.iloc[0]['context']\n",
    "\n",
    "# Tokenizer\n",
    "sample_sequence_encoded = tokenizer(\n",
    "    # TODO\n",
    ")\n",
    "\n",
    "print(f'Question:\\n{sample_question}\\n')\n",
    "print(f'Context:\\n{sample_context}\\n')\n",
    "print(f'input_ids:\\n{sample_sequence_encoded[\"input_ids\"]}\\n')\n",
    "print(f'token_type_ids:\\n{sample_sequence_encoded[\"token_type_ids\"]}\\n')\n",
    "print(f'attention_mask:\\n{sample_sequence_encoded[\"attention_mask\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img width=100 src=\"https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/resources/question.png\"></center>\n",
    "\n",
    "<p><center><b>What can we observe here?<br>What is the tensor <code>token_type_ids</code> good for?</b><br><b>What is the purpose of the <code>attention_mask</code> tensor?</b></center></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########################\n",
    "# Generate DataLoaders #\n",
    "########################\n",
    "\n",
    "# Import\n",
    "import torch\n",
    "\n",
    "def dataloader_generator(dataframe, batch_size):\n",
    "    \n",
    "    # Maximum sequence length\n",
    "    MAX_LENGTH = 512\n",
    "    \n",
    "    # Tokenizer\n",
    "    sequences = tokenizer(\n",
    "        dataframe.question.tolist(), \n",
    "        dataframe.context.tolist(), \n",
    "        add_special_tokens=True,\n",
    "        padding=True, \n",
    "        truncation=True, \n",
    "        max_length=MAX_LENGTH,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Initialize\n",
    "    start_tokens = list()\n",
    "    end_tokens = list()\n",
    "    \n",
    "    for index, answer in enumerate(dataframe.answer_text.tolist()):\n",
    "    \n",
    "        # Define context start\n",
    "        context_start = (sequences['input_ids'][index]==102).nonzero()[0]\n",
    "\n",
    "        # Encode answers\n",
    "        answer = tokenizer.encode(answer, add_special_tokens=False)\n",
    "        answer = list(filter(lambda token: token in sequences['input_ids'][index][context_start:], answer))\n",
    "            \n",
    "        # Find start/end tokens (answer)    \n",
    "        for token in (sequences['input_ids'][index]==answer[0]).nonzero():\n",
    "\n",
    "            if token.item() > context_start:\n",
    "\n",
    "                if sequences['input_ids'][index][token.item()+len(answer)-1]==answer[-1]:\n",
    "\n",
    "                    start_tokens.append(token.item())\n",
    "                    end_tokens.append(token.item()+len(answer)-1)\n",
    "\n",
    "                    break\n",
    "    \n",
    "    # Generate dataset\n",
    "    dataset = torch.utils.data.TensorDataset(\n",
    "        sequences['input_ids'], \n",
    "        sequences['token_type_ids'], \n",
    "        sequences['attention_mask'], \n",
    "        torch.tensor(start_tokens), \n",
    "        torch.tensor(end_tokens)\n",
    "    )\n",
    "                \n",
    "    return torch.utils.data.DataLoader(dataset, batch_size=batch_size)\n",
    "\n",
    "# Define batch size\n",
    "\n",
    "# (bert-base-uncased)\n",
    "# BATCH_SIZE = 6  ~ 8 GB RAM (GPU) required! \n",
    "# BATCH_SIZE = 12 ~ 12 GB RAM (GPU) required! \n",
    "# BATCH_SIZE = 24 ~ 22 GB RAM (GPU) required!\n",
    "\n",
    "# (MiniLM / 6 layers)\n",
    "# BATCH_SIZE = 6  ~  3 GB RAM (GPU) required! \n",
    "# BATCH_SIZE = 12 ~  5 GB RAM (GPU) required! \n",
    "# BATCH_SIZE = 24 ~  8 GB RAM (GPU) required! \n",
    "# BATCH_SIZE = 48 ~ 14 GB RAM (GPU) required! \n",
    "\n",
    "BATCH_SIZE = 48\n",
    "\n",
    "train_dataloader = dataloader_generator(train, BATCH_SIZE)\n",
    "val_dataloader = dataloader_generator(val, BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Model\n",
    "\n",
    "<center><img width=100 src=\"https://git.uni-paderborn.de/data.analytics.teaching/aml4ta-2020/-/raw/master/resources/tip.png\"></center>\n",
    "\n",
    "<p><center><font color=\"#003660\"><strong><i>The following model is based on the implementation of the <a href=\"https://huggingface.co/docs/transformers/model_doc/bert#transformers.BertForQuestionAnswering\">BertForQuestionAnswering</a> module by huggingface (Wolf et al., 2019).</i></strong></font></center></p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########\n",
    "# Model #\n",
    "#########\n",
    "\n",
    "# Import\n",
    "from torch import nn\n",
    "from transformers import AutoModel, logging\n",
    "\n",
    "# Disable warnings\n",
    "logging.set_verbosity_error()\n",
    "\n",
    "\n",
    "# Define model\n",
    "class QuestionAnswering_Model(nn.Module):\n",
    "\n",
    "    def __init__(self, model):\n",
    "\n",
    "        super(QuestionAnswering_Model, self).__init__()\n",
    "        \n",
    "        # Define model\n",
    "        self.lm = # TODO\n",
    "        \n",
    "        # Define FCL\n",
    "        self.linear = # TODO\n",
    "                \n",
    "    def forward(self, input_ids, attention_mask, token_type_ids):\n",
    "        \n",
    "        # Transformer\n",
    "        # Output[0] -> (batch_size, seq_len, hidden_size)\n",
    "        outputs = self.lm(\n",
    "            # TODO\n",
    "        )\n",
    "        \n",
    "        # FCL\n",
    "        # Output -> (batch_size, seq_len, 2)\n",
    "        ouputs = # TODO\n",
    "        \n",
    "        return ouputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Debug model #\n",
    "###############\n",
    "\n",
    "sample_question = train.iloc[0]['question']\n",
    "sample_context = train.iloc[0]['context']\n",
    "\n",
    "# Tokenizer\n",
    "sample_sequence_encoded = tokenizer(\n",
    "    sample_question,\n",
    "    sample_context, \n",
    "    add_special_tokens=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "\n",
    "model = QuestionAnswering_Model(PATH)\n",
    "outputs = model.forward(**sample_sequence_encoded)\n",
    "\n",
    "print(outputs.shape)\n",
    "\n",
    "# Debugging\n",
    "# (if required!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# Explore #\n",
    "# Part 1  #\n",
    "###########\n",
    "\n",
    "# Split\n",
    "start_logits, end_logits = outputs[0].split(1, dim=-1)\n",
    "\n",
    "print(start_logits.shape); print(end_logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########\n",
    "# Explore #\n",
    "# Part 2  #\n",
    "###########\n",
    "\n",
    "# Get start_position / end_position\n",
    "start_position = torch.argmax(start_logits)\n",
    "end_position = torch.argmax(end_logits)\n",
    "\n",
    "print(start_position); print(end_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####################\n",
    "# Initialize model #\n",
    "####################\n",
    "\n",
    "# Define device (GPU vs CPU)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "   \n",
    "#  Model\n",
    "model = QuestionAnswering_Model(PATH)\n",
    "model.to(device)\n",
    "\n",
    "# Optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=4e-5)\n",
    "\n",
    "# Loss function (a.k.a. criterion)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "    \n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#################\n",
    "# Loss function #\n",
    "#################\n",
    "\n",
    "def compute_loss(outputs, true_start, true_end):\n",
    "    \n",
    "    # Split logits – i.e., 1x tensor for start_logits, 1x tensor for end_logits\n",
    "    # Output: (batch_size, seq_len, 1)\n",
    "    pred_start, pred_end = outputs.split(1, dim=-1)\n",
    "\n",
    "    # Squeeze last dimension\n",
    "    # Output -> (batch_size, seq_len)\n",
    "    pred_start = pred_start.squeeze(-1)\n",
    "    pred_end = pred_end.squeeze(-1)\n",
    "\n",
    "    # Squeeze last dimension\n",
    "    # Output -> (batch_size, seq_len)\n",
    "    true_start = true_start.squeeze(-1)\n",
    "    true_end = true_end.squeeze(-1)\n",
    "\n",
    "    # Compute loss \n",
    "    # loss -> mean(start_loss, end_loss)\n",
    "    loss_start = criterion(pred_start, true_start.to(device))\n",
    "    loss_end = criterion(pred_end, true_end.to(device))\n",
    "    loss = torch.mean(torch.stack([loss_start, loss_end]))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#######################\n",
    "# Evaluation function #\n",
    "#######################\n",
    "\n",
    "def evaluate(model, dataloader, criterion):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_id, batch in enumerate(dataloader):\n",
    "\n",
    "        # Get validation/test data\n",
    "        input_ids, token_type_ids, attention_mask, true_start, true_end = batch\n",
    "\n",
    "        # Feedforward\n",
    "        with torch.no_grad():\n",
    "            \n",
    "            outputs = model(\n",
    "                input_ids.to(device),\n",
    "                attention_mask.to(device),\n",
    "                token_type_ids.to(device)\n",
    "            )\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = compute_loss(outputs, true_start, true_end)\n",
    "\n",
    "        # Update loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "    return total_loss / len(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "###############\n",
    "# Train model #\n",
    "###############\n",
    "\n",
    "EPOCHS = 5\n",
    "\n",
    "train_history = list()\n",
    "validation_history = list()\n",
    "\n",
    "best_validation_loss = np.inf\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(EPOCHS):\n",
    "    \n",
    "    model.train()\n",
    "        \n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch_id, batch in enumerate(train_dataloader):\n",
    "        \n",
    "        # Get training data\n",
    "        input_ids, token_type_ids, attention_mask, true_start, true_end = batch\n",
    "        \n",
    "        # Clear gradients\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Feedforward\n",
    "        outputs = model(\n",
    "            input_ids.to(device),\n",
    "            attention_mask.to(device),\n",
    "            token_type_ids.to(device)\n",
    "        )\n",
    "        \n",
    "        # Compute loss\n",
    "        loss = compute_loss(outputs, true_start, true_end)\n",
    "        \n",
    "        # Backpropagate errors\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.)\n",
    "        \n",
    "        # Update weights\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Update loss\n",
    "        total_loss =+ loss.item()\n",
    "        \n",
    "    # Validation\n",
    "    validation_loss = evaluate(model, val_dataloader, criterion)\n",
    "    \n",
    "    if validation_loss < best_validation_loss:\n",
    "\n",
    "        best_validation_loss = validation_loss\n",
    "        torch.save(model, 'best_QA_model.pt')\n",
    "    \n",
    "    train_history.append(total_loss)\n",
    "    validation_history.append(validation_loss)\n",
    "    \n",
    "    print({ 'epoch': epoch, 'training loss': total_loss, 'validation loss': validation_loss})\n",
    "    \n",
    "print('\\n>>> DONE!')\n",
    "print(f'>>> BEST MODEL (VALIDATION): {best_validation_loss}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################\n",
    "# Plot history #\n",
    "################\n",
    "\n",
    "fig, axs = plt.subplots(nrows=1, ncols=2, figsize=(12, 4))\n",
    "\n",
    "axs[0].plot(train_history)\n",
    "axs[0].set_title('Training Loss')\n",
    "\n",
    "axs[1].plot(validation_history)\n",
    "axs[1].set_title('Validation Loss')\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###############\n",
    "# Clear cache #\n",
    "###############\n",
    "\n",
    "del model, train, train_dataloader, val, val_dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################\n",
    "# Evaluate model (test) #\n",
    "#########################\n",
    "\n",
    "# Load model\n",
    "best_model = torch.load('best_QA_model.pt')\n",
    "\n",
    "# Evaluate\n",
    "test_dataloader = dataloader_generator(test, BATCH_SIZE)\n",
    "print(evaluate(best_model, test_dataloader, criterion))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Test model (best model) #\n",
    "###########################\n",
    "\n",
    "def get_answer(model, model_path, question, context):\n",
    "    \n",
    "    model.eval()\n",
    "    \n",
    "    # Tokenizer / Encoder\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=True)\n",
    "    sequence = tokenizer(\n",
    "        question, \n",
    "        context, \n",
    "        add_special_tokens=True, \n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    sequence.to(device)\n",
    "    \n",
    "    # Feedforward\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**sequence)\n",
    "    \n",
    "    # Logits\n",
    "    if len(outputs) == 1:\n",
    "        start_logits, end_logits = outputs.split(1, dim=-1)        \n",
    "    else:\n",
    "        start_logits  = outputs[0]\n",
    "        end_logits = outputs[1]\n",
    "   \n",
    "    # Get start_position / end_position\n",
    "    start_position = torch.argmax(start_logits.cpu())\n",
    "    end_position = torch.argmax(end_logits.cpu())\n",
    "            \n",
    "    # Convert sequence to tokens\n",
    "    sequence = tokenizer.convert_ids_to_tokens(sequence['input_ids'][0].cpu())\n",
    "    sequence = ' '.join(sequence[start_position:end_position+1])\n",
    "       \n",
    "    return re.sub(r' \\#\\#', '', sequence) if sequence else 'Sorry... I could not find an answer :-('\n",
    " \n",
    "# Question ID\n",
    "QUESTION_ID = 0\n",
    "\n",
    "# Answer (Model)\n",
    "question = test.iloc[QUESTION_ID]['question']\n",
    "context = test.iloc[QUESTION_ID]['context']\n",
    "model_answer = get_answer(best_model, PATH, question, context)\n",
    "                    \n",
    "# Question / Context / Answer (Model) / Answer (Truth) \n",
    "print(f\">>> Context: {context}\")\n",
    "print(f\"\\n>>> Question: {question}\")\n",
    "print(f\">>> Answer (Model): {model_answer}\")\n",
    "print(f\">>> Answer (Truth): {test.iloc[QUESTION_ID]['answer_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. The Easy Approach &rarr; <code>BertForQuestionAnswering(...)</code>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "################\n",
    "# Define model #\n",
    "################\n",
    "\n",
    "# Import\n",
    "from transformers import BertForQuestionAnswering\n",
    "\n",
    "# Path\n",
    "MODEL_PATH = 'bert-large-uncased-whole-word-masking-finetuned-squad'\n",
    "\n",
    "# Load model\n",
    "finetuned_model = BertForQuestionAnswering.from_pretrained(MODEL_PATH)\n",
    "finetuned_model.to(device)\n",
    "\n",
    "print(finetuned_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###########################\n",
    "# Test model (fine-tuned) #\n",
    "###########################\n",
    "\n",
    "# Question ID \n",
    "QUESTION_ID = 0\n",
    "\n",
    "# Answer (Model)\n",
    "question = test.iloc[QUESTION_ID]['question']\n",
    "context = test.iloc[QUESTION_ID]['context']\n",
    "model_answer = get_answer(finetuned_model, MODEL_PATH, question, context)\n",
    "                    \n",
    "# Question / Context / Answer (Model) / Answer (Truth) \n",
    "print(f\">>> Question: {question}\")\n",
    "print(f\"\\n>>> Context: {context}\")\n",
    "print(f\"\\n>>> Answer (Model): {model_answer}\")\n",
    "print(f\">>> Answer (Truth): {test.iloc[QUESTION_ID]['answer_text']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################################\n",
    "# Example (UPB)                                      #\n",
    "# https://en.wikipedia.org/wiki/Paderborn_University #\n",
    "######################################################\n",
    "\n",
    "# Answer (Model)\n",
    "question = \"where is paderborn university located\"\n",
    "context = \"\"\"Paderborn University is one of the fourteen public research universities in the state of\n",
    "North Rhine-Westphalia in Germany. It was founded in 1972 and 20,308 students were enrolled at the university \n",
    "in the wintersemester 2016/2017.[1] It offers 62 different degree programmes. The university has several winners of the Gottfried Wilhelm Leibniz Prize awarded by the German Research Foundation \n",
    "(DFG) and ERC grant recipients of the European Research Council. In 2002, the Romanian mathematician Preda Mihailescu proved the Catalan conjecture, a number-theoretical conjecture, formulated by the French and \n",
    "Belgian mathematician Eugène Charles Catalan, which had stood unresolved for 158 years. The University Closely \n",
    "Collaborates with the Heinz Nixdorf Institute, Paderborn Center for Parallel Computing and two Fraunhofer Institutes\n",
    "for research in Computer Science, Mathematics, Electrical Engineering and Quantum Photonics.\"\"\"\n",
    "model_answer = get_answer(finetuned_model, MODEL_PATH, question, re.sub(r'[^a-z0-9\\-\\'\\s]', '', context.lower()))\n",
    "\n",
    "# Question / Answer (Model)\n",
    "print(f\">>> Question: {question}\")\n",
    "print(f\">>> Answer (Model): {model_answer}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<ul style=\"list-style-type:round\">\n",
    "<i>\n",
    "    <li>Bahdanau, D., Cho, K., &amp; Bengio, Y. (2014). Neural machine translation by jointly learning to align and translate.</li>\n",
    "    <li>Devlin, J., Chang, M. W., Lee, K., &amp; Toutanova, K. (2018). Bert: Pre-training of deep bidirectional transformers for language understanding.</li>\n",
    "    <li>Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A. N., &amp; Polosukhin, I. (2017). Attention is all you need. Advances in neural information processing systems, 30, 5998-6008.</li>\n",
    "    <li>Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., &amp; Brew, J. (2019). HuggingFace's Transformers: State-of-the-art Natural Language Processing.</li>\n",
    "</i>\n",
    "</ul>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
